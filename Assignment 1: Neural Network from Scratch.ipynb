{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Fundamentals.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plESS57GbdkD",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning Fundamentals - Feedforward Neural Network from Scratch\n",
        "_Yoav Rabinovich, October 2019_\n",
        "\n",
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7lfJsQXv75k",
        "colab_type": "text"
      },
      "source": [
        "In this assignment I avoided using any external package bar CSV and Numpy. I implemented a simple feedforward, fully connected neural network of arbitrary size and activation function, and arbitrary input and output dimensions. I've implemented the Sigmoid, Tanh, and Leaky ReLU activation functions, matching them with approporiate weight initialization methods, as well as the logarithmic loss function.\n",
        "\n",
        "At first, I implemented Sigmoid and Tanh, an interesting result occurs when the network was trained using either: The network defaults to outputing 1 for both classes regardless of input. This was caused due to saturation of the activation functions: the activation values were all high enough to map to 1 on the activation function curves. Indeed, implementing and training the network using Leaky ReLU, the problem disappeared.\n",
        "\n",
        "Training on the Titanic dataset using Leaky ReLU, the network hit a local minimum at loss=500. An evaluation of the result shows perfect precision and recall, but low accuracy. This is because the local minimum found defaulted to declaring all people on board as casualties automatically. This suggests a need to decrease the biasof the network, but as this wasn't the focus of the assignment, this step was skipped to conserve time.\n",
        "\n",
        "Colab Link: https://colab.research.google.com/drive/1LSppmf8Ws0H4jtTkcoltxGdmZjc8MNGq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lb14dy8Zvva",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports and Data (Automatic Import)\n",
        "\n",
        "Kaggle_Username = 'xxx' #@param {type:\"string\"}\n",
        "Kaggle_Key = 'xxxx' #@param {type:\"string\"}\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = Kaggle_Username \n",
        "os.environ['KAGGLE_KEY'] = Kaggle_Key\n",
        "!kaggle competitions download -c titanic\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygLUEI1nmaGq",
        "colab_type": "text"
      },
      "source": [
        "#### Neuron and Layer Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUfEsTGmmctQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "    \"\"\"\n",
        "    Initializes weights and bias using schemes depending on activation function:\n",
        "    - Sigmoid -> uniform from [0,1).\n",
        "    - Tanh -> Xavier initialization.\n",
        "    - Leaky ReLU -> He initialization.\n",
        "    \"\"\"\n",
        "    def __init__(self, inputs_n, activation_f):\n",
        "        if activation_f == sigmoid_activation_f:\n",
        "            self.weights = np.random.uniform(-1,1,inputs_n+1)\n",
        "        if activation_f == tanh_activation_f:\n",
        "            self.weights = np.random.uniform(-1,1,inputs_n+1)*np.sqrt(3/inputs_n)\n",
        "        if activation_f == lrelu_activation_f:\n",
        "            self.weights = np.random.uniform(-1,1,inputs_n+1)*np.sqrt(2/inputs_n)\n",
        "        self.activation = activation_f\n",
        "        self.output = 0\n",
        "        self.delta = 0\n",
        "\n",
        "    def activate(self,inputs):\n",
        "        \"\"\"\n",
        "        Adds together bias and weighted inputs and applies the activation function.\n",
        "        \"\"\"\n",
        "        a = self.weights[-1]\n",
        "        for i in range(len(self.weights)-1):\n",
        "            a += self.weights[i]*inputs[i]\n",
        "        self.output = self.activation.evaluate(a)\n",
        "        return (self.output)\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"\n",
        "    Collects neurons into a list representing a layer\n",
        "    \"\"\"\n",
        "    def __init__(self, neurons_n, inputs_n, activation_f):\n",
        "        self.neurons = []\n",
        "        for i in range(neurons_n):\n",
        "            self.neurons.append(Neuron(inputs_n,activation_f))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfHgQGLOmhZp",
        "colab_type": "text"
      },
      "source": [
        "#### Activation Function Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWWjlvHkmjxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activation:\n",
        "    \"\"\"\n",
        "    Wraps an activation function together with its derivative\n",
        "    \"\"\"\n",
        "    def __init__(self,f,fp):\n",
        "        self.evaluate = f\n",
        "        self.evaluate_derivative = fp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vigD1g4abtnS",
        "colab_type": "text"
      },
      "source": [
        "#### Feedforward Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNQt8IBhbyEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNN:\n",
        "    \"\"\"\n",
        "        Takes:\n",
        "        - Input_dim (Integer): number of input features.\n",
        "        - Hidden_dims (List of integers): where the length represents the number\n",
        "            of hidden layers, and each entry represents the number of neurons\n",
        "            in that layer.\n",
        "        - Output_dim (Integer): number of output neurons.\n",
        "        - Activation_f (activation object): activation function.\n",
        "            Activations are assigned per network for now, but can be set by hand\n",
        "            per neuron if one wishes to experiment.\n",
        "        - Eta (Float): Initial learning rate.\n",
        "        \"\"\"\n",
        "    def __init__(self,input_dim,hidden_dims,output_dim,activation_f,loss_f):\n",
        "        self.layers = []\n",
        "        self.layers.append(Layer(hidden_dims[0],input_dim,activation_f))\n",
        "        for i in range(1,len(hidden_dims)):\n",
        "            self.layers.append(Layer(hidden_dims[i],hidden_dims[i-1],activation_f))\n",
        "        self.layers.append(Layer(output_dim,hidden_dims[-1],activation_f))\n",
        "        self.loss = loss_f\n",
        "\n",
        "    # Pass an input through the system, writing output values for each neuron,\n",
        "    # and returning the output prediction\n",
        "    def forward_prop(self, inputs):\n",
        "        # Walk through layers\n",
        "        for l in self.layers:\n",
        "            activations=[]\n",
        "            # Activate each neuron and record\n",
        "            for n in l.neurons:\n",
        "                activations.append(n.activate(inputs))\n",
        "            inputs = activations\n",
        "        return inputs\n",
        "\n",
        "    # Pass the true label backwards through the network, updating delta values\n",
        "    # for each neuron, based on the product rule using the derivative of the\n",
        "    # activation functions\n",
        "    def back_prop(self, labels):\n",
        "        # Backward walk through layers\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            errors = []\n",
        "            # For intermediate layers, propagate error\n",
        "            if i != len(self.layers)-1:\n",
        "                for j in range(len(self.layers[i].neurons)):\n",
        "                    error = 0\n",
        "                    for n in self.layers[i+1].neurons:\n",
        "                        error += (n.weights[j] * n.delta)\n",
        "                    errors.append(error)\n",
        "            # For first layer, calculate loss\n",
        "            else:\n",
        "                for j in range(len(self.layers[i].neurons)):\n",
        "                    errors.append(self.loss(labels[j],self.layers[i].neurons[j].output))\n",
        "            # For all layers, set delta\n",
        "            for j in range(len(self.layers[i].neurons)):\n",
        "                self.layers[i].neurons[j].delta = errors[j] * self.layers[i].neurons[j].activation.evaluate_derivative(self.layers[i].neurons[j].output)\n",
        "\n",
        "    # Update the weights of each neurons based on the delta values and input\n",
        "    def update_weights(self, inputs, eta):\n",
        "        # Walk through layers\n",
        "        for i in range(len(self.layers)):\n",
        "            inputs = inputs[0:-1]\n",
        "            # For intermediate layers, set inputs as outputs of prev layer\n",
        "            if i != 0:\n",
        "                inputs = []\n",
        "                for n in self.layers[i-1].neurons:\n",
        "                    inputs.append(n.output)\n",
        "            # Update weights based on delta and inputs (except bias)\n",
        "            for n in self.layers[i].neurons:\n",
        "                n.weights[-1] += eta * n.delta\n",
        "                for j in range(len(inputs)):\n",
        "                    n.weights[j] += eta * n.delta * inputs[j]\n",
        "    # Run forward and back propagation for all data for each epoch\n",
        "    def fit(self,X,y,eta,epochs_n,verbose=1):\n",
        "\n",
        "        for e in range(epochs_n):\n",
        "            loss = 0\n",
        "            for i in range(len(X)):\n",
        "                outputs = self.forward_prop(X[i])\n",
        "                for j in range(len(y[i])):\n",
        "                    loss += np.abs(self.loss(y[i][j],outputs[j]))\n",
        "                self.back_prop(y[i])\n",
        "                self.update_weights(X[i], eta)\n",
        "            if verbose==1:\n",
        "                print('Epoch: %d, Loss=%.2f' % (e, loss))\n",
        "            np.random.shuffle(X)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnSv1t4fo4tM",
        "colab_type": "text"
      },
      "source": [
        "#### Sigmoid Activation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9GJihc0eas2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_p(x):\n",
        "    sx = sigmoid(x)\n",
        "    return sx*(1-sx)\n",
        "\n",
        "sigmoid_activation_f = Activation(sigmoid,sigmoid_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqd3c6JsbMQ2",
        "colab_type": "text"
      },
      "source": [
        "#### Tanh Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXMpUPLobOzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(x):\n",
        "    return np.sinh(x)/np.cosh(x)\n",
        "\n",
        "def tanh_p(x):\n",
        "    return 1-tanh(x)**2\n",
        "\n",
        "tanh_activation_f = Activation(tanh,tanh_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENgbHYNqjWU8",
        "colab_type": "text"
      },
      "source": [
        "#### Leaky Relu Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwqsAj16jWfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lrelu(x):\n",
        "    return x/20 if x<0 else x\n",
        "\n",
        "def lrelu_p(x):\n",
        "    return 1/20 if x<=0 else 1\n",
        "\n",
        "lrelu_activation_f = Activation(lrelu,lrelu_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfOCZv7Esxf6",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDRZFhSGtR4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_loss(y,y_hat):\n",
        "    # print(y,y_hat)\n",
        "    return -np.log(np.abs(1-y_hat)) if y==0 else -np.log(np.abs(y_hat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZdOUeycE_Kd",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsb46G-NE_cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data\n",
        "X_raw = []\n",
        "with open(\"train.csv\", 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        if not row:\n",
        "            continue\n",
        "        X_raw.append(row)\n",
        "del X_raw[0]\n",
        "\n",
        "# Separate out labels, discard name, ticket number, cabin and origin, \n",
        "# remove incomplete observations, destring numbers,\n",
        "# convert gender to one-hot encoding\n",
        "X = []\n",
        "y = []\n",
        "for row in X_raw:\n",
        "    if int(row[1])==0:\n",
        "        y.append([1,0])\n",
        "    else:\n",
        "        y.append([0,1])\n",
        "    del row[1],row[2],row[6],row[7],row[7]\n",
        "    processed_row=[]\n",
        "    incomplete = False\n",
        "    for i in range(len(row)):\n",
        "        if row[i]==\"\":\n",
        "            incomplete = True\n",
        "            break\n",
        "        if i in [0,1,4,5]:\n",
        "            row[i] = int(row[i])\n",
        "            processed_row.append(row[i])\n",
        "        if i in [3,6]:\n",
        "            row[i] = float(row[i])\n",
        "            processed_row.append(row[i])\n",
        "        if i==2:\n",
        "            if row[i]==\"male\":\n",
        "                processed_row.append(0)\n",
        "                processed_row.append(1)\n",
        "            else:\n",
        "                processed_row.append(1)\n",
        "                processed_row.append(0)\n",
        "    if not incomplete:\n",
        "        X.append(processed_row)\n",
        "    incomplete = False\n",
        "\n",
        "    # Train/test split\n",
        "    split = len(X)//4\n",
        "    X_train = X[split:]\n",
        "    X_test = X[:split]\n",
        "    y_train = y[split:]\n",
        "    y_test = y[:split]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjpL2rKptSGT",
        "colab_type": "text"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMATCNo5edWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = FFNN(8,[8,8,8,8,8],2,lrelu_activation_f,log_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD7uNMw89m1B",
        "colab_type": "code",
        "outputId": "8e32d6af-6bad-4789-fe2e-cadc123b5bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "nn.fit(X_train,y_train,1e-3,50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss=945.50\n",
            "Epoch: 1, Loss=792.56\n",
            "Epoch: 2, Loss=756.39\n",
            "Epoch: 3, Loss=690.52\n",
            "Epoch: 4, Loss=603.64\n",
            "Epoch: 5, Loss=574.78\n",
            "Epoch: 6, Loss=552.89\n",
            "Epoch: 7, Loss=543.16\n",
            "Epoch: 8, Loss=533.27\n",
            "Epoch: 9, Loss=527.34\n",
            "Epoch: 10, Loss=523.59\n",
            "Epoch: 11, Loss=520.38\n",
            "Epoch: 12, Loss=517.33\n",
            "Epoch: 13, Loss=514.79\n",
            "Epoch: 14, Loss=513.00\n",
            "Epoch: 15, Loss=512.01\n",
            "Epoch: 16, Loss=511.22\n",
            "Epoch: 17, Loss=511.14\n",
            "Epoch: 18, Loss=508.93\n",
            "Epoch: 19, Loss=508.33\n",
            "Epoch: 20, Loss=507.64\n",
            "Epoch: 21, Loss=508.21\n",
            "Epoch: 22, Loss=507.95\n",
            "Epoch: 23, Loss=507.64\n",
            "Epoch: 24, Loss=506.05\n",
            "Epoch: 25, Loss=506.66\n",
            "Epoch: 26, Loss=506.50\n",
            "Epoch: 27, Loss=506.36\n",
            "Epoch: 28, Loss=505.97\n",
            "Epoch: 29, Loss=506.11\n",
            "Epoch: 30, Loss=506.38\n",
            "Epoch: 31, Loss=505.46\n",
            "Epoch: 32, Loss=505.71\n",
            "Epoch: 33, Loss=505.48\n",
            "Epoch: 34, Loss=505.77\n",
            "Epoch: 35, Loss=505.56\n",
            "Epoch: 36, Loss=505.38\n",
            "Epoch: 37, Loss=505.61\n",
            "Epoch: 38, Loss=505.43\n",
            "Epoch: 39, Loss=505.44\n",
            "Epoch: 40, Loss=504.96\n",
            "Epoch: 41, Loss=505.40\n",
            "Epoch: 42, Loss=504.66\n",
            "Epoch: 43, Loss=504.32\n",
            "Epoch: 44, Loss=505.71\n",
            "Epoch: 45, Loss=505.44\n",
            "Epoch: 46, Loss=504.45\n",
            "Epoch: 47, Loss=504.87\n",
            "Epoch: 48, Loss=505.05\n",
            "Epoch: 49, Loss=505.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GzLAJ_qE4J",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu9bebQjeq1l",
        "colab_type": "code",
        "outputId": "c06d0922-aeaa-41db-b272-95de046ba747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "n = len(X_test)\n",
        "T=0\n",
        "P=0\n",
        "FP = 0\n",
        "TP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "for i in range(n):\n",
        "    output = nn.forward_prop(X_test[i])\n",
        "    decision = np.argmax(output)\n",
        "    true = False\n",
        "    positive = False\n",
        "    if y_test[i][1]==1:\n",
        "        true = True\n",
        "        T+=1\n",
        "    if y_test[i][decision] == 1:\n",
        "        positive = True\n",
        "        P+=0\n",
        "    if true and positive:\n",
        "        TP+=1\n",
        "    if true and not positive:\n",
        "        TN+=1\n",
        "    if positive and not true:\n",
        "        FP+=1\n",
        "    if not positive and not true:\n",
        "        FN+=1\n",
        "print(\"Precision: \",TP/(TP+FP))\n",
        "print(\"Recall: \",TP/T)\n",
        "print(\"Accuracy: \",(TN+TP)/n)\n",
        "print(n,T,P)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  1.0\n",
            "Recall:  1.0\n",
            "Accuracy:  0.33146067415730335\n",
            "178 59 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}