{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantum CodeGen.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N1FZLzbfzVU",
        "colab_type": "text"
      },
      "source": [
        "#### Tutorial Joint Final Project: Deep Learning and Quantum Information Theory\n",
        "\n",
        "### Quantum Code Generation using Conditional RNNs\n",
        "_Yoav Rabinovich, December 2019_\n",
        "\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsYAQ-ONbD4S",
        "colab_type": "text"
      },
      "source": [
        "For this final project I trained Conditional RNNs to generate quantum algorithms based on a specified target state for an $n$-qubit circuit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIC-tAGDk1VG",
        "colab_type": "text"
      },
      "source": [
        "#### Background: Quantum Computing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO8D5GR5k3vR",
        "colab_type": "text"
      },
      "source": [
        "Quantum computers employ a gate model of computing, analogous to bitwise operations on a bitstring: From a set of $n$-qubits initialized to state 0, we get to a target state for the system by applying gates on one or two qubits at a time.\n",
        "\n",
        "**Statespace**: Unlike the state of a bitstring, which can be described by definition using $n$ bits, an $n$-qubit system is described by a set complex amplitudes that correspond to the probabilities to measure the system in any configuration of $n$ bits, leading to a statespace of $2^n$ complex numbers, or $2^{n+1}$ floats. These complex amplitudes are achieved through the ability of qubits to be put into superpositions of the 0 and 1 computational states, as well as the ability of two qubits to be entangled, such that the state describing their 2-qubit system is not separable into two 1-qubit states. However, this is not pertinent to explain our implementation.\n",
        "\n",
        "**Gates:** There are infinitely many sets of possible gates that can map the initial state into any state in the statespace of an $n$-qubit system. For this implementation, I made a choice of set, but in principle the model can be trained for any set of gates. The gates included in this implementation are the $H$ gate, which puts qubits in and out of superpositions of computational states; the $S$ gate which shifts superpositions but doesn't act on computational states; and the $CNOT$ gate which puts pairs of qubits into entangled states by flipping one target qubit depending on the state of another control qubit.\n",
        "\n",
        "**QisKit:** Using IBM's QisKit API, along with the QASM format for circuit specification, we are able to both sample random circuits by sampling gates and attaching them to random qubits in a sequence, and to simulate circuits to find the output states they yield when fed an initialized state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aULD6jVk7cm",
        "colab_type": "text"
      },
      "source": [
        "#### Background: Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfL4Hm1k-Gh",
        "colab_type": "text"
      },
      "source": [
        "**RNNs:** Recurrent neural networks generate a language-model for a dataset of sequences, learning to predict the next element in a sequence given the preceding elements in a computationally parallelizable way. In a stacked (or \"deep\") RNN model, RNN layers are fed into each other in a sequence, each layer containing several computational units. These units, such as the GRU used in this implementation, can learn to retain important information about the long-range structure of sequences, allowing them to draw on the entire preceding sequence to generate a prediction.\n",
        "\n",
        "**Conditional RNNs:** However, RNNs lack a mechanism by which to consider non-temporal information in the correlations they learn. A conditional RNN attemps to force the network to output different predictions based on conditions external to the preceding sequence. There are three overall schemes to construct \"conditional\" RNNs in the literature:\n",
        "\n",
        "1. Have the first timestep contain conditioning data, to set the internal state of your RNN. This is similar to what happens in encoder-decoder networks, which encode sequences into an input element which is then interepreted by a dedicated decoder, which is a \"stateful\" RNN, feeding the prediction of a cell as input to the next until an end-of-sentece token is predicted. However, this restricts the conditioning data to the shape of an element in the sequence, and also doesn't guarantee that the network retains the information as prediction continues.\n",
        "\n",
        "2. Append the condition to the temporal input, such that each feature vector includes both the time series data and a constant condition vector. However, this approach pollutes the data and requires the network to learn the two different types of correlations itself, which is inefficient.\n",
        "\n",
        "3. Finally, we can directly affect the intenral state of the RNN at time zero. For each training sample, we apply a dense layer to reshape the condition data into the right shape as the internal state of the RNN, and for the first timestep, we add the result to the internal state of the RNN directly. This cannot be implemented naturally with Keras, so I found a package that implements this behavior in Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSF1AU6aazFa",
        "colab_type": "text"
      },
      "source": [
        "#### Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3U8SBAha1hY",
        "colab_type": "text"
      },
      "source": [
        "I train the model to predict the next gate in a circuit given previous gates, and given an external condition set to an output state of the circuit. This way, I hope to be able to input a target state as a condition for the trained network to generate a circuit that will reach it.\n",
        "\n",
        "I have built two similar models, with a difference only in the circuit encodig scheme: One has the gates encoded as integers indexing a vocabulary, while the other has the integers translated to one-hot encoding first. The latter seems preferable, since it doesn't posit any linear relationship between elements close to each other in the vocabulary, but unfortunately neither performed well under the conditions they were trained.\n",
        "\n",
        "**Data Synthesis:** I use QisKit to sample a dataset of random $n$-qubit circuits of a chosen length, and encode them in a vocabulary scheme described in the code comments, such that every possible gate application on any qubit is encoded with a different index. I then simulate each circuit to produce output states to use as conditions while training. To train the network to predict the next gate in a sequence, I simply offset the input dataset by one gate to create the target dataset.\n",
        "\n",
        "**Model Structure:**\n",
        "The inputs are fed into a stacked Conditional RNN with GRU units, the outputs of which are then passed through a Dense layer for classification and prediction. In the case of the integer encoding, the model's optimized according to mean squared error loss, and under the one-hot scheme I use cross-entropy.\n",
        "\n",
        "**Results:** Both models learn the rules regarding \"end-of-sequence\" and padding tokens very quickly, but the training is slow and the results are bad. As expected, the one-hot encoded model performs better than the integer scheme, and when its outputs are simulated seems to be able to match some of the target amplitudes in the condition state, but the model only outputs very simple circuits that don't include $CNOT$ gates which are essential for universal computation. In fact, this means the model ignores most of the entire vocabulary available to it. This might be solved by longer training, but I can't confirm this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81HTxP78eRyk",
        "colab_type": "text"
      },
      "source": [
        "#### Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byyx3cLjeT8Q",
        "colab_type": "text"
      },
      "source": [
        "I hoped that the conditional RNN layers would be able to learn correlations between the conditions and sequences of applied gates, since the elements in the sequence were sampled uniformly and independently of each other and should result in no information for the model to use other than the condition-sequence correlations. However, this is a problem where the space of sequences is much larger than the space of condition states, which might confuse the network. A dataset of more intelligently constructed algorithms might also teach the network some best-practices that will help it converge on a correct sequence.\n",
        "\n",
        "Another avenue to explore is models with the ability to asses their progress towards their goal as they predict elements of the sequence: where the loss function is derived from the output of the sequence constructed. However, simulating quantum computers, while deterministic, requires the use of external packages employing numerical models, which is not differentiable. Other than rewriting a differentiable Quantum simulator in TensorFlow, I could turn instead to reinforcement learning. I stayed away from this avenus since I was afraid that simulation during each step in the training will take to long to be feasible, but in the long run it might be a profitable path to take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNdghp0rfvQd",
        "colab_type": "text"
      },
      "source": [
        "#### Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZaX7qNKHD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install qiskit\n",
        "!pip install tensorflow-gpu --upgrade\n",
        "!pip install cond_rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ycOxIXKsKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import qiskit as qk\n",
        "qs_backend = qk.Aer.get_backend('statevector_simulator')\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import cond_rnn as crnn\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG_UWsozhs3i",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing and Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPgC4Wa8Ky4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_circuits(n,size,amount):\n",
        "    \"\"\"Sample an amount of random n-qubit circuits with a certain size in\n",
        "    number of operations from the allowed set\"\"\"\n",
        "\n",
        "    circuits =[]\n",
        "    for _ in range(amount):\n",
        "        # Create circuit object of n qubits\n",
        "        circ = qk.QuantumCircuit(n)\n",
        "        # Generate random gates on random qubits from the universal set {H,S,CX}\n",
        "        for _ in range(size):\n",
        "            gate = np.random.randint(0,3)\n",
        "            target = np.random.randint(0,n)\n",
        "            if gate==0: # Hadamard\n",
        "                circ.h(target)\n",
        "            if gate==1: # S-gate\n",
        "                circ.s(target)\n",
        "            if gate==2: # CNOT\n",
        "                control = np.random.randint(0,n)\n",
        "                if control == target:\n",
        "                    circ.h(target)\n",
        "                else:\n",
        "                    circ.cx(control,target)\n",
        "        circuits.append(circ)\n",
        "    return circuits\n",
        "\n",
        "def generate_labels(circuits,encoded=True):\n",
        "    \"\"\"Simulate each in an array of circuits, and return the resultant state.\n",
        "    The state can be encoded as an array of size 2*(2^n) where the real and imaginary\n",
        "    components of each amplitude are concatenated.\"\"\"\n",
        "\n",
        "    global qs_backend\n",
        "    labels=[]\n",
        "    for circ in circuits:\n",
        "        # Simulate each circuit and retrieve final quantum state\n",
        "        job = qk.execute(circ, qs_backend)\n",
        "        outputstate = job.result().get_statevector(circ, decimals=3)\n",
        "        if encoded:\n",
        "            # encode complex amplitudes as flattened arrays\n",
        "            separated = []\n",
        "            separated.append(outputstate.real)\n",
        "            separated.append(outputstate.imag)\n",
        "            outputstate = separated\n",
        "        labels.append(np.array(outputstate).flatten())\n",
        "    return np.array(labels)\n",
        "\n",
        "def encode_circuits(circuits,n,max_size,label=True):\n",
        "    \"\"\"Takes an array of n-qubit QuantumCircuit objects, and encodes them based on a\n",
        "    vocabulary of possible gates to apply, including tokens to signify the start\n",
        "    and end of sequences. Elements after EoS are padded to match maximum circuit\n",
        "    size using a special token.\n",
        "    Labels can be also be generated for the circuits.\"\n",
        "\n",
        "    Vocabulary scheme:\n",
        "    Padding = 0,\n",
        "    SoS = 1,\n",
        "    EoS = 2,\n",
        "    h[0]=3, h[1]=3+1...\n",
        "    s[0]=3+n, s[1]=3+n+1...\n",
        "    cx[0,0]=3+2n, cx[0,1]=3+2n+1...\n",
        "    cx[1,0]=3+(2+1)n, cx[1,1]=3+(2+1)n+1... etc. \"\"\"\n",
        "\n",
        "    encoded = []\n",
        "    for circ in circuits:\n",
        "        # Use the QASM format to convert the circuit to a string\n",
        "        lines = circ.qasm().splitlines()[3:]\n",
        "        size = len(lines)\n",
        "        # Initialize to padding tokens\n",
        "        encoded_circ = np.zeros(max_size+2)\n",
        "        # Add SoS and EoS tokens\n",
        "        encoded_circ[0] = 1\n",
        "        encoded_circ[size+1]=2\n",
        "        for i,line in enumerate(lines):\n",
        "            # Detect gate name and qubits involved\n",
        "            gate_str = line[:2]\n",
        "            integers = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*',line)]\n",
        "            # Encode gates based on scheme above\n",
        "            if gate_str==\"h \":\n",
        "                encoded_circ[i+1]=int(3+integers[0])\n",
        "            if gate_str==\"s \":\n",
        "                encoded_circ[i+1]=int(3+n+integers[0])\n",
        "            if gate_str==\"cx\":\n",
        "                encoded_circ[i+1]=int(3+(2+integers[0])*n+integers[1])\n",
        "        encoded.append(encoded_circ)\n",
        "    encoded = np.array(encoded)\n",
        "    if label:\n",
        "        # Simulate labels for each circuit and attach to dataset\n",
        "        labels = generate_labels(circuits)\n",
        "        return np.concatenate((encoded,labels),axis=1)\n",
        "    else:\n",
        "        return np.array(encoded)\n",
        "\n",
        "def decode_circuit(encoded,n):\n",
        "    \"\"\"Takes an encoded output from the network and generates the corresponding\n",
        "    circuit as described above.\"\"\"\n",
        "\n",
        "    # Start with opening syntax\n",
        "    decoded = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[\"+str(n)+\"];\\n\"\n",
        "    for line in encoded:\n",
        "        # decode each non-token element into its QASM string\n",
        "        line = int(line)\n",
        "        if line > 2:\n",
        "            gate_num = int(np.ceil((line-2)/n))\n",
        "            if gate_num==1:\n",
        "                decoded += \"h q[\"+str(line-3)\n",
        "            elif gate_num==2:\n",
        "                decoded += \"s q[\"+str(line-n-3)\n",
        "            else:\n",
        "                decoded += \"cx q[\"+str(gate_num-3)+\"],q[\"+str(line-(gate_num-1)*n-3)\n",
        "            decoded += \"];\\n\"\n",
        "        if line == 2:\n",
        "            decoded = decoded[:-1]\n",
        "    # print(decoded)\n",
        "    # Build circuit object from QASM string\n",
        "    return qk.QuantumCircuit.from_qasm_str(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzIg4aOrfqhI",
        "colab_type": "text"
      },
      "source": [
        "#### Demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQEY6ukkH9rm",
        "colab_type": "text"
      },
      "source": [
        "We demonstrate our ability to sample algorithms of arbitrary dimensions, encode them correctly using our vocabulary scheme, and decode them exactly to their original circuit objects (up to allowable swaps in non-interacting gates)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9zaYBzcfqCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "max_size = 30\n",
        "\n",
        "# Sample, Encode and Label circuits\n",
        "sampled_circuits = sample_circuits(n, max_size, 20)\n",
        "encoded_circuits = encode_circuits(sampled_circuits, n, max_size, label=False)\n",
        "labels = generate_labels(sampled_circuits, encoded=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oAPxQPcIJ9a",
        "colab_type": "code",
        "outputId": "5da590d6-5237-4e69-daab-3684d1a73520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Graph sample circuit\n",
        "samp = sampled_circuits[0]\n",
        "samp.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐     ┌───┐┌───┐┌───┐                         \n",
              "q_0: |0>┤ H ├─────┤ X ├┤ S ├┤ S ├─────────────────────────\n",
              "        ├───┤┌───┐└─┬─┘├───┤└───┘                         \n",
              "q_1: |0>┤ H ├┤ H ├──┼──┤ H ├──────────────────────────────\n",
              "        ├───┤└───┘  │  └───┘┌───┐┌───┐┌───┐               \n",
              "q_2: |0>┤ H ├───────┼───────┤ X ├┤ S ├┤ S ├───────────────\n",
              "        ├───┤       │       └─┬─┘└───┘└───┘               \n",
              "q_3: |0>┤ H ├───────┼─────────┼───────────────────────────\n",
              "        ├───┤┌───┐  │  ┌───┐  │            ┌───┐┌───┐┌───┐\n",
              "q_4: |0>┤ H ├┤ X ├──┼──┤ S ├──■─────────■──┤ X ├┤ H ├┤ S ├\n",
              "        ├───┤└─┬─┘  │  ├───┤            │  └─┬─┘└───┘└───┘\n",
              "q_5: |0>┤ H ├──┼────┼──┤ H ├────────────┼────┼────────────\n",
              "        ├───┤  │    │  ├───┤┌───┐┌───┐┌─┴─┐  │            \n",
              "q_6: |0>┤ S ├──■────┼──┤ X ├┤ H ├┤ S ├┤ X ├──┼────────────\n",
              "        ├───┤       │  └─┬─┘└───┘└───┘└───┘  │            \n",
              "q_7: |0>┤ H ├───────┼────┼───────────────────┼────────────\n",
              "        └───┘       │    │  ┌───┐            │            \n",
              "q_8: |0>────────────┼────■──┤ H ├────────────┼────────────\n",
              "        ┌───┐       │  ┌───┐├───┤            │            \n",
              "q_9: |0>┤ H ├───────■──┤ S ├┤ S ├────────────■────────────\n",
              "        └───┘          └───┘└───┘                         </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7fab8fc7c0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGtS9TQuIWDO",
        "colab_type": "code",
        "outputId": "4a1073f2-0bcf-4f77-c7ae-07ad5a688b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "# Present QASM string and encoded representation\n",
        "print(samp.qasm())\n",
        "enc = encoded_circuits[0]\n",
        "print(enc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPENQASM 2.0;\n",
            "include \"qelib1.inc\";\n",
            "qreg q[10];\n",
            "s q[6];\n",
            "h q[4];\n",
            "h q[9];\n",
            "cx q[6],q[4];\n",
            "cx q[8],q[6];\n",
            "h q[2];\n",
            "h q[3];\n",
            "h q[0];\n",
            "h q[5];\n",
            "s q[4];\n",
            "h q[1];\n",
            "cx q[9],q[0];\n",
            "h q[5];\n",
            "h q[7];\n",
            "cx q[4],q[2];\n",
            "s q[0];\n",
            "s q[2];\n",
            "s q[9];\n",
            "h q[6];\n",
            "s q[6];\n",
            "h q[1];\n",
            "cx q[4],q[6];\n",
            "s q[2];\n",
            "s q[0];\n",
            "h q[8];\n",
            "s q[9];\n",
            "cx q[9],q[4];\n",
            "h q[4];\n",
            "s q[4];\n",
            "h q[1];\n",
            "\n",
            "[  1.  19.   7.  12.  87. 109.   5.   6.   3.   8.  17.   4. 113.   8.\n",
            "  10.  65.  13.  15.  22.   9.  19.   4.  69.  15.  13.  11.  22. 117.\n",
            "   7.  17.   4.   2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnUY7qzhIuNA",
        "colab_type": "code",
        "outputId": "673d8af0-d199-4088-ab92-0db5486de3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Decode the encoded representation and graph it to demonstrate similarity\n",
        "dec = decode_circuit(enc,n)\n",
        "dec.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐          ┌───┐┌───┐┌───┐                         \n",
              "q_0: |0>┤ H ├──────────┤ X ├┤ S ├┤ S ├─────────────────────────\n",
              "        ├───┤┌───┐┌───┐└─┬─┘└───┘└───┘                         \n",
              "q_1: |0>┤ H ├┤ H ├┤ H ├──┼─────────────────────────────────────\n",
              "        ├───┤└───┘└───┘  │       ┌───┐┌───┐┌───┐               \n",
              "q_2: |0>┤ H ├────────────┼───────┤ X ├┤ S ├┤ S ├───────────────\n",
              "        ├───┤            │       └─┬─┘└───┘└───┘               \n",
              "q_3: |0>┤ H ├────────────┼─────────┼───────────────────────────\n",
              "        ├───┤     ┌───┐  │  ┌───┐  │            ┌───┐┌───┐┌───┐\n",
              "q_4: |0>┤ H ├─────┤ X ├──┼──┤ S ├──■─────────■──┤ X ├┤ H ├┤ S ├\n",
              "        ├───┤┌───┐└─┬─┘  │  └───┘            │  └─┬─┘└───┘└───┘\n",
              "q_5: |0>┤ H ├┤ H ├──┼────┼───────────────────┼────┼────────────\n",
              "        ├───┤└───┘  │    │  ┌───┐┌───┐┌───┐┌─┴─┐  │            \n",
              "q_6: |0>┤ S ├───────■────┼──┤ X ├┤ H ├┤ S ├┤ X ├──┼────────────\n",
              "        ├───┤            │  └─┬─┘└───┘└───┘└───┘  │            \n",
              "q_7: |0>┤ H ├────────────┼────┼───────────────────┼────────────\n",
              "        └───┘            │    │  ┌───┐            │            \n",
              "q_8: |0>─────────────────┼────■──┤ H ├────────────┼────────────\n",
              "        ┌───┐            │  ┌───┐├───┤            │            \n",
              "q_9: |0>┤ H ├────────────■──┤ S ├┤ S ├────────────■────────────\n",
              "        └───┘               └───┘└───┘                         </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7faaf22cdc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYlmYm12dixQ",
        "colab_type": "text"
      },
      "source": [
        "#### Vocabulary Circuit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jufgi7NEbyNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "N = 5\n",
        "VOCAB_DIM = 2+2*N+N**2\n",
        "NUM_SAMPLES = 100000\n",
        "TRAINTEST = 0.75\n",
        "CUTOFF = int(NUM_SAMPLES//2)#(1/TRAINTEST))\n",
        "MAX_LENGTH = 10\n",
        "TIME_STEPS = MAX_LENGTH+2\n",
        "INPUT_DIM = 1\n",
        "COND_DIM = 2*(2**N)\n",
        "NUM_CELLS = 256\n",
        "STACK_DEPTH = 5\n",
        "PRINT_DELAY = 100\n",
        "SAVE_DELAY = 10000\n",
        "\n",
        "# Training Parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C-Q6inddm5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Data\n",
        "# Data = encode_circuits(sample_circuits(N,MAX_LENGTH,NUM_SAMPLES),N,MAX_LENGTH,label=True)\n",
        "# Save Data\n",
        "# np.savetxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", Data, delimiter=\",\")\n",
        "# Load Data\n",
        "# Data = np.loadtxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", delimiter=\",\")\n",
        "np.random.shuffle(Data)\n",
        "X = Data[:,:TIME_STEPS]\n",
        "X = X.reshape((X.shape[0],X.shape[1],1))\n",
        "c = Data[:,TIME_STEPS:]\n",
        "offset = X[:,1:]\n",
        "y = np.concatenate((offset,np.zeros((X.shape[0],1,1))),axis=1)\n",
        "X_train = X[:CUTOFF]\n",
        "X_test = X[CUTOFF:]\n",
        "c_test = c[:CUTOFF]\n",
        "c_train = c[CUTOFF:]\n",
        "y_train = y[:CUTOFF]\n",
        "y_test = y[CUTOFF:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6jX8x5_c7eA",
        "colab_type": "code",
        "outputId": "11e7b977-edd7-4271-9b71-3b5021c70152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Placeholders.\n",
        "inputs = tf.placeholder(name='inputs', dtype=tf.float32, shape=(None, TIME_STEPS, INPUT_DIM))\n",
        "targets = tf.placeholder(name='targets', dtype=tf.float32, shape=(None, TIME_STEPS, INPUT_DIM))\n",
        "cond = tf.placeholder(name='conditions', dtype=tf.float32, shape=(None, COND_DIM))\n",
        "\n",
        "# Conditional RNN.\n",
        "outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "for _ in range(STACK_DEPTH-1):\n",
        "    outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "\n",
        "# Classification layer.\n",
        "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1, activation='relu'))(outputs)\n",
        "\n",
        "# Loss + Optimizer.\n",
        "cost = tf.reduce_sum(tf.reduce_mean(tf.squared_difference(outputs, targets)))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Initialize variables (tensorflow)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Define the binding between placeholders and real data.\n",
        "train_feed_dict = {inputs: X_train, targets: y_train, cond: c_train}\n",
        "test_feed_dict = {inputs: X_test, targets: y_test, cond: c_test}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v7CoDbby89K",
        "colab_type": "code",
        "outputId": "b50ab750-5f9e-4f0e-a52d-bcaddb4886bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Main loop. Optimize then evaluate.\n",
        "saver = tf.train.Saver()\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    sess.run(optimizer, train_feed_dict)\n",
        "    if epoch % PRINT_DELAY == 0:\n",
        "        train_outputs, train_loss = sess.run([outputs, cost], train_feed_dict)\n",
        "        test_outputs, test_loss = sess.run([outputs, cost], test_feed_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f'[{str(epoch).zfill(4)}] train cost = {train_loss:.4f}, test cost = {test_loss:.4f}.')\n",
        "    if epoch % SAVE_DELAY == 0:\n",
        "        saver.save(sess, f'Checkpoints/QCG-{str(NUM_CELLS)}', global_step=int(epoch/SAVE_DELAY))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0000] train cost = 173.6090, test cost = 173.7048.\n",
            "[0100] train cost = 68.7404, test cost = 68.7548.\n",
            "[0200] train cost = 66.8745, test cost = 66.9583.\n",
            "[0300] train cost = 66.6586, test cost = 66.7598.\n",
            "[0400] train cost = 66.6138, test cost = 66.7233.\n",
            "[0500] train cost = 66.5873, test cost = 66.7125.\n",
            "[0600] train cost = 66.5673, test cost = 66.7099.\n",
            "[0700] train cost = 70.7206, test cost = 70.7812.\n",
            "[0800] train cost = 67.2028, test cost = 67.2783.\n",
            "[0900] train cost = 66.7442, test cost = 66.8253.\n",
            "[1000] train cost = 66.6275, test cost = 66.7237.\n",
            "[1100] train cost = 66.5869, test cost = 66.6958.\n",
            "[1200] train cost = 66.5700, test cost = 66.6900.\n",
            "[1300] train cost = 66.5602, test cost = 66.6912.\n",
            "[1400] train cost = 66.5518, test cost = 66.6949.\n",
            "[1500] train cost = 75.5182, test cost = 75.6060.\n",
            "[1600] train cost = 66.6130, test cost = 66.7421.\n",
            "[1700] train cost = 66.5618, test cost = 66.7066.\n",
            "[1800] train cost = 66.5470, test cost = 66.7045.\n",
            "[1900] train cost = 66.5366, test cost = 66.7071.\n",
            "[2000] train cost = 66.5283, test cost = 66.7114.\n",
            "[2100] train cost = 66.5205, test cost = 66.7164.\n",
            "[2200] train cost = 66.5126, test cost = 66.7221.\n",
            "[2300] train cost = 66.5039, test cost = 66.7285.\n",
            "[2400] train cost = 66.4941, test cost = 66.7352.\n",
            "[2500] train cost = 66.4823, test cost = 66.7421.\n",
            "[2600] train cost = 66.4685, test cost = 66.7490.\n",
            "[2700] train cost = 71.9786, test cost = 72.1031.\n",
            "[2800] train cost = 66.9791, test cost = 67.0924.\n",
            "[2900] train cost = 66.6259, test cost = 66.7554.\n",
            "[3000] train cost = 66.5789, test cost = 66.7222.\n",
            "[3100] train cost = 66.5568, test cost = 66.7174.\n",
            "[3200] train cost = 66.5380, test cost = 66.7236.\n",
            "[3300] train cost = 66.5923, test cost = 66.7423.\n",
            "[3400] train cost = 66.5495, test cost = 66.7187.\n",
            "[3500] train cost = 66.5307, test cost = 66.7254.\n",
            "[3600] train cost = 66.5143, test cost = 66.7339.\n",
            "[3700] train cost = 66.4980, test cost = 66.7435.\n",
            "[3800] train cost = 66.4798, test cost = 66.7544.\n",
            "[3900] train cost = 76.1942, test cost = 76.3451.\n",
            "[4000] train cost = 66.5505, test cost = 66.7435.\n",
            "[4100] train cost = 66.4935, test cost = 66.7485.\n",
            "[4200] train cost = 66.4634, test cost = 66.7629.\n",
            "[4300] train cost = 66.4406, test cost = 66.7789.\n",
            "[4400] train cost = 66.4177, test cost = 66.7934.\n",
            "[4500] train cost = 83.0891, test cost = 83.2469.\n",
            "[4600] train cost = 66.6334, test cost = 66.8405.\n",
            "[4700] train cost = 66.4653, test cost = 66.7751.\n",
            "[4800] train cost = 66.4112, test cost = 66.7966.\n",
            "[4900] train cost = 66.7561, test cost = 66.9362.\n",
            "[5000] train cost = 66.5252, test cost = 66.7335.\n",
            "[5100] train cost = 66.3593, test cost = 66.8508.\n",
            "[5200] train cost = 66.6338, test cost = 66.8234.\n",
            "[5300] train cost = 66.4918, test cost = 66.8176.\n",
            "[5400] train cost = 66.4214, test cost = 66.8402.\n",
            "[5500] train cost = 66.3685, test cost = 66.8497.\n",
            "[5600] train cost = 66.3443, test cost = 66.8437.\n",
            "[5700] train cost = 66.3418, test cost = 66.8408.\n",
            "[5800] train cost = 66.4388, test cost = 66.8052.\n",
            "[5900] train cost = 66.3237, test cost = 66.8569.\n",
            "[6000] train cost = 66.2788, test cost = 66.8898.\n",
            "[6100] train cost = 66.2513, test cost = 66.9131.\n",
            "[6200] train cost = 75.0706, test cost = 75.3043.\n",
            "[6300] train cost = 66.3810, test cost = 66.8896.\n",
            "[6400] train cost = 66.2645, test cost = 66.8844.\n",
            "[6500] train cost = 66.5797, test cost = 66.8498.\n",
            "[6600] train cost = 66.4026, test cost = 66.8008.\n",
            "[6700] train cost = 67.0005, test cost = 67.3338.\n",
            "[6800] train cost = 66.1982, test cost = 66.9605.\n",
            "[6900] train cost = 66.1514, test cost = 66.9973.\n",
            "[7000] train cost = 66.1334, test cost = 67.0524.\n",
            "[7100] train cost = 66.1068, test cost = 67.0809.\n",
            "[7200] train cost = 66.0787, test cost = 67.1085.\n",
            "[7300] train cost = 66.0486, test cost = 67.1373.\n",
            "[7400] train cost = 66.0162, test cost = 67.1671.\n",
            "[7500] train cost = 66.2967, test cost = 67.0563.\n",
            "[7600] train cost = 66.1474, test cost = 66.9641.\n",
            "[7700] train cost = 65.9478, test cost = 67.1239.\n",
            "[7800] train cost = 65.8884, test cost = 67.1657.\n",
            "[7900] train cost = 65.9171, test cost = 67.1073.\n",
            "[8000] train cost = 65.8117, test cost = 67.2697.\n",
            "[8100] train cost = 65.7620, test cost = 67.3287.\n",
            "[8200] train cost = 65.9196, test cost = 67.3996.\n",
            "[8300] train cost = 65.6779, test cost = 67.2822.\n",
            "[8400] train cost = 65.8734, test cost = 67.2675.\n",
            "[8500] train cost = 65.7677, test cost = 67.7547.\n",
            "[8600] train cost = 65.3785, test cost = 67.5536.\n",
            "[8700] train cost = 65.4270, test cost = 67.7911.\n",
            "[8800] train cost = 65.2569, test cost = 67.6656.\n",
            "[8900] train cost = 65.0125, test cost = 67.8766.\n",
            "[9000] train cost = 64.8508, test cost = 68.0271.\n",
            "[9100] train cost = 64.6643, test cost = 68.1798.\n",
            "[9200] train cost = 64.4581, test cost = 68.3405.\n",
            "[9300] train cost = 64.2829, test cost = 68.4574.\n",
            "[9400] train cost = 63.9875, test cost = 68.7907.\n",
            "[9500] train cost = 63.9358, test cost = 69.1871.\n",
            "[9600] train cost = 63.6163, test cost = 69.3940.\n",
            "[9700] train cost = 63.3020, test cost = 69.5366.\n",
            "[9800] train cost = 63.5485, test cost = 69.6287.\n",
            "[9900] train cost = 62.7646, test cost = 70.1235.\n",
            "[10000] train cost = 62.6540, test cost = 70.5787.\n",
            "[10100] train cost = 62.5233, test cost = 70.6878.\n",
            "[10200] train cost = 62.0213, test cost = 71.0275.\n",
            "[10300] train cost = 62.1965, test cost = 71.2857.\n",
            "[10400] train cost = 61.6523, test cost = 71.7923.\n",
            "[10500] train cost = 61.2989, test cost = 72.2254.\n",
            "[10600] train cost = 61.0995, test cost = 72.5527.\n",
            "[10700] train cost = 60.8154, test cost = 72.9279.\n",
            "[10800] train cost = 60.6300, test cost = 73.2115.\n",
            "[10900] train cost = 60.3966, test cost = 73.6749.\n",
            "[11000] train cost = 60.2099, test cost = 73.6805.\n",
            "[11100] train cost = 59.9412, test cost = 74.0274.\n",
            "[11200] train cost = 59.6661, test cost = 74.6463.\n",
            "[11300] train cost = 59.4511, test cost = 75.0700.\n",
            "[11400] train cost = 59.2217, test cost = 75.6436.\n",
            "[11500] train cost = 59.1988, test cost = 75.6743.\n",
            "[11600] train cost = 58.9199, test cost = 76.3628.\n",
            "[11700] train cost = 58.7412, test cost = 76.4787.\n",
            "[11800] train cost = 58.6220, test cost = 77.0914.\n",
            "[11900] train cost = 58.2503, test cost = 77.4309.\n",
            "[12000] train cost = 58.1824, test cost = 77.3359.\n",
            "[12100] train cost = 57.8429, test cost = 78.3291.\n",
            "[12200] train cost = 57.6880, test cost = 78.2142.\n",
            "[12300] train cost = 57.6474, test cost = 79.1575.\n",
            "[12400] train cost = 57.4427, test cost = 79.3183.\n",
            "[12500] train cost = 57.9719, test cost = 79.7263.\n",
            "[12600] train cost = 57.7802, test cost = 80.3346.\n",
            "[12700] train cost = 57.0933, test cost = 79.6051.\n",
            "[12800] train cost = 56.6690, test cost = 80.1563.\n",
            "[12900] train cost = 56.4690, test cost = 80.5950.\n",
            "[13000] train cost = 56.3742, test cost = 81.1635.\n",
            "[13100] train cost = 56.1153, test cost = 81.5419.\n",
            "[13200] train cost = 56.2897, test cost = 81.0582.\n",
            "[13300] train cost = 55.8097, test cost = 82.0840.\n",
            "[13400] train cost = 56.0092, test cost = 82.2729.\n",
            "[13500] train cost = 55.8400, test cost = 82.8880.\n",
            "[13600] train cost = 55.5933, test cost = 83.5136.\n",
            "[13700] train cost = 55.4023, test cost = 83.1158.\n",
            "[13800] train cost = 55.2440, test cost = 83.6280.\n",
            "[13900] train cost = 55.1107, test cost = 84.1006.\n",
            "[14000] train cost = 55.4708, test cost = 83.2889.\n",
            "[14100] train cost = 54.8108, test cost = 84.5324.\n",
            "[14200] train cost = 54.5504, test cost = 85.2169.\n",
            "[14300] train cost = 54.7828, test cost = 85.2116.\n",
            "[14400] train cost = 55.8596, test cost = 86.1470.\n",
            "[14500] train cost = 55.2398, test cost = 86.0693.\n",
            "[14600] train cost = 54.9511, test cost = 84.8475.\n",
            "[14700] train cost = 54.1553, test cost = 86.9651.\n",
            "[14800] train cost = 54.4691, test cost = 87.8166.\n",
            "[14900] train cost = 53.9700, test cost = 86.7381.\n",
            "[15000] train cost = 53.6003, test cost = 87.5199.\n",
            "[15100] train cost = 53.7410, test cost = 87.1136.\n",
            "[15200] train cost = 54.4107, test cost = 88.1957.\n",
            "[15300] train cost = 53.8256, test cost = 87.0401.\n",
            "[15400] train cost = 53.3679, test cost = 87.9634.\n",
            "[15500] train cost = 53.3211, test cost = 88.9966.\n",
            "[15600] train cost = 54.2010, test cost = 88.6751.\n",
            "[15700] train cost = 53.7036, test cost = 90.2879.\n",
            "[15800] train cost = 52.7283, test cost = 90.1315.\n",
            "[15900] train cost = 52.8665, test cost = 89.8281.\n",
            "[16000] train cost = 53.8571, test cost = 88.0884.\n",
            "[16100] train cost = 52.5993, test cost = 90.3339.\n",
            "[16200] train cost = 52.2990, test cost = 90.9875.\n",
            "[16300] train cost = 52.2728, test cost = 91.2768.\n",
            "[16400] train cost = 52.2578, test cost = 91.1213.\n",
            "[16500] train cost = 54.2156, test cost = 93.8689.\n",
            "[16600] train cost = 53.2296, test cost = 93.6382.\n",
            "[16700] train cost = 52.3355, test cost = 91.7753.\n",
            "[16800] train cost = 51.9442, test cost = 92.1807.\n",
            "[16900] train cost = 52.2426, test cost = 92.0340.\n",
            "[17000] train cost = 55.3550, test cost = 88.3901.\n",
            "[17100] train cost = 51.5510, test cost = 92.8661.\n",
            "[17200] train cost = 53.7711, test cost = 90.2450.\n",
            "[17300] train cost = 51.7057, test cost = 93.3269.\n",
            "[17400] train cost = 52.2472, test cost = 93.5583.\n",
            "[17500] train cost = 51.4648, test cost = 93.4765.\n",
            "[17600] train cost = 52.5270, test cost = 92.4743.\n",
            "[17700] train cost = 52.1394, test cost = 93.6183.\n",
            "[17800] train cost = 51.2256, test cost = 95.2406.\n",
            "[17900] train cost = 50.9952, test cost = 94.6072.\n",
            "[18000] train cost = 50.7842, test cost = 95.2626.\n",
            "[18100] train cost = 50.8518, test cost = 95.2172.\n",
            "[18200] train cost = 50.7108, test cost = 95.4664.\n",
            "[18300] train cost = 50.6567, test cost = 96.4125.\n",
            "[18400] train cost = 50.9736, test cost = 95.8893.\n",
            "[18500] train cost = 51.1580, test cost = 95.7690.\n",
            "[18600] train cost = 51.4193, test cost = 96.3759.\n",
            "[18700] train cost = 50.1622, test cost = 96.5615.\n",
            "[18800] train cost = 50.1913, test cost = 96.5239.\n",
            "[18900] train cost = 50.7425, test cost = 96.8702.\n",
            "[19000] train cost = 50.1071, test cost = 97.0171.\n",
            "[19100] train cost = 50.0372, test cost = 97.0717.\n",
            "[19200] train cost = 49.8795, test cost = 97.3803.\n",
            "[19300] train cost = 51.3774, test cost = 98.5282.\n",
            "[19400] train cost = 50.5337, test cost = 96.8661.\n",
            "[19500] train cost = 49.9938, test cost = 98.4057.\n",
            "[19600] train cost = 52.0890, test cost = 95.4973.\n",
            "[19700] train cost = 50.7651, test cost = 99.2443.\n",
            "[19800] train cost = 49.8852, test cost = 98.4657.\n",
            "[19900] train cost = 49.4970, test cost = 98.6752.\n",
            "[20000] train cost = 52.9981, test cost = 98.8626.\n",
            "[20100] train cost = 49.5404, test cost = 99.5341.\n",
            "[20200] train cost = 53.0311, test cost = 103.9428.\n",
            "[20300] train cost = 50.4873, test cost = 100.5857.\n",
            "[20400] train cost = 50.2088, test cost = 99.5383.\n",
            "[20500] train cost = 49.0424, test cost = 100.1067.\n",
            "[20600] train cost = 49.1960, test cost = 100.5526.\n",
            "[20700] train cost = 51.8931, test cost = 96.2583.\n",
            "[20800] train cost = 49.0037, test cost = 99.8203.\n",
            "[20900] train cost = 49.9925, test cost = 100.3736.\n",
            "[21000] train cost = 52.3565, test cost = 105.0811.\n",
            "[21100] train cost = 48.6891, test cost = 101.4468.\n",
            "[21200] train cost = 49.6694, test cost = 101.0935.\n",
            "[21300] train cost = 49.0392, test cost = 102.0729.\n",
            "[21400] train cost = 48.9915, test cost = 100.3782.\n",
            "[21500] train cost = 48.6436, test cost = 101.0608.\n",
            "[21600] train cost = 48.4844, test cost = 101.4140.\n",
            "[21700] train cost = 48.4298, test cost = 101.9596.\n",
            "[21800] train cost = 48.2471, test cost = 102.1599.\n",
            "[21900] train cost = 49.6788, test cost = 103.6617.\n",
            "[22000] train cost = 48.1739, test cost = 102.0502.\n",
            "[22100] train cost = 48.2946, test cost = 102.7303.\n",
            "[22200] train cost = 48.4702, test cost = 101.7091.\n",
            "[22300] train cost = 48.7243, test cost = 102.8551.\n",
            "[22400] train cost = 51.3123, test cost = 107.4962.\n",
            "[22500] train cost = 49.5457, test cost = 100.6034.\n",
            "[22600] train cost = 47.7408, test cost = 103.7780.\n",
            "[22700] train cost = 47.9393, test cost = 103.8811.\n",
            "[22800] train cost = 48.8032, test cost = 103.9090.\n",
            "[22900] train cost = 49.6344, test cost = 100.6200.\n",
            "[23000] train cost = 48.0954, test cost = 104.3588.\n",
            "[23100] train cost = 48.4353, test cost = 103.0399.\n",
            "[23200] train cost = 48.0769, test cost = 103.3715.\n",
            "[23300] train cost = 48.0091, test cost = 105.5546.\n",
            "[23400] train cost = 47.4006, test cost = 104.8859.\n",
            "[23500] train cost = 48.2642, test cost = 103.2360.\n",
            "[23600] train cost = 48.9777, test cost = 101.4492.\n",
            "[23700] train cost = 48.6869, test cost = 104.0335.\n",
            "[23800] train cost = 48.3608, test cost = 102.8464.\n",
            "[23900] train cost = 47.4463, test cost = 105.1884.\n",
            "[24000] train cost = 49.2616, test cost = 103.1213.\n",
            "[24100] train cost = 48.4265, test cost = 105.2985.\n",
            "[24200] train cost = 47.2001, test cost = 105.2109.\n",
            "[24300] train cost = 47.2539, test cost = 105.6230.\n",
            "[24400] train cost = 47.0982, test cost = 105.9901.\n",
            "[24500] train cost = 47.1448, test cost = 105.4505.\n",
            "[24600] train cost = 47.4968, test cost = 105.8340.\n",
            "[24700] train cost = 47.1486, test cost = 105.5356.\n",
            "[24800] train cost = 47.6759, test cost = 107.3283.\n",
            "[24900] train cost = 46.8368, test cost = 107.1940.\n",
            "[25000] train cost = 46.7740, test cost = 107.5656.\n",
            "[25100] train cost = 47.7940, test cost = 106.7887.\n",
            "[25200] train cost = 48.4863, test cost = 107.2178.\n",
            "[25300] train cost = 46.5251, test cost = 106.9892.\n",
            "[25400] train cost = 46.6261, test cost = 107.2498.\n",
            "[25500] train cost = 46.4959, test cost = 108.4732.\n",
            "[25600] train cost = 46.6555, test cost = 108.0298.\n",
            "[25700] train cost = 50.2866, test cost = 101.9493.\n",
            "[25800] train cost = 47.6117, test cost = 107.3753.\n",
            "[25900] train cost = 47.9599, test cost = 107.2556.\n",
            "[26000] train cost = 46.4975, test cost = 107.8961.\n",
            "[26100] train cost = 47.8784, test cost = 106.5786.\n",
            "[26200] train cost = 47.9277, test cost = 106.6996.\n",
            "[26300] train cost = 46.4223, test cost = 108.2282.\n",
            "[26400] train cost = 46.1074, test cost = 107.7836.\n",
            "[26500] train cost = 46.0938, test cost = 107.8477.\n",
            "[26600] train cost = 46.6993, test cost = 108.0979.\n",
            "[26700] train cost = 46.2776, test cost = 108.3709.\n",
            "[26800] train cost = 51.7653, test cost = 100.8176.\n",
            "[26900] train cost = 46.3221, test cost = 109.4370.\n",
            "[27000] train cost = 47.0696, test cost = 108.2803.\n",
            "[27100] train cost = 45.8990, test cost = 109.6958.\n",
            "[27200] train cost = 45.9402, test cost = 108.9762.\n",
            "[27300] train cost = 46.0876, test cost = 108.6389.\n",
            "[27400] train cost = 45.7394, test cost = 109.7939.\n",
            "[27500] train cost = 45.7223, test cost = 109.2698.\n",
            "[27600] train cost = 45.8772, test cost = 109.3942.\n",
            "[27700] train cost = 45.8672, test cost = 109.3240.\n",
            "[27800] train cost = 45.6998, test cost = 109.4410.\n",
            "[27900] train cost = 46.4386, test cost = 110.0616.\n",
            "[28000] train cost = 45.5206, test cost = 109.7011.\n",
            "[28100] train cost = 45.9336, test cost = 109.8476.\n",
            "[28200] train cost = 46.9568, test cost = 108.1460.\n",
            "[28300] train cost = 46.9986, test cost = 109.1421.\n",
            "[28400] train cost = 45.1912, test cost = 110.8594.\n",
            "[28500] train cost = 47.5316, test cost = 108.1029.\n",
            "[28600] train cost = 46.3492, test cost = 111.8200.\n",
            "[28700] train cost = 45.4405, test cost = 110.9929.\n",
            "[28800] train cost = 47.1172, test cost = 109.8901.\n",
            "[28900] train cost = 45.2688, test cost = 112.1893.\n",
            "[29000] train cost = 45.9367, test cost = 110.0667.\n",
            "[29100] train cost = 47.2214, test cost = 108.0353.\n",
            "[29200] train cost = 45.0827, test cost = 111.4798.\n",
            "[29300] train cost = 44.9176, test cost = 112.3143.\n",
            "[29400] train cost = 44.8245, test cost = 111.4639.\n",
            "[29500] train cost = 45.0087, test cost = 112.7666.\n",
            "[29600] train cost = 44.8180, test cost = 112.1761.\n",
            "[29700] train cost = 44.8093, test cost = 111.4211.\n",
            "[29800] train cost = 44.6646, test cost = 112.8094.\n",
            "[29900] train cost = 46.4759, test cost = 109.5217.\n",
            "[30000] train cost = 45.9226, test cost = 109.5938.\n",
            "[30100] train cost = 44.5589, test cost = 112.6294.\n",
            "[30200] train cost = 44.5858, test cost = 113.3894.\n",
            "[30300] train cost = 44.6931, test cost = 113.1457.\n",
            "[30400] train cost = 44.9953, test cost = 113.5695.\n",
            "[30500] train cost = 45.1619, test cost = 113.8246.\n",
            "[30600] train cost = 44.5199, test cost = 113.9895.\n",
            "[30700] train cost = 44.3639, test cost = 113.0560.\n",
            "[30800] train cost = 46.1218, test cost = 113.1939.\n",
            "[30900] train cost = 44.6313, test cost = 111.9430.\n",
            "[31000] train cost = 44.5317, test cost = 112.8414.\n",
            "[31100] train cost = 44.2207, test cost = 112.5746.\n",
            "[31200] train cost = 44.5772, test cost = 112.6499.\n",
            "[31300] train cost = 46.2009, test cost = 114.4687.\n",
            "[31400] train cost = 44.1091, test cost = 113.3707.\n",
            "[31500] train cost = 44.1781, test cost = 113.6251.\n",
            "[31600] train cost = 44.0317, test cost = 114.2525.\n",
            "[31700] train cost = 47.7971, test cost = 113.2858.\n",
            "[31800] train cost = 45.8554, test cost = 111.4635.\n",
            "[31900] train cost = 44.2253, test cost = 113.7373.\n",
            "[32000] train cost = 46.9052, test cost = 108.2348.\n",
            "[32100] train cost = 44.7695, test cost = 112.8642.\n",
            "[32200] train cost = 43.7477, test cost = 114.7124.\n",
            "[32300] train cost = 46.5457, test cost = 113.1075.\n",
            "[32400] train cost = 45.1535, test cost = 116.3642.\n",
            "[32500] train cost = 43.8284, test cost = 114.3473.\n",
            "[32600] train cost = 47.1381, test cost = 114.6956.\n",
            "[32700] train cost = 43.5985, test cost = 114.6370.\n",
            "[32800] train cost = 43.6642, test cost = 114.8243.\n",
            "[32900] train cost = 44.3595, test cost = 114.2037.\n",
            "[33000] train cost = 43.7874, test cost = 115.2965.\n",
            "[33100] train cost = 47.1542, test cost = 112.7807.\n",
            "[33200] train cost = 44.0840, test cost = 115.3577.\n",
            "[33300] train cost = 43.4508, test cost = 115.1332.\n",
            "[33400] train cost = 43.7417, test cost = 115.3733.\n",
            "[33500] train cost = 44.8856, test cost = 113.7789.\n",
            "[33600] train cost = 43.5832, test cost = 114.4143.\n",
            "[33700] train cost = 43.7205, test cost = 115.9595.\n",
            "[33800] train cost = 43.9340, test cost = 114.5861.\n",
            "[33900] train cost = 43.7272, test cost = 115.4447.\n",
            "[34000] train cost = 44.2915, test cost = 114.4649.\n",
            "[34100] train cost = 43.1570, test cost = 116.0979.\n",
            "[34200] train cost = 43.5254, test cost = 115.5455.\n",
            "[34300] train cost = 43.4601, test cost = 117.2403.\n",
            "[34400] train cost = 44.2434, test cost = 115.2454.\n",
            "[34500] train cost = 44.8047, test cost = 115.1426.\n",
            "[34600] train cost = 43.2618, test cost = 115.8542.\n",
            "[34700] train cost = 44.3485, test cost = 115.2275.\n",
            "[34800] train cost = 43.9169, test cost = 118.0342.\n",
            "[34900] train cost = 43.1635, test cost = 115.5505.\n",
            "[35000] train cost = 43.6038, test cost = 114.5616.\n",
            "[35100] train cost = 46.5495, test cost = 114.6152.\n",
            "[35200] train cost = 45.9192, test cost = 117.3098.\n",
            "[35300] train cost = 42.7180, test cost = 116.7478.\n",
            "[35400] train cost = 43.0301, test cost = 117.2657.\n",
            "[35500] train cost = 42.8686, test cost = 117.1016.\n",
            "[35600] train cost = 44.2177, test cost = 118.3944.\n",
            "[35700] train cost = 42.5829, test cost = 117.5908.\n",
            "[35800] train cost = 42.8003, test cost = 117.9385.\n",
            "[35900] train cost = 42.4875, test cost = 119.0554.\n",
            "[36000] train cost = 42.9415, test cost = 117.5917.\n",
            "[36100] train cost = 42.9481, test cost = 117.7883.\n",
            "[36200] train cost = 42.7924, test cost = 119.3359.\n",
            "[36300] train cost = 44.3809, test cost = 116.8802.\n",
            "[36400] train cost = 42.6454, test cost = 118.4742.\n",
            "[36500] train cost = 43.8778, test cost = 116.8039.\n",
            "[36600] train cost = 43.0113, test cost = 116.6466.\n",
            "[36700] train cost = 42.4193, test cost = 118.8278.\n",
            "[36800] train cost = 42.4451, test cost = 118.1286.\n",
            "[36900] train cost = 42.3781, test cost = 118.0640.\n",
            "[37000] train cost = 43.2736, test cost = 118.1338.\n",
            "[37100] train cost = 42.8257, test cost = 118.9216.\n",
            "[37200] train cost = 43.5449, test cost = 115.9370.\n",
            "[37300] train cost = 44.4856, test cost = 115.7034.\n",
            "[37400] train cost = 43.1086, test cost = 116.7458.\n",
            "[37500] train cost = 42.8677, test cost = 118.6723.\n",
            "[37600] train cost = 42.1519, test cost = 117.7580.\n",
            "[37700] train cost = 43.9393, test cost = 118.1763.\n",
            "[37800] train cost = 42.0045, test cost = 117.8529.\n",
            "[37900] train cost = 42.9301, test cost = 117.8224.\n",
            "[38000] train cost = 47.6112, test cost = 116.4416.\n",
            "[38100] train cost = 42.1080, test cost = 117.5267.\n",
            "[38200] train cost = 42.0884, test cost = 117.8156.\n",
            "[38300] train cost = 42.0844, test cost = 119.0094.\n",
            "[38400] train cost = 43.5853, test cost = 117.0119.\n",
            "[38500] train cost = 42.3443, test cost = 118.6249.\n",
            "[38600] train cost = 42.6435, test cost = 120.2674.\n",
            "[38700] train cost = 41.8149, test cost = 119.0451.\n",
            "[38800] train cost = 41.9156, test cost = 118.3466.\n",
            "[38900] train cost = 42.0083, test cost = 119.0770.\n",
            "[39000] train cost = 42.4077, test cost = 119.2973.\n",
            "[39100] train cost = 41.7829, test cost = 119.7767.\n",
            "[39200] train cost = 43.4975, test cost = 118.5780.\n",
            "[39300] train cost = 41.6187, test cost = 119.5034.\n",
            "[39400] train cost = 41.7421, test cost = 119.4767.\n",
            "[39500] train cost = 43.3109, test cost = 120.0446.\n",
            "[39600] train cost = 44.7892, test cost = 115.8252.\n",
            "[39700] train cost = 42.8400, test cost = 118.2953.\n",
            "[39800] train cost = 41.7859, test cost = 119.0136.\n",
            "[39900] train cost = 41.9486, test cost = 119.8885.\n",
            "[40000] train cost = 41.8029, test cost = 119.5375.\n",
            "[40100] train cost = 41.7694, test cost = 119.7649.\n",
            "[40200] train cost = 41.4203, test cost = 121.0190.\n",
            "[40300] train cost = 44.6985, test cost = 116.6382.\n",
            "[40400] train cost = 41.5571, test cost = 119.7743.\n",
            "[40500] train cost = 41.9744, test cost = 119.7765.\n",
            "[40600] train cost = 41.5867, test cost = 120.1911.\n",
            "[40700] train cost = 41.3807, test cost = 119.9054.\n",
            "[40800] train cost = 41.4969, test cost = 118.9548.\n",
            "[40900] train cost = 41.3261, test cost = 120.6050.\n",
            "[41000] train cost = 41.2767, test cost = 120.6046.\n",
            "[41100] train cost = 41.5479, test cost = 120.0246.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a30fb34e48c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPRINT_DELAY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujtxMM5Anyyt",
        "colab_type": "code",
        "outputId": "0f84626e-2b22-43a1-ae94-450d9b32541f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(range(len(train_losses)),train_losses)\n",
        "plt.plot(range(len(test_losses)),test_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faaf1ac5978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBnellGqH/h8TNxVUhSggGgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqgUXBzVoecS",
        "colab_type": "code",
        "outputId": "70e7c8a1-4184-4453-f365-a1acce1d290b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.scatter(train_losses[1:],test_losses[1:],s=0.2,c=range(len(train_losses))[1:],cmap=\"plasma\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7faaf1f8b710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQz2pAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzlq_zZP9ype",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples, example_loss = sess.run([outputs, cost], train_feed_dict)#{inputs:X_train[[0]],targets:y_train[[0]],cond:c_train[[0]]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ynu75vk92ay",
        "colab_type": "code",
        "outputId": "5df3c8a2-fbce-4259-b53a-d4c05e36afa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "i=np.random.randint(test_outputs.shape[0])\n",
        "examples = np.around(examples)\n",
        "print(examples[i].T)\n",
        "print(y_train[i].T)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[20. 10. 16.  7.  7.  8.  8.  5.  8.  6.  1.  0.]]\n",
            "[[20.  6. 10.  6.  6.  9. 12.  5.  9.  5.  2.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlnjnyZId3tS",
        "colab_type": "code",
        "outputId": "30e4d520-ba2e-4332-c505-a9e848763a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "encoded = np.concatenate((np.array([1]),examples[i].flatten()[:-2],np.array([2])))\n",
        "print(encoded)\n",
        "example_circ = decode_circuit(encoded,N)\n",
        "example_circ.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1. 20. 10. 16.  7.  7.  8.  8.  5.  8.  6.  2.]\n",
            "OPENQASM 2.0;\n",
            "include \"qelib1.inc\";\n",
            "qreg q[5];\n",
            "cx q[1],q[2];\n",
            "s q[2];\n",
            "cx q[0],q[3];\n",
            "h q[4];\n",
            "h q[4];\n",
            "s q[0];\n",
            "s q[0];\n",
            "h q[2];\n",
            "s q[0];\n",
            "h q[3];\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">                  ┌───┐┌───┐┌───┐\n",
              "q_0: |0>───────■──┤ S ├┤ S ├┤ S ├\n",
              "               │  └───┘└───┘└───┘\n",
              "q_1: |0>──■────┼─────────────────\n",
              "        ┌─┴─┐  │  ┌───┐┌───┐     \n",
              "q_2: |0>┤ X ├──┼──┤ S ├┤ H ├─────\n",
              "        └───┘┌─┴─┐├───┤└───┘     \n",
              "q_3: |0>─────┤ X ├┤ H ├──────────\n",
              "             ├───┤├───┤          \n",
              "q_4: |0>─────┤ H ├┤ H ├──────────\n",
              "             └───┘└───┘          </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7faaef5b7f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7loLgREMfba0",
        "colab_type": "code",
        "outputId": "38ca1b2d-5b7f-4b6e-bfc0-7192f7dbc212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "label = generate_labels([example_circ])\n",
        "print(label)\n",
        "print(c_train[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5  0.   0.   0.   0.5  0.   0.   0.   0.5  0.   0.   0.   0.5  0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  -0.   0.   0.   0.  -0.   0.   0.   0.  -0.   0.\n",
            "   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
            "[ 0.354  0.     0.354  0.     0.     0.     0.     0.     0.     0.\n",
            "  0.     0.    -0.354  0.    -0.354  0.     0.354  0.     0.354  0.\n",
            "  0.     0.     0.     0.     0.     0.     0.     0.    -0.354  0.\n",
            " -0.354  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "  0.     0.     0.     0.   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMpd-F8Na8m7",
        "colab_type": "text"
      },
      "source": [
        "#### OHE Circuit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-NMrr0a_dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "N = 5\n",
        "VOCAB_DIM = 2+2*N+N**2\n",
        "NUM_SAMPLES = 100000\n",
        "TRAINTEST = 0.75\n",
        "CUTOFF = int(NUM_SAMPLES//2)#(1/TRAINTEST))\n",
        "MAX_LENGTH = 10\n",
        "TIME_STEPS = MAX_LENGTH+2\n",
        "COND_DIM = 2**(N+1)\n",
        "NUM_CELLS = 256\n",
        "STACK_DEPTH = 5\n",
        "PRINT_DELAY = 100\n",
        "SAVE_DELAY = 10000\n",
        "\n",
        "# Training Parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FdLLM4YQJ22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Data\n",
        "# Data = encode_circuits(sample_circuits(N,MAX_LENGTH,NUM_SAMPLES),N,MAX_LENGTH,label=True)\n",
        "# Save Data\n",
        "# np.savetxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", Data, delimiter=\",\")\n",
        "# Load Data\n",
        "Data = np.loadtxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", delimiter=\",\")\n",
        "np.random.shuffle(Data)\n",
        "X = Data[:,:TIME_STEPS]\n",
        "# Convert to onehot\n",
        "X = np.array([[eye[int(gate)] for gate in X[int(row),:]] for row in range(X.shape[0])])\n",
        "c = Data[:,TIME_STEPS:]\n",
        "offset = X[:,1:]\n",
        "y = np.concatenate((offset,np.zeros((X.shape[0],1,VOCAB_DIM))),axis=1)\n",
        "X_train = X[:CUTOFF]\n",
        "X_test = X[CUTOFF:]\n",
        "c_test = c[:CUTOFF]\n",
        "c_train = c[CUTOFF:]\n",
        "y_train = y[:CUTOFF]\n",
        "y_test = y[CUTOFF:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hoM_-mPfE0tg",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Placeholders.\n",
        "inputs = tf.placeholder(name='inputs', dtype=tf.float32, shape=(None, None, VOCAB_DIM))\n",
        "targets = tf.placeholder(name='targets', dtype=tf.float32, shape=(None, None, VOCAB_DIM))\n",
        "cond = tf.placeholder(name='conditions', dtype=tf.float32, shape=(None, COND_DIM))\n",
        "\n",
        "# Conditional RNN.\n",
        "outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "for _ in range(STACK_DEPTH-1):\n",
        "    outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "\n",
        "# Classification layer.\n",
        "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=VOCAB_DIM, activation='softmax'))(outputs)\n",
        "\n",
        "# Loss + Optimizer.\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Initialize variables (tensorflow)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Define the binding between placeholders and real data.\n",
        "train_feed_dict = {inputs: X_train, targets: y_train, cond: c_train}\n",
        "test_feed_dict = {inputs: X_test, targets: y_test, cond: c_test}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq1w4E0sccjs",
        "colab_type": "code",
        "outputId": "ade4e47c-44cd-4fb6-db98-0604a9689b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Main loop. Optimize then evaluate.\n",
        "#saver = tf.train.Saver()\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    sess.run(optimizer, train_feed_dict)\n",
        "    if epoch % PRINT_DELAY == 0:\n",
        "        train_outputs, train_loss = sess.run([outputs, cost], train_feed_dict)\n",
        "        test_outputs, test_loss = sess.run([outputs, cost], test_feed_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f'[{str(epoch).zfill(4)}] train cost = {train_loss:.4f}, test cost = {test_loss:.4f}.')\n",
        "    if epoch % SAVE_DELAY == 0:\n",
        "        saver.save(sess, f'Checkpoints/QCG-ohe-{str(NUM_CELLS)}', global_step=int(epoch/SAVE_DELAY))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0000] train cost = 3.3097, test cost = 3.3097.\n",
            "[0100] train cost = 3.1948, test cost = 3.1948.\n",
            "[0200] train cost = 3.1888, test cost = 3.1886.\n",
            "[0300] train cost = 3.1885, test cost = 3.1884.\n",
            "[0400] train cost = 3.1884, test cost = 3.1883.\n",
            "[0500] train cost = 3.1883, test cost = 3.1883.\n",
            "[0600] train cost = 3.1883, test cost = 3.1883.\n",
            "[0700] train cost = 3.1882, test cost = 3.1883.\n",
            "[0800] train cost = 3.1881, test cost = 3.1884.\n",
            "[0900] train cost = 3.1879, test cost = 3.1884.\n",
            "[1000] train cost = 3.1877, test cost = 3.1885.\n",
            "[1100] train cost = 3.1875, test cost = 3.1887.\n",
            "[1200] train cost = 3.1872, test cost = 3.1889.\n",
            "[1300] train cost = 3.1868, test cost = 3.1893.\n",
            "[1400] train cost = 3.2689, test cost = 3.2713.\n",
            "[1500] train cost = 3.2684, test cost = 3.2717.\n",
            "[1600] train cost = 3.2676, test cost = 3.2722.\n",
            "[1700] train cost = 3.2666, test cost = 3.2730.\n",
            "[1800] train cost = 3.2655, test cost = 3.2739.\n",
            "[1900] train cost = 3.1852, test cost = 3.1920.\n",
            "[2000] train cost = 3.1820, test cost = 3.1924.\n",
            "[2100] train cost = 3.1804, test cost = 3.1932.\n",
            "[2200] train cost = 3.1788, test cost = 3.1939.\n",
            "[2300] train cost = 3.1768, test cost = 3.1946.\n",
            "[2400] train cost = 3.1749, test cost = 3.1952.\n",
            "[2500] train cost = 3.1735, test cost = 3.1957.\n",
            "[2600] train cost = 3.1724, test cost = 3.1960.\n",
            "[2700] train cost = 3.1703, test cost = 3.1964.\n",
            "[2800] train cost = 3.1680, test cost = 3.1967.\n",
            "[2900] train cost = 3.1664, test cost = 3.1969.\n",
            "[3000] train cost = 3.1656, test cost = 3.1972.\n",
            "[3100] train cost = 3.1634, test cost = 3.1974.\n",
            "[3200] train cost = 3.1618, test cost = 3.1976.\n",
            "[3300] train cost = 3.1605, test cost = 3.1977.\n",
            "[3400] train cost = 3.1597, test cost = 3.1980.\n",
            "[3500] train cost = 3.1580, test cost = 3.1981.\n",
            "[3600] train cost = 3.1574, test cost = 3.1982.\n",
            "[3700] train cost = 3.1561, test cost = 3.1982.\n",
            "[3800] train cost = 3.1544, test cost = 3.1984.\n",
            "[3900] train cost = 3.1539, test cost = 3.1984.\n",
            "[4000] train cost = 3.1535, test cost = 3.1985.\n",
            "[4100] train cost = 3.1517, test cost = 3.1986.\n",
            "[4200] train cost = 3.1510, test cost = 3.1987.\n",
            "[4300] train cost = 3.1503, test cost = 3.1988.\n",
            "[4400] train cost = 3.1494, test cost = 3.1989.\n",
            "[4500] train cost = 3.1530, test cost = 3.1992.\n",
            "[4600] train cost = 3.1472, test cost = 3.1991.\n",
            "[4700] train cost = 3.1479, test cost = 3.1991.\n",
            "[4800] train cost = 3.1459, test cost = 3.1992.\n",
            "[4900] train cost = 3.1454, test cost = 3.1993.\n",
            "[5000] train cost = 3.1451, test cost = 3.1993.\n",
            "[5100] train cost = 3.1458, test cost = 3.1993.\n",
            "[5200] train cost = 3.1439, test cost = 3.1995.\n",
            "[5300] train cost = 3.1438, test cost = 3.1995.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-327-d1f68b4af65a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPRINT_DELAY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           if (not is_tensor_handle_feed and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGL_GTpkfZyN",
        "colab_type": "code",
        "outputId": "1db74d95-3fb7-4e9a-8e26-eabb82f9de3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "ts = range(len(train_losses))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(ts,train_losses)\n",
        "ax.plot(ts,test_losses)\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.plot(ts,np.divide(train_losses,test_losses))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAFlCAYAAAAUHQWiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyGxVkc4TWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFJVAPnXgGjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "# new_saver = tf.train.import_meta_graph('Checkpoints/QCG-ohe-256-5.meta')\n",
        "# new_saver.restore(sess, tf.train.latest_checkpoint('Checkpoints'))\n",
        "examples, example_loss = sess.run([outputs, cost], train_feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqUMZGX5gIw-",
        "colab_type": "code",
        "outputId": "58098e84-c0b0-4efc-986b-a92297854f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "i = np.random.randint(examples.shape[0])\n",
        "ones = np.full((examples.shape[0],1),1)\n",
        "vocab = np.argmax(np.around(examples),2)[:,:-1]\n",
        "final = np.concatenate((ones,vocab),1)\n",
        "decoded = decode_circuit(final[i],N)\n",
        "label = generate_labels([decoded])\n",
        "print(final[i])\n",
        "print(np.concatenate(([1],np.argmax(y_train[i][:-1],1))))\n",
        "print(label)\n",
        "print(c_train[i])\n",
        "decoded.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 5 5 5 6 7 7 5 7 7 5 2]\n",
            "[ 1  3 10  8 24  9  9 26  9 20 14  2]\n",
            "[[ 0.5  0.   0.   0.   0.5  0.   0.   0.   0.5  0.   0.   0.   0.5  0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  -0.   0.   0.   0.  -0.   0.   0.   0.  -0.   0.\n",
            "   0.   0.  -0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
            "[0.5 0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.\n",
            " 0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">                                 \n",
              "q_0: |0>─────────────────────────\n",
              "                                 \n",
              "q_1: |0>─────────────────────────\n",
              "        ┌───┐┌───┐┌───┐┌───┐┌───┐\n",
              "q_2: |0>┤ H ├┤ H ├┤ H ├┤ H ├┤ H ├\n",
              "        ├───┤└───┘└───┘└───┘└───┘\n",
              "q_3: |0>┤ H ├────────────────────\n",
              "        ├───┤┌───┐┌───┐┌───┐     \n",
              "q_4: |0>┤ H ├┤ H ├┤ H ├┤ H ├─────\n",
              "        └───┘└───┘└───┘└───┘     </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7f0e3e3feeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9HgWMyLldpj",
        "colab_type": "text"
      },
      "source": [
        "#### Code Graveyard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsemFtmoS-VW",
        "colab_type": "text"
      },
      "source": [
        "Failed model: Naive RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY96ywLFTnID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "n = 5\n",
        "vocab_dim = 2+2*n+n**2\n",
        "embedding_dim = 32\n",
        "max_length = 30\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01_2pq0lV-3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data\n",
        "l_m = encode_circuits(sample_circuits(n,max_length,2000),n,max_length,label=True)\n",
        "l_q = encode_circuits(sample_circuits(n,int(np.ceil(max_length/4)),500),n,max_length,label=True)\n",
        "l_h = encode_circuits(sample_circuits(n,int(np.ceil(max_length/2)),500),n,max_length,label=True)\n",
        "l_tq = encode_circuits(sample_circuits(n,int(np.ceil(3*max_length/4)),500),n,max_length,label=True)\n",
        "Data = np.concatenate((l_m,l_q,l_h,l_tq),axis=0)\n",
        "np.random.shuffle(Data)\n",
        "X = Data[:,32:]\n",
        "y = Data[:,:32]\n",
        "X_train = X[:1500]\n",
        "X_test = X[1500:]\n",
        "y_train = y[:1500]\n",
        "y_test = y[1500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQHAq62aTDvx",
        "colab_type": "code",
        "outputId": "df99ee42-c980-4468-ac93-22aeb7ed3509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# Model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_length+2,embedding_dim,mask_zero=True))\n",
        "#model.add(tf.keras.layers.Masking(mask_value=0, input_shape=(max_length,)))\n",
        "#model.add(tf.keras.layers.RepeatVector(max_length, input_shape=(max_length,)))\n",
        "model.add(tf.keras.layers.LSTM(max_length+2, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(max_length+2, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"Adadelta\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_34 (Embedding)     (None, None, 32)          1024      \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                1056      \n",
            "=================================================================\n",
            "Total params: 10,400\n",
            "Trainable params: 10,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awdnLm9neBkJ",
        "colab_type": "code",
        "outputId": "dc2e10a3-8c03-487b-8938-286525774198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train\n",
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size,\n",
        "          epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.1108 - accuracy: 0.0193 - val_loss: 794.2617 - val_accuracy: 0.0140\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0986 - accuracy: 0.0233 - val_loss: 794.2491 - val_accuracy: 0.0260\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0851 - accuracy: 0.0347 - val_loss: 794.2361 - val_accuracy: 0.0260\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0718 - accuracy: 0.0440 - val_loss: 794.2226 - val_accuracy: 0.0440\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0572 - accuracy: 0.0433 - val_loss: 794.2086 - val_accuracy: 0.0420\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0427 - accuracy: 0.0560 - val_loss: 794.1940 - val_accuracy: 0.0680\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0276 - accuracy: 0.0680 - val_loss: 794.1790 - val_accuracy: 0.0680\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0118 - accuracy: 0.0680 - val_loss: 794.1634 - val_accuracy: 0.0680\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9950 - accuracy: 0.0680 - val_loss: 794.1470 - val_accuracy: 0.0680\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9782 - accuracy: 0.0680 - val_loss: 794.1300 - val_accuracy: 0.0680\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9606 - accuracy: 0.0680 - val_loss: 794.1122 - val_accuracy: 0.0680\n",
            "Epoch 12/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9417 - accuracy: 0.0680 - val_loss: 794.0936 - val_accuracy: 0.0680\n",
            "Epoch 13/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9222 - accuracy: 0.0680 - val_loss: 794.0741 - val_accuracy: 0.0680\n",
            "Epoch 14/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9017 - accuracy: 0.0680 - val_loss: 794.0535 - val_accuracy: 0.0680\n",
            "Epoch 15/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8799 - accuracy: 0.0680 - val_loss: 794.0319 - val_accuracy: 0.0680\n",
            "Epoch 16/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8572 - accuracy: 0.0680 - val_loss: 794.0092 - val_accuracy: 0.0680\n",
            "Epoch 17/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8333 - accuracy: 0.0680 - val_loss: 793.9852 - val_accuracy: 0.0680\n",
            "Epoch 18/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8081 - accuracy: 0.0680 - val_loss: 793.9597 - val_accuracy: 0.0680\n",
            "Epoch 19/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7812 - accuracy: 0.0680 - val_loss: 793.9327 - val_accuracy: 0.0680\n",
            "Epoch 20/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7522 - accuracy: 0.0680 - val_loss: 793.9039 - val_accuracy: 0.0680\n",
            "Epoch 21/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7214 - accuracy: 0.0680 - val_loss: 793.8732 - val_accuracy: 0.0680\n",
            "Epoch 22/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6885 - accuracy: 0.0680 - val_loss: 793.8400 - val_accuracy: 0.0680\n",
            "Epoch 23/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6528 - accuracy: 0.0680 - val_loss: 793.8042 - val_accuracy: 0.0680\n",
            "Epoch 24/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6150 - accuracy: 0.0680 - val_loss: 793.7653 - val_accuracy: 0.0680\n",
            "Epoch 25/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.5724 - accuracy: 0.0680 - val_loss: 793.7226 - val_accuracy: 0.0680\n",
            "Epoch 26/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.5261 - accuracy: 0.0680 - val_loss: 793.6755 - val_accuracy: 0.0680\n",
            "Epoch 27/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.4745 - accuracy: 0.0687 - val_loss: 793.6229 - val_accuracy: 0.0840\n",
            "Epoch 28/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.4164 - accuracy: 0.0773 - val_loss: 793.5636 - val_accuracy: 0.0840\n",
            "Epoch 29/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.3492 - accuracy: 0.0773 - val_loss: 793.4954 - val_accuracy: 0.0840\n",
            "Epoch 30/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.2706 - accuracy: 0.0773 - val_loss: 793.4154 - val_accuracy: 0.0840\n",
            "Epoch 31/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.1739 - accuracy: 0.0773 - val_loss: 793.3188 - val_accuracy: 0.0840\n",
            "Epoch 32/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.0570 - accuracy: 0.0773 - val_loss: 793.1983 - val_accuracy: 0.0840\n",
            "Epoch 33/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.8992 - accuracy: 0.0773 - val_loss: 793.0473 - val_accuracy: 0.0840\n",
            "Epoch 34/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.7315 - accuracy: 0.0773 - val_loss: 792.9285 - val_accuracy: 0.0840\n",
            "Epoch 35/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.6533 - accuracy: 0.0773 - val_loss: 793.3630 - val_accuracy: 0.0840\n",
            "Epoch 36/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6683 - accuracy: 0.0773 - val_loss: 795.2918 - val_accuracy: 0.0840\n",
            "Epoch 37/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 796.2934 - accuracy: 0.0773 - val_loss: 798.7723 - val_accuracy: 0.0840\n",
            "Epoch 38/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 800.9558 - accuracy: 0.0773 - val_loss: 804.4900 - val_accuracy: 0.0840\n",
            "Epoch 39/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 806.7342 - accuracy: 0.0687 - val_loss: 809.5878 - val_accuracy: 0.0680\n",
            "Epoch 40/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 810.9744 - accuracy: 0.0667 - val_loss: 812.6163 - val_accuracy: 0.0680\n",
            "Epoch 41/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 813.3979 - accuracy: 0.0667 - val_loss: 814.4037 - val_accuracy: 0.0680\n",
            "Epoch 42/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 815.1448 - accuracy: 0.0667 - val_loss: 816.5336 - val_accuracy: 0.0680\n",
            "Epoch 43/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 817.5818 - accuracy: 0.0667 - val_loss: 819.2408 - val_accuracy: 0.0680\n",
            "Epoch 44/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 820.3315 - accuracy: 0.0667 - val_loss: 821.9930 - val_accuracy: 0.0680\n",
            "Epoch 45/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 823.3867 - accuracy: 0.0667 - val_loss: 824.9569 - val_accuracy: 0.0680\n",
            "Epoch 46/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 826.7348 - accuracy: 0.0667 - val_loss: 828.4964 - val_accuracy: 0.0680\n",
            "Epoch 47/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 830.3835 - accuracy: 0.0667 - val_loss: 832.8303 - val_accuracy: 0.0680\n",
            "Epoch 48/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 835.1679 - accuracy: 0.0667 - val_loss: 838.0462 - val_accuracy: 0.0680\n",
            "Epoch 49/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 840.8740 - accuracy: 0.0667 - val_loss: 844.2008 - val_accuracy: 0.0680\n",
            "Epoch 50/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 847.6452 - accuracy: 0.0667 - val_loss: 851.3121 - val_accuracy: 0.0680\n",
            "Epoch 51/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 855.2304 - accuracy: 0.0660 - val_loss: 859.3821 - val_accuracy: 0.0680\n",
            "Epoch 52/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 864.1146 - accuracy: 0.0653 - val_loss: 868.4477 - val_accuracy: 0.0680\n",
            "Epoch 53/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 873.4734 - accuracy: 0.0653 - val_loss: 878.5035 - val_accuracy: 0.0680\n",
            "Epoch 54/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 884.2753 - accuracy: 0.0667 - val_loss: 889.4948 - val_accuracy: 0.0740\n",
            "Epoch 55/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 895.7303 - accuracy: 0.0747 - val_loss: 901.3611 - val_accuracy: 0.0740\n",
            "Epoch 56/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 908.2810 - accuracy: 0.0747 - val_loss: 913.9891 - val_accuracy: 0.0740\n",
            "Epoch 57/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 921.2766 - accuracy: 0.0747 - val_loss: 927.3201 - val_accuracy: 0.0740\n",
            "Epoch 58/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 935.2397 - accuracy: 0.0747 - val_loss: 941.3236 - val_accuracy: 0.0740\n",
            "Epoch 59/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 949.5889 - accuracy: 0.0747 - val_loss: 955.9353 - val_accuracy: 0.0740\n",
            "Epoch 60/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 964.7316 - accuracy: 0.0747 - val_loss: 971.0822 - val_accuracy: 0.0740\n",
            "Epoch 61/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 980.3476 - accuracy: 0.0747 - val_loss: 986.7279 - val_accuracy: 0.0740\n",
            "Epoch 62/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 996.4534 - accuracy: 0.0747 - val_loss: 1002.8438 - val_accuracy: 0.0740\n",
            "Epoch 63/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1013.4105 - accuracy: 0.0747 - val_loss: 1019.3913 - val_accuracy: 0.0740\n",
            "Epoch 64/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1030.0822 - accuracy: 0.0747 - val_loss: 1036.3138 - val_accuracy: 0.0740\n",
            "Epoch 65/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1047.1509 - accuracy: 0.0747 - val_loss: 1053.5756 - val_accuracy: 0.0740\n",
            "Epoch 66/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1064.5440 - accuracy: 0.0747 - val_loss: 1071.1331 - val_accuracy: 0.0740\n",
            "Epoch 67/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1082.3884 - accuracy: 0.0747 - val_loss: 1089.0033 - val_accuracy: 0.0740\n",
            "Epoch 68/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1100.9237 - accuracy: 0.0747 - val_loss: 1107.1381 - val_accuracy: 0.0740\n",
            "Epoch 69/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1119.3317 - accuracy: 0.0747 - val_loss: 1125.5131 - val_accuracy: 0.0740\n",
            "Epoch 70/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1137.9003 - accuracy: 0.0747 - val_loss: 1144.1294 - val_accuracy: 0.0740\n",
            "Epoch 71/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1156.4520 - accuracy: 0.0747 - val_loss: 1162.9580 - val_accuracy: 0.0740\n",
            "Epoch 72/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1175.9621 - accuracy: 0.0747 - val_loss: 1182.0212 - val_accuracy: 0.0740\n",
            "Epoch 73/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1195.5856 - accuracy: 0.0747 - val_loss: 1201.2620 - val_accuracy: 0.0740\n",
            "Epoch 74/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1214.2600 - accuracy: 0.0747 - val_loss: 1220.6798 - val_accuracy: 0.0740\n",
            "Epoch 75/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1234.0045 - accuracy: 0.0747 - val_loss: 1240.2935 - val_accuracy: 0.0740\n",
            "Epoch 76/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1254.2018 - accuracy: 0.0747 - val_loss: 1260.0628 - val_accuracy: 0.0740\n",
            "Epoch 77/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1273.8898 - accuracy: 0.0747 - val_loss: 1279.9555 - val_accuracy: 0.0740\n",
            "Epoch 78/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1294.3798 - accuracy: 0.0747 - val_loss: 1300.0232 - val_accuracy: 0.0740\n",
            "Epoch 79/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1314.3088 - accuracy: 0.0747 - val_loss: 1320.2064 - val_accuracy: 0.0740\n",
            "Epoch 80/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1334.9257 - accuracy: 0.0747 - val_loss: 1340.5135 - val_accuracy: 0.0740\n",
            "Epoch 81/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1355.1978 - accuracy: 0.0747 - val_loss: 1360.9623 - val_accuracy: 0.0740\n",
            "Epoch 82/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1376.0900 - accuracy: 0.0747 - val_loss: 1381.5232 - val_accuracy: 0.0740\n",
            "Epoch 83/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1397.0069 - accuracy: 0.0747 - val_loss: 1402.2087 - val_accuracy: 0.0740\n",
            "Epoch 84/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1417.6396 - accuracy: 0.0747 - val_loss: 1422.9876 - val_accuracy: 0.0740\n",
            "Epoch 85/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1438.5361 - accuracy: 0.0747 - val_loss: 1443.8418 - val_accuracy: 0.0740\n",
            "Epoch 86/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1459.9821 - accuracy: 0.0747 - val_loss: 1464.7824 - val_accuracy: 0.0740\n",
            "Epoch 87/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1480.7023 - accuracy: 0.0747 - val_loss: 1485.8067 - val_accuracy: 0.0740\n",
            "Epoch 88/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1502.2275 - accuracy: 0.0747 - val_loss: 1506.9167 - val_accuracy: 0.0740\n",
            "Epoch 89/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1523.7325 - accuracy: 0.0747 - val_loss: 1528.1184 - val_accuracy: 0.0740\n",
            "Epoch 90/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1544.4803 - accuracy: 0.0747 - val_loss: 1549.4111 - val_accuracy: 0.0740\n",
            "Epoch 91/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1566.1620 - accuracy: 0.0747 - val_loss: 1570.7809 - val_accuracy: 0.0740\n",
            "Epoch 92/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1587.5873 - accuracy: 0.0747 - val_loss: 1592.2300 - val_accuracy: 0.0740\n",
            "Epoch 93/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1609.1873 - accuracy: 0.0747 - val_loss: 1613.7593 - val_accuracy: 0.0740\n",
            "Epoch 94/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1630.8284 - accuracy: 0.0747 - val_loss: 1635.3507 - val_accuracy: 0.0740\n",
            "Epoch 95/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1652.5624 - accuracy: 0.0747 - val_loss: 1657.0287 - val_accuracy: 0.0740\n",
            "Epoch 96/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1674.0945 - accuracy: 0.0747 - val_loss: 1678.7179 - val_accuracy: 0.0740\n",
            "Epoch 97/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1696.1633 - accuracy: 0.0747 - val_loss: 1700.4490 - val_accuracy: 0.0740\n",
            "Epoch 98/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1718.2308 - accuracy: 0.0747 - val_loss: 1722.2234 - val_accuracy: 0.0740\n",
            "Epoch 99/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1739.8223 - accuracy: 0.0747 - val_loss: 1744.0254 - val_accuracy: 0.0740\n",
            "Epoch 100/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1761.7105 - accuracy: 0.0747 - val_loss: 1765.8690 - val_accuracy: 0.0740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fa8495240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqougJSpSScn",
        "colab_type": "code",
        "outputId": "c2a545f8-25b9-4a6d-81b8-67479a3b4f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "model.predict(X_test)[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.07551420e-02, 3.53694521e-02, 5.65202162e-02, 9.26227644e-02,\n",
              "       1.31801024e-01, 1.92374557e-01, 2.02626482e-01, 2.44499505e-01,\n",
              "       1.21816928e-02, 6.54638512e-03, 9.04475630e-04, 8.65014081e-05,\n",
              "       1.11552211e-03, 8.93953547e-04, 1.12635980e-03, 5.64334448e-04,\n",
              "       1.97271265e-06, 7.51783830e-07, 9.01427597e-07, 5.44743079e-06,\n",
              "       4.84584291e-07, 3.19799511e-07, 7.80020173e-07, 9.12153382e-07,\n",
              "       2.41702547e-09, 4.57604565e-10, 1.11226334e-10, 3.81706311e-09,\n",
              "       8.70810757e-10, 1.63690728e-10, 3.51982332e-10, 1.91976812e-09],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1Arf1fAqwE",
        "colab_type": "text"
      },
      "source": [
        "Old encoding, based on one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb-iCh1_lfZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_circuits(circuits,label=True):\n",
        "    \"\"\"Takes a QuantumCircuit object, and generates an encoding\n",
        "    for use by the network as an array of gates, where each gate is encoded in\n",
        "    one-hot encoding for gate type, target qubit and control qubit.\n",
        "    We also embed tokens to signify the start and end of each sequence, encoded\n",
        "    as extra gate types.\n",
        "    Labels can be included or excluded.\"\"\"\n",
        "    encoded = []\n",
        "    for circ in circuits:\n",
        "        lines = circ.qasm().splitlines()[2:]\n",
        "        n = int(lines[0][7:-2])\n",
        "        lines = lines[1:]\n",
        "        size = len(lines)\n",
        "        encoded_circ = []\n",
        "        eye_g = np.eye(5)\n",
        "        eye_n = np.eye(n)\n",
        "        encoded_circ.append(eye_g[3])\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        for line in lines:\n",
        "            gate_str = line[:2]\n",
        "            integers = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*',line)]\n",
        "            if gate_str==\"h \":\n",
        "                encoded_circ.append(eye_g[0])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "                encoded_circ.append(np.zeros(n))\n",
        "            if gate_str==\"s \":\n",
        "                encoded_circ.append(eye_g[1])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "                encoded_circ.append(np.zeros(n))\n",
        "            if gate_str==\"cx\":\n",
        "                encoded_circ.append(eye_g[2])\n",
        "                encoded_circ.append(eye_n[integers[1]])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "        encoded_circ.append(eye_g[4])\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded.append(encoded_circ)\n",
        "    encoded = np.array(encoded)\n",
        "    if label:\n",
        "        labels = generate_labels(circuits)\n",
        "        return np.array(encoded),labels\n",
        "    else:\n",
        "        return np.array(encoded)\n",
        "\n",
        "def decode_circuit(encoded):\n",
        "    \"\"\"Takes an encoded output from the network and generates the corresponding\n",
        "    circuit as described above.\"\"\"\n",
        "    decoded = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[\"\n",
        "    decoded += str(len(encoded[1]))+\"];\\n\"\n",
        "    encoded = [encoded[n:n+3] for n in range(0, len(encoded), 3)]\n",
        "    for line in encoded:\n",
        "        gate_num = np.argmax(line[0])\n",
        "        target = str(np.argmax(line[1]))\n",
        "        if gate_num==0:\n",
        "            decoded += \"h q[\"+target\n",
        "        if gate_num==1:\n",
        "            decoded += \"s q[\"+target\n",
        "        if gate_num==2:\n",
        "            control = str(np.argmax(line[2]))\n",
        "            decoded += \"cx q[\"+control+\"],q[\"+target\n",
        "        if gate_num < 3:\n",
        "            decoded += \"];\"\n",
        "        if gate_num==4:\n",
        "            decoded += \"\\n\"\n",
        "    return qk.QuantumCircuit.from_qasm_str(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi4rG-xPFdjZ",
        "colab_type": "text"
      },
      "source": [
        "Old loss function, uses simulation to test generated circuits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnTdN252p-Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function\n",
        "@tf.function\n",
        "def simulation_loss(y_true,y_pred):\n",
        "    # global N\n",
        "    decoded = decode_circuit(y_pred,5)\n",
        "    y_pred = generate_labels([decoded])[0]\n",
        "    return K.mse(y_true,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQiIpTokB9Zv",
        "colab_type": "text"
      },
      "source": [
        "Failed Model: OHE Gate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yTKfPwLpFaKK",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "N = 5\n",
        "VOCAB_DIM = 2+2*N+N**2\n",
        "NUM_SAMPLES = 1000\n",
        "TRAINTEST = 0.75\n",
        "CUTOFF = int(TIME_STEPS*NUM_SAMPLES//2)#(1/TRAINTEST))\n",
        "MAX_LENGTH = 10\n",
        "TIME_STEPS = MAX_LENGTH+2\n",
        "COND_DIM = 2*(2**N)\n",
        "NUM_CELLS = 256\n",
        "STACK_DEPTH = 5\n",
        "PRINT_DELAY = 100\n",
        "SAVE_DELAY = 10000\n",
        "\n",
        "# Training Parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yycAySmfFaKP",
        "colab": {}
      },
      "source": [
        "# Generate Data\n",
        "#Data = encode_circuits(sample_circuits(N,MAX_LENGTH,NUM_SAMPLES),N,MAX_LENGTH,label=True)\n",
        "# Save Data\n",
        "# np.savetxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", Data, delimiter=\",\")\n",
        "# Load Data\n",
        "# Data = np.loadtxt(f\"Encoded_Circuits_{str(N)}_{str(MAX_LENGTH)}_{str(NUM_SAMPLES)}.csv\", delimiter=\",\")\n",
        "np.random.shuffle(Data)\n",
        "X = Data[:,:TIME_STEPS]\n",
        "# Convert to onehot\n",
        "X = np.array([[eye[int(gate)] for gate in X[int(row),:]] for row in range(X.shape[0])])\n",
        "c = Data[:,TIME_STEPS:]\n",
        "# Flatten\n",
        "X = X.reshape((NUM_SAMPLES*TIME_STEPS,1,VOCAB_DIM))\n",
        "offset = X[1:,:]\n",
        "y = np.concatenate((offset,np.zeros((1,1,VOCAB_DIM))),0)\n",
        "c = np.repeat(c,12,0)\n",
        "\n",
        "X_train = X[:CUTOFF]\n",
        "X_test = X[CUTOFF:]\n",
        "c_test = c[:CUTOFF]\n",
        "c_train = c[CUTOFF:]\n",
        "y_train = y[:CUTOFF]\n",
        "y_test = y[CUTOFF:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cKd3VGrYFaKR",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Placeholders.\n",
        "inputs = tf.placeholder(name='inputs', dtype=tf.float32, shape=(None, 1, VOCAB_DIM))\n",
        "targets = tf.placeholder(name='targets', dtype=tf.float32, shape=(None, 1, VOCAB_DIM))\n",
        "cond = tf.placeholder(name='conditions', dtype=tf.float32, shape=(None, COND_DIM))\n",
        "\n",
        "# Conditional RNN.\n",
        "outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "for _ in range(STACK_DEPTH-1):\n",
        "    outputs = crnn.ConditionalRNN(NUM_CELLS, cell='GRU', cond=cond, dtype=tf.float32, return_sequences=True)(inputs)\n",
        "\n",
        "# Classification layer.\n",
        "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=VOCAB_DIM, activation='softmax'))(outputs)\n",
        "\n",
        "# Loss + Optimizer.\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Initialize variables (tensorflow)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Define the binding between placeholders and real data.\n",
        "train_feed_dict = {inputs: X_train, targets: y_train, cond: c_train}\n",
        "test_feed_dict = {inputs: X_test, targets: y_test, cond: c_test}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dd07d830-c199-46cb-ad9c-dda44306c4aa",
        "id": "D8m1QBwtFaKT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Main loop. Optimize then evaluate.\n",
        "saver = tf.train.Saver()\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    sess.run(optimizer, train_feed_dict)\n",
        "    if epoch % PRINT_DELAY == 0:\n",
        "        train_outputs, train_loss = sess.run([outputs, cost], train_feed_dict)\n",
        "        test_outputs, test_loss = sess.run([outputs, cost], test_feed_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f'[{str(epoch).zfill(4)}] train cost = {train_loss:.4f}, test cost = {test_loss:.4f}.')\n",
        "    if epoch % SAVE_DELAY == 0:\n",
        "        saver.save(sess, f'Checkpoints/QCG-ohe-g-{str(NUM_CELLS)}', global_step=int(epoch/SAVE_DELAY))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0000] train cost = 3.6105, test cost = 3.6100.\n",
            "[0100] train cost = 3.4769, test cost = 3.4875.\n",
            "[0200] train cost = 3.4636, test cost = 3.4891.\n",
            "[0300] train cost = 3.4519, test cost = 3.4892.\n",
            "[0400] train cost = 3.4413, test cost = 3.4910.\n",
            "[0500] train cost = 3.4370, test cost = 3.4907.\n",
            "[0600] train cost = 3.4347, test cost = 3.4908.\n",
            "[0700] train cost = 3.4130, test cost = 3.4932.\n",
            "[0800] train cost = 3.4036, test cost = 3.4934.\n",
            "[0900] train cost = 3.3998, test cost = 3.4930.\n",
            "[1000] train cost = 3.3923, test cost = 3.4940.\n",
            "[1100] train cost = 3.3846, test cost = 3.4948.\n",
            "[1200] train cost = 3.3804, test cost = 3.4932.\n",
            "[1300] train cost = 3.3738, test cost = 3.4925.\n",
            "[1400] train cost = 3.3681, test cost = 3.4922.\n",
            "[1500] train cost = 3.3655, test cost = 3.4932.\n",
            "[1600] train cost = 3.3596, test cost = 3.4928.\n",
            "[1700] train cost = 3.3538, test cost = 3.4937.\n",
            "[1800] train cost = 3.3519, test cost = 3.4938.\n",
            "[1900] train cost = 3.3496, test cost = 3.4939.\n",
            "[2000] train cost = 3.3456, test cost = 3.4938.\n",
            "[2100] train cost = 3.3397, test cost = 3.4934.\n",
            "[2200] train cost = 3.3324, test cost = 3.4930.\n",
            "[2300] train cost = 3.3288, test cost = 3.4931.\n",
            "[2400] train cost = 3.3258, test cost = 3.4934.\n",
            "[2500] train cost = 3.3238, test cost = 3.4933.\n",
            "[2600] train cost = 3.3210, test cost = 3.4933.\n",
            "[2700] train cost = 3.3186, test cost = 3.4935.\n",
            "[2800] train cost = 3.3098, test cost = 3.4950.\n",
            "[2900] train cost = 3.3071, test cost = 3.4959.\n",
            "[3000] train cost = 3.3045, test cost = 3.4959.\n",
            "[3100] train cost = 3.3009, test cost = 3.4971.\n",
            "[3200] train cost = 3.2989, test cost = 3.4969.\n",
            "[3300] train cost = 3.2959, test cost = 3.4968.\n",
            "[3400] train cost = 3.2921, test cost = 3.4971.\n",
            "[3500] train cost = 3.2893, test cost = 3.4968.\n",
            "[3600] train cost = 3.2874, test cost = 3.4972.\n",
            "[3700] train cost = 3.2849, test cost = 3.4978.\n",
            "[3800] train cost = 3.2821, test cost = 3.4970.\n",
            "[3900] train cost = 3.2785, test cost = 3.4975.\n",
            "[4000] train cost = 3.2761, test cost = 3.4976.\n",
            "[4100] train cost = 3.2732, test cost = 3.4971.\n",
            "[4200] train cost = 3.2690, test cost = 3.4978.\n",
            "[4300] train cost = 3.2662, test cost = 3.4978.\n",
            "[4400] train cost = 3.2652, test cost = 3.4979.\n",
            "[4500] train cost = 3.2633, test cost = 3.4973.\n",
            "[4600] train cost = 3.2615, test cost = 3.4978.\n",
            "[4700] train cost = 3.2597, test cost = 3.4980.\n",
            "[4800] train cost = 3.2570, test cost = 3.4989.\n",
            "[4900] train cost = 3.2525, test cost = 3.4988.\n",
            "[5000] train cost = 3.2482, test cost = 3.4986.\n",
            "[5100] train cost = 3.2421, test cost = 3.4998.\n",
            "[5200] train cost = 3.2382, test cost = 3.5014.\n",
            "[5300] train cost = 3.2352, test cost = 3.5010.\n",
            "[5400] train cost = 3.2320, test cost = 3.5017.\n",
            "[5500] train cost = 3.2302, test cost = 3.5017.\n",
            "[5600] train cost = 3.2266, test cost = 3.5020.\n",
            "[5700] train cost = 3.2230, test cost = 3.5019.\n",
            "[5800] train cost = 3.2209, test cost = 3.5019.\n",
            "[5900] train cost = 3.2195, test cost = 3.5024.\n",
            "[6000] train cost = 3.2172, test cost = 3.5025.\n",
            "[6100] train cost = 3.2149, test cost = 3.5023.\n",
            "[6200] train cost = 3.2122, test cost = 3.5027.\n",
            "[6300] train cost = 3.2096, test cost = 3.5034.\n",
            "[6400] train cost = 3.2056, test cost = 3.5030.\n",
            "[6500] train cost = 3.2036, test cost = 3.5031.\n",
            "[6600] train cost = 3.1997, test cost = 3.5028.\n",
            "[6700] train cost = 3.1973, test cost = 3.5034.\n",
            "[6800] train cost = 3.1947, test cost = 3.5042.\n",
            "[6900] train cost = 3.1913, test cost = 3.5039.\n",
            "[7000] train cost = 3.1892, test cost = 3.5036.\n",
            "[7100] train cost = 3.1872, test cost = 3.5047.\n",
            "[7200] train cost = 3.1842, test cost = 3.5041.\n",
            "[7300] train cost = 3.1823, test cost = 3.5042.\n",
            "[7400] train cost = 3.1807, test cost = 3.5049.\n",
            "[7500] train cost = 3.1782, test cost = 3.5050.\n",
            "[7600] train cost = 3.1764, test cost = 3.5052.\n",
            "[7700] train cost = 3.1737, test cost = 3.5058.\n",
            "[7800] train cost = 3.1715, test cost = 3.5058.\n",
            "[7900] train cost = 3.1703, test cost = 3.5056.\n",
            "[8000] train cost = 3.1686, test cost = 3.5055.\n",
            "[8100] train cost = 3.1663, test cost = 3.5058.\n",
            "[8200] train cost = 3.1634, test cost = 3.5062.\n",
            "[8300] train cost = 3.1622, test cost = 3.5065.\n",
            "[8400] train cost = 3.1603, test cost = 3.5066.\n",
            "[8500] train cost = 3.1586, test cost = 3.5069.\n",
            "[8600] train cost = 3.1561, test cost = 3.5074.\n",
            "[8700] train cost = 3.1548, test cost = 3.5078.\n",
            "[8800] train cost = 3.1528, test cost = 3.5079.\n",
            "[8900] train cost = 3.1511, test cost = 3.5086.\n",
            "[9000] train cost = 3.1498, test cost = 3.5082.\n",
            "[9100] train cost = 3.1486, test cost = 3.5083.\n",
            "[9200] train cost = 3.1465, test cost = 3.5081.\n",
            "[9300] train cost = 3.1455, test cost = 3.5089.\n",
            "[9400] train cost = 3.1437, test cost = 3.5087.\n",
            "[9500] train cost = 3.1426, test cost = 3.5084.\n",
            "[9600] train cost = 3.1411, test cost = 3.5078.\n",
            "[9700] train cost = 3.1392, test cost = 3.5080.\n",
            "[9800] train cost = 3.1373, test cost = 3.5077.\n",
            "[9900] train cost = 3.1351, test cost = 3.5078.\n",
            "[10000] train cost = 3.1314, test cost = 3.5084.\n",
            "[10100] train cost = 3.1300, test cost = 3.5091.\n",
            "[10200] train cost = 3.1284, test cost = 3.5100.\n",
            "[10300] train cost = 3.1265, test cost = 3.5101.\n",
            "[10400] train cost = 3.1254, test cost = 3.5100.\n",
            "[10500] train cost = 3.1232, test cost = 3.5114.\n",
            "[10600] train cost = 3.1217, test cost = 3.5116.\n",
            "[10700] train cost = 3.1205, test cost = 3.5116.\n",
            "[10800] train cost = 3.1194, test cost = 3.5113.\n",
            "[10900] train cost = 3.1187, test cost = 3.5114.\n",
            "[11000] train cost = 3.1174, test cost = 3.5112.\n",
            "[11100] train cost = 3.1167, test cost = 3.5110.\n",
            "[11200] train cost = 3.1154, test cost = 3.5112.\n",
            "[11300] train cost = 3.1144, test cost = 3.5114.\n",
            "[11400] train cost = 3.1129, test cost = 3.5115.\n",
            "[11500] train cost = 3.1123, test cost = 3.5119.\n",
            "[11600] train cost = 3.1110, test cost = 3.5123.\n",
            "[11700] train cost = 3.1093, test cost = 3.5126.\n",
            "[11800] train cost = 3.1084, test cost = 3.5116.\n",
            "[11900] train cost = 3.1072, test cost = 3.5121.\n",
            "[12000] train cost = 3.1055, test cost = 3.5129.\n",
            "[12100] train cost = 3.1045, test cost = 3.5126.\n",
            "[12200] train cost = 3.1025, test cost = 3.5127.\n",
            "[12300] train cost = 3.1018, test cost = 3.5129.\n",
            "[12400] train cost = 3.1015, test cost = 3.5135.\n",
            "[12500] train cost = 3.1002, test cost = 3.5142.\n",
            "[12600] train cost = 3.0985, test cost = 3.5136.\n",
            "[12700] train cost = 3.0972, test cost = 3.5136.\n",
            "[12800] train cost = 3.0967, test cost = 3.5133.\n",
            "[12900] train cost = 3.0957, test cost = 3.5129.\n",
            "[13000] train cost = 3.0957, test cost = 3.5129.\n",
            "[13100] train cost = 3.0946, test cost = 3.5135.\n",
            "[13200] train cost = 3.0939, test cost = 3.5129.\n",
            "[13300] train cost = 3.0934, test cost = 3.5127.\n",
            "[13400] train cost = 3.0932, test cost = 3.5127.\n",
            "[13500] train cost = 3.0927, test cost = 3.5126.\n",
            "[13600] train cost = 3.0927, test cost = 3.5126.\n",
            "[13700] train cost = 3.0926, test cost = 3.5123.\n",
            "[13800] train cost = 3.0920, test cost = 3.5125.\n",
            "[13900] train cost = 3.0913, test cost = 3.5123.\n",
            "[14000] train cost = 3.0909, test cost = 3.5122.\n",
            "[14100] train cost = 3.0909, test cost = 3.5121.\n",
            "[14200] train cost = 3.0904, test cost = 3.5118.\n",
            "[14300] train cost = 3.0896, test cost = 3.5124.\n",
            "[14400] train cost = 3.0888, test cost = 3.5123.\n",
            "[14500] train cost = 3.0880, test cost = 3.5124.\n",
            "[14600] train cost = 3.0870, test cost = 3.5120.\n",
            "[14700] train cost = 3.0862, test cost = 3.5136.\n",
            "[14800] train cost = 3.0857, test cost = 3.5136.\n",
            "[14900] train cost = 3.0847, test cost = 3.5139.\n",
            "[15000] train cost = 3.0842, test cost = 3.5145.\n",
            "[15100] train cost = 3.0834, test cost = 3.5146.\n",
            "[15200] train cost = 3.0822, test cost = 3.5146.\n",
            "[15300] train cost = 3.0811, test cost = 3.5158.\n",
            "[15400] train cost = 3.0799, test cost = 3.5159.\n",
            "[15500] train cost = 3.0792, test cost = 3.5151.\n",
            "[15600] train cost = 3.0790, test cost = 3.5150.\n",
            "[15700] train cost = 3.0779, test cost = 3.5152.\n",
            "[15800] train cost = 3.0776, test cost = 3.5143.\n",
            "[15900] train cost = 3.0772, test cost = 3.5137.\n",
            "[16000] train cost = 3.0769, test cost = 3.5144.\n",
            "[16100] train cost = 3.0766, test cost = 3.5140.\n",
            "[16200] train cost = 3.0757, test cost = 3.5144.\n",
            "[16300] train cost = 3.0754, test cost = 3.5144.\n",
            "[16400] train cost = 3.0752, test cost = 3.5147.\n",
            "[16500] train cost = 3.0746, test cost = 3.5146.\n",
            "[16600] train cost = 3.0739, test cost = 3.5149.\n",
            "[16700] train cost = 3.0730, test cost = 3.5152.\n",
            "[16800] train cost = 3.0726, test cost = 3.5154.\n",
            "[16900] train cost = 3.0716, test cost = 3.5149.\n",
            "[17000] train cost = 3.0713, test cost = 3.5151.\n",
            "[17100] train cost = 3.0708, test cost = 3.5155.\n",
            "[17200] train cost = 3.0706, test cost = 3.5152.\n",
            "[17300] train cost = 3.0701, test cost = 3.5148.\n",
            "[17400] train cost = 3.0693, test cost = 3.5138.\n",
            "[17500] train cost = 3.0693, test cost = 3.5139.\n",
            "[17600] train cost = 3.0686, test cost = 3.5142.\n",
            "[17700] train cost = 3.0681, test cost = 3.5145.\n",
            "[17800] train cost = 3.0673, test cost = 3.5147.\n",
            "[17900] train cost = 3.0667, test cost = 3.5143.\n",
            "[18000] train cost = 3.0660, test cost = 3.5144.\n",
            "[18100] train cost = 3.0660, test cost = 3.5145.\n",
            "[18200] train cost = 3.0653, test cost = 3.5140.\n",
            "[18300] train cost = 3.0650, test cost = 3.5144.\n",
            "[18400] train cost = 3.0647, test cost = 3.5140.\n",
            "[18500] train cost = 3.0645, test cost = 3.5142.\n",
            "[18600] train cost = 3.0645, test cost = 3.5140.\n",
            "[18700] train cost = 3.0639, test cost = 3.5143.\n",
            "[18800] train cost = 3.0634, test cost = 3.5143.\n",
            "[18900] train cost = 3.0634, test cost = 3.5142.\n",
            "[19000] train cost = 3.0630, test cost = 3.5141.\n",
            "[19100] train cost = 3.0627, test cost = 3.5143.\n",
            "[19200] train cost = 3.0624, test cost = 3.5140.\n",
            "[19300] train cost = 3.0624, test cost = 3.5141.\n",
            "[19400] train cost = 3.0616, test cost = 3.5147.\n",
            "[19500] train cost = 3.0608, test cost = 3.5143.\n",
            "[19600] train cost = 3.0603, test cost = 3.5143.\n",
            "[19700] train cost = 3.0593, test cost = 3.5140.\n",
            "[19800] train cost = 3.0583, test cost = 3.5142.\n",
            "[19900] train cost = 3.0578, test cost = 3.5142.\n",
            "[20000] train cost = 3.0575, test cost = 3.5139.\n",
            "[20100] train cost = 3.0573, test cost = 3.5135.\n",
            "[20200] train cost = 3.0571, test cost = 3.5128.\n",
            "[20300] train cost = 3.0570, test cost = 3.5136.\n",
            "[20400] train cost = 3.0568, test cost = 3.5127.\n",
            "[20500] train cost = 3.0560, test cost = 3.5132.\n",
            "[20600] train cost = 3.0558, test cost = 3.5128.\n",
            "[20700] train cost = 3.0553, test cost = 3.5131.\n",
            "[20800] train cost = 3.0552, test cost = 3.5134.\n",
            "[20900] train cost = 3.0550, test cost = 3.5129.\n",
            "[21000] train cost = 3.0547, test cost = 3.5129.\n",
            "[21100] train cost = 3.0547, test cost = 3.5128.\n",
            "[21200] train cost = 3.0545, test cost = 3.5133.\n",
            "[21300] train cost = 3.0544, test cost = 3.5131.\n",
            "[21400] train cost = 3.0543, test cost = 3.5129.\n",
            "[21500] train cost = 3.0543, test cost = 3.5128.\n",
            "[21600] train cost = 3.0542, test cost = 3.5131.\n",
            "[21700] train cost = 3.0540, test cost = 3.5130.\n",
            "[21800] train cost = 3.0540, test cost = 3.5130.\n",
            "[21900] train cost = 3.0540, test cost = 3.5131.\n",
            "[22000] train cost = 3.0538, test cost = 3.5130.\n",
            "[22100] train cost = 3.0538, test cost = 3.5130.\n",
            "[22200] train cost = 3.0537, test cost = 3.5133.\n",
            "[22300] train cost = 3.0535, test cost = 3.5130.\n",
            "[22400] train cost = 3.0533, test cost = 3.5132.\n",
            "[22500] train cost = 3.0527, test cost = 3.5135.\n",
            "[22600] train cost = 3.0519, test cost = 3.5136.\n",
            "[22700] train cost = 3.0518, test cost = 3.5135.\n",
            "[22800] train cost = 3.0518, test cost = 3.5135.\n",
            "[22900] train cost = 3.0515, test cost = 3.5137.\n",
            "[23000] train cost = 3.0512, test cost = 3.5139.\n",
            "[23100] train cost = 3.0512, test cost = 3.5139.\n",
            "[23200] train cost = 3.0509, test cost = 3.5138.\n",
            "[23300] train cost = 3.0509, test cost = 3.5137.\n",
            "[23400] train cost = 3.0509, test cost = 3.5137.\n",
            "[23500] train cost = 3.0509, test cost = 3.5142.\n",
            "[23600] train cost = 3.0505, test cost = 3.5134.\n",
            "[23700] train cost = 3.0502, test cost = 3.5136.\n",
            "[23800] train cost = 3.0499, test cost = 3.5131.\n",
            "[23900] train cost = 3.0497, test cost = 3.5130.\n",
            "[24000] train cost = 3.0494, test cost = 3.5130.\n",
            "[24100] train cost = 3.0494, test cost = 3.5131.\n",
            "[24200] train cost = 3.0492, test cost = 3.5133.\n",
            "[24300] train cost = 3.0489, test cost = 3.5138.\n",
            "[24400] train cost = 3.0484, test cost = 3.5134.\n",
            "[24500] train cost = 3.0482, test cost = 3.5134.\n",
            "[24600] train cost = 3.0477, test cost = 3.5134.\n",
            "[24700] train cost = 3.0476, test cost = 3.5136.\n",
            "[24800] train cost = 3.0473, test cost = 3.5134.\n",
            "[24900] train cost = 3.0469, test cost = 3.5136.\n",
            "[25000] train cost = 3.0468, test cost = 3.5131.\n",
            "[25100] train cost = 3.0468, test cost = 3.5132.\n",
            "[25200] train cost = 3.0463, test cost = 3.5135.\n",
            "[25300] train cost = 3.0461, test cost = 3.5134.\n",
            "[25400] train cost = 3.0461, test cost = 3.5132.\n",
            "[25500] train cost = 3.0458, test cost = 3.5138.\n",
            "[25600] train cost = 3.0456, test cost = 3.5138.\n",
            "[25700] train cost = 3.0453, test cost = 3.5135.\n",
            "[25800] train cost = 3.0451, test cost = 3.5133.\n",
            "[25900] train cost = 3.0448, test cost = 3.5129.\n",
            "[26000] train cost = 3.0441, test cost = 3.5127.\n",
            "[26100] train cost = 3.0438, test cost = 3.5133.\n",
            "[26200] train cost = 3.0437, test cost = 3.5133.\n",
            "[26300] train cost = 3.0434, test cost = 3.5138.\n",
            "[26400] train cost = 3.0428, test cost = 3.5144.\n",
            "[26500] train cost = 3.0427, test cost = 3.5141.\n",
            "[26600] train cost = 3.0421, test cost = 3.5140.\n",
            "[26700] train cost = 3.0418, test cost = 3.5142.\n",
            "[26800] train cost = 3.0416, test cost = 3.5137.\n",
            "[26900] train cost = 3.0416, test cost = 3.5137.\n",
            "[27000] train cost = 3.0414, test cost = 3.5134.\n",
            "[27100] train cost = 3.0413, test cost = 3.5141.\n",
            "[27200] train cost = 3.0408, test cost = 3.5130.\n",
            "[27300] train cost = 3.0406, test cost = 3.5129.\n",
            "[27400] train cost = 3.0405, test cost = 3.5137.\n",
            "[27500] train cost = 3.0404, test cost = 3.5133.\n",
            "[27600] train cost = 3.0404, test cost = 3.5133.\n",
            "[27700] train cost = 3.0403, test cost = 3.5131.\n",
            "[27800] train cost = 3.0403, test cost = 3.5131.\n",
            "[27900] train cost = 3.0400, test cost = 3.5134.\n",
            "[28000] train cost = 3.0398, test cost = 3.5137.\n",
            "[28100] train cost = 3.0396, test cost = 3.5135.\n",
            "[28200] train cost = 3.0395, test cost = 3.5137.\n",
            "[28300] train cost = 3.0393, test cost = 3.5139.\n",
            "[28400] train cost = 3.0391, test cost = 3.5136.\n",
            "[28500] train cost = 3.0391, test cost = 3.5131.\n",
            "[28600] train cost = 3.0390, test cost = 3.5132.\n",
            "[28700] train cost = 3.0386, test cost = 3.5127.\n",
            "[28800] train cost = 3.0386, test cost = 3.5128.\n",
            "[28900] train cost = 3.0386, test cost = 3.5128.\n",
            "[29000] train cost = 3.0383, test cost = 3.5126.\n",
            "[29100] train cost = 3.0380, test cost = 3.5127.\n",
            "[29200] train cost = 3.0378, test cost = 3.5127.\n",
            "[29300] train cost = 3.0373, test cost = 3.5122.\n",
            "[29400] train cost = 3.0372, test cost = 3.5125.\n",
            "[29500] train cost = 3.0368, test cost = 3.5122.\n",
            "[29600] train cost = 3.0364, test cost = 3.5120.\n",
            "[29700] train cost = 3.0363, test cost = 3.5119.\n",
            "[29800] train cost = 3.0358, test cost = 3.5122.\n",
            "[29900] train cost = 3.0357, test cost = 3.5125.\n",
            "[30000] train cost = 3.0355, test cost = 3.5126.\n",
            "[30100] train cost = 3.0352, test cost = 3.5123.\n",
            "[30200] train cost = 3.0352, test cost = 3.5122.\n",
            "[30300] train cost = 3.0352, test cost = 3.5121.\n",
            "[30400] train cost = 3.0352, test cost = 3.5121.\n",
            "[30500] train cost = 3.0352, test cost = 3.5121.\n",
            "[30600] train cost = 3.0348, test cost = 3.5121.\n",
            "[30700] train cost = 3.0348, test cost = 3.5120.\n",
            "[30800] train cost = 3.0345, test cost = 3.5125.\n",
            "[30900] train cost = 3.0345, test cost = 3.5123.\n",
            "[31000] train cost = 3.0344, test cost = 3.5123.\n",
            "[31100] train cost = 3.0340, test cost = 3.5125.\n",
            "[31200] train cost = 3.0340, test cost = 3.5126.\n",
            "[31300] train cost = 3.0337, test cost = 3.5130.\n",
            "[31400] train cost = 3.0330, test cost = 3.5135.\n",
            "[31500] train cost = 3.0330, test cost = 3.5134.\n",
            "[31600] train cost = 3.0329, test cost = 3.5138.\n",
            "[31700] train cost = 3.0329, test cost = 3.5138.\n",
            "[31800] train cost = 3.0328, test cost = 3.5138.\n",
            "[31900] train cost = 3.0325, test cost = 3.5134.\n",
            "[32000] train cost = 3.0325, test cost = 3.5135.\n",
            "[32100] train cost = 3.0325, test cost = 3.5135.\n",
            "[32200] train cost = 3.0322, test cost = 3.5137.\n",
            "[32300] train cost = 3.0322, test cost = 3.5138.\n",
            "[32400] train cost = 3.0322, test cost = 3.5137.\n",
            "[32500] train cost = 3.0320, test cost = 3.5139.\n",
            "[32600] train cost = 3.0320, test cost = 3.5138.\n",
            "[32700] train cost = 3.0317, test cost = 3.5140.\n",
            "[32800] train cost = 3.0311, test cost = 3.5155.\n",
            "[32900] train cost = 3.0307, test cost = 3.5150.\n",
            "[33000] train cost = 3.0306, test cost = 3.5150.\n",
            "[33100] train cost = 3.0303, test cost = 3.5148.\n",
            "[33200] train cost = 3.0302, test cost = 3.5145.\n",
            "[33300] train cost = 3.0302, test cost = 3.5145.\n",
            "[33400] train cost = 3.0302, test cost = 3.5145.\n",
            "[33500] train cost = 3.0302, test cost = 3.5145.\n",
            "[33600] train cost = 3.0302, test cost = 3.5145.\n",
            "[33700] train cost = 3.0302, test cost = 3.5144.\n",
            "[33800] train cost = 3.0301, test cost = 3.5138.\n",
            "[33900] train cost = 3.0297, test cost = 3.5138.\n",
            "[34000] train cost = 3.0294, test cost = 3.5139.\n",
            "[34100] train cost = 3.0294, test cost = 3.5137.\n",
            "[34200] train cost = 3.0289, test cost = 3.5140.\n",
            "[34300] train cost = 3.0289, test cost = 3.5137.\n",
            "[34400] train cost = 3.0289, test cost = 3.5135.\n",
            "[34500] train cost = 3.0284, test cost = 3.5133.\n",
            "[34600] train cost = 3.0279, test cost = 3.5132.\n",
            "[34700] train cost = 3.0279, test cost = 3.5129.\n",
            "[34800] train cost = 3.0279, test cost = 3.5128.\n",
            "[34900] train cost = 3.0276, test cost = 3.5126.\n",
            "[35000] train cost = 3.0276, test cost = 3.5126.\n",
            "[35100] train cost = 3.0274, test cost = 3.5126.\n",
            "[35200] train cost = 3.0273, test cost = 3.5123.\n",
            "[35300] train cost = 3.0273, test cost = 3.5123.\n",
            "[35400] train cost = 3.0271, test cost = 3.5129.\n",
            "[35500] train cost = 3.0271, test cost = 3.5128.\n",
            "[35600] train cost = 3.0269, test cost = 3.5133.\n",
            "[35700] train cost = 3.0268, test cost = 3.5136.\n",
            "[35800] train cost = 3.0266, test cost = 3.5137.\n",
            "[35900] train cost = 3.0266, test cost = 3.5138.\n",
            "[36000] train cost = 3.0266, test cost = 3.5138.\n",
            "[36100] train cost = 3.0266, test cost = 3.5137.\n",
            "[36200] train cost = 3.0266, test cost = 3.5137.\n",
            "[36300] train cost = 3.0264, test cost = 3.5134.\n",
            "[36400] train cost = 3.0264, test cost = 3.5134.\n",
            "[36500] train cost = 3.0263, test cost = 3.5137.\n",
            "[36600] train cost = 3.0263, test cost = 3.5137.\n",
            "[36700] train cost = 3.0263, test cost = 3.5137.\n",
            "[36800] train cost = 3.0263, test cost = 3.5138.\n",
            "[36900] train cost = 3.0263, test cost = 3.5138.\n",
            "[37000] train cost = 3.0261, test cost = 3.5136.\n",
            "[37100] train cost = 3.0259, test cost = 3.5137.\n",
            "[37200] train cost = 3.0258, test cost = 3.5139.\n",
            "[37300] train cost = 3.0258, test cost = 3.5139.\n",
            "[37400] train cost = 3.0256, test cost = 3.5140.\n",
            "[37500] train cost = 3.0256, test cost = 3.5137.\n",
            "[37600] train cost = 3.0256, test cost = 3.5138.\n",
            "[37700] train cost = 3.0256, test cost = 3.5137.\n",
            "[37800] train cost = 3.0256, test cost = 3.5137.\n",
            "[37900] train cost = 3.0256, test cost = 3.5137.\n",
            "[38000] train cost = 3.0256, test cost = 3.5137.\n",
            "[38100] train cost = 3.0254, test cost = 3.5140.\n",
            "[38200] train cost = 3.0251, test cost = 3.5139.\n",
            "[38300] train cost = 3.0249, test cost = 3.5141.\n",
            "[38400] train cost = 3.0245, test cost = 3.5146.\n",
            "[38500] train cost = 3.0243, test cost = 3.5146.\n",
            "[38600] train cost = 3.0241, test cost = 3.5148.\n",
            "[38700] train cost = 3.0241, test cost = 3.5148.\n",
            "[38800] train cost = 3.0238, test cost = 3.5150.\n",
            "[38900] train cost = 3.0238, test cost = 3.5149.\n",
            "[39000] train cost = 3.0238, test cost = 3.5149.\n",
            "[39100] train cost = 3.0238, test cost = 3.5149.\n",
            "[39200] train cost = 3.0237, test cost = 3.5143.\n",
            "[39300] train cost = 3.0236, test cost = 3.5145.\n",
            "[39400] train cost = 3.0236, test cost = 3.5146.\n",
            "[39500] train cost = 3.0235, test cost = 3.5141.\n",
            "[39600] train cost = 3.0235, test cost = 3.5142.\n",
            "[39700] train cost = 3.0235, test cost = 3.5141.\n",
            "[39800] train cost = 3.0232, test cost = 3.5136.\n",
            "[39900] train cost = 3.0230, test cost = 3.5134.\n",
            "[40000] train cost = 3.0230, test cost = 3.5133.\n",
            "[40100] train cost = 3.0228, test cost = 3.5136.\n",
            "[40200] train cost = 3.0228, test cost = 3.5136.\n",
            "[40300] train cost = 3.0228, test cost = 3.5136.\n",
            "[40400] train cost = 3.0228, test cost = 3.5136.\n",
            "[40500] train cost = 3.0227, test cost = 3.5142.\n",
            "[40600] train cost = 3.0227, test cost = 3.5140.\n",
            "[40700] train cost = 3.0227, test cost = 3.5140.\n",
            "[40800] train cost = 3.0225, test cost = 3.5140.\n",
            "[40900] train cost = 3.0222, test cost = 3.5144.\n",
            "[41000] train cost = 3.0222, test cost = 3.5141.\n",
            "[41100] train cost = 3.0220, test cost = 3.5147.\n",
            "[41200] train cost = 3.0218, test cost = 3.5152.\n",
            "[41300] train cost = 3.0218, test cost = 3.5149.\n",
            "[41400] train cost = 3.0215, test cost = 3.5142.\n",
            "[41500] train cost = 3.0215, test cost = 3.5141.\n",
            "[41600] train cost = 3.0212, test cost = 3.5139.\n",
            "[41700] train cost = 3.0212, test cost = 3.5137.\n",
            "[41800] train cost = 3.0212, test cost = 3.5138.\n",
            "[41900] train cost = 3.0210, test cost = 3.5138.\n",
            "[42000] train cost = 3.0210, test cost = 3.5137.\n",
            "[42100] train cost = 3.0207, test cost = 3.5148.\n",
            "[42200] train cost = 3.0207, test cost = 3.5144.\n",
            "[42300] train cost = 3.0205, test cost = 3.5142.\n",
            "[42400] train cost = 3.0204, test cost = 3.5144.\n",
            "[42500] train cost = 3.0204, test cost = 3.5147.\n",
            "[42600] train cost = 3.0204, test cost = 3.5146.\n",
            "[42700] train cost = 3.0202, test cost = 3.5146.\n",
            "[42800] train cost = 3.0200, test cost = 3.5145.\n",
            "[42900] train cost = 3.0197, test cost = 3.5147.\n",
            "[43000] train cost = 3.0195, test cost = 3.5150.\n",
            "[43100] train cost = 3.0187, test cost = 3.5151.\n",
            "[43200] train cost = 3.0184, test cost = 3.5147.\n",
            "[43300] train cost = 3.0184, test cost = 3.5146.\n",
            "[43400] train cost = 3.0184, test cost = 3.5146.\n",
            "[43500] train cost = 3.0184, test cost = 3.5145.\n",
            "[43600] train cost = 3.0184, test cost = 3.5144.\n",
            "[43700] train cost = 3.0184, test cost = 3.5144.\n",
            "[43800] train cost = 3.0181, test cost = 3.5149.\n",
            "[43900] train cost = 3.0179, test cost = 3.5140.\n",
            "[44000] train cost = 3.0177, test cost = 3.5142.\n",
            "[44100] train cost = 3.0177, test cost = 3.5143.\n",
            "[44200] train cost = 3.0176, test cost = 3.5137.\n",
            "[44300] train cost = 3.0172, test cost = 3.5135.\n",
            "[44400] train cost = 3.0172, test cost = 3.5135.\n",
            "[44500] train cost = 3.0172, test cost = 3.5135.\n",
            "[44600] train cost = 3.0171, test cost = 3.5139.\n",
            "[44700] train cost = 3.0169, test cost = 3.5141.\n",
            "[44800] train cost = 3.0167, test cost = 3.5137.\n",
            "[44900] train cost = 3.0167, test cost = 3.5134.\n",
            "[45000] train cost = 3.0167, test cost = 3.5134.\n",
            "[45100] train cost = 3.0167, test cost = 3.5133.\n",
            "[45200] train cost = 3.0167, test cost = 3.5133.\n",
            "[45300] train cost = 3.0167, test cost = 3.5132.\n",
            "[45400] train cost = 3.0161, test cost = 3.5135.\n",
            "[45500] train cost = 3.0161, test cost = 3.5134.\n",
            "[45600] train cost = 3.0159, test cost = 3.5136.\n",
            "[45700] train cost = 3.0159, test cost = 3.5135.\n",
            "[45800] train cost = 3.0159, test cost = 3.5134.\n",
            "[45900] train cost = 3.0154, test cost = 3.5135.\n",
            "[46000] train cost = 3.0154, test cost = 3.5133.\n",
            "[46100] train cost = 3.0149, test cost = 3.5138.\n",
            "[46200] train cost = 3.0149, test cost = 3.5138.\n",
            "[46300] train cost = 3.0149, test cost = 3.5137.\n",
            "[46400] train cost = 3.0148, test cost = 3.5135.\n",
            "[46500] train cost = 3.0148, test cost = 3.5134.\n",
            "[46600] train cost = 3.0146, test cost = 3.5135.\n",
            "[46700] train cost = 3.0146, test cost = 3.5133.\n",
            "[46800] train cost = 3.0144, test cost = 3.5134.\n",
            "[46900] train cost = 3.0144, test cost = 3.5134.\n",
            "[47000] train cost = 3.0144, test cost = 3.5135.\n",
            "[47100] train cost = 3.0143, test cost = 3.5133.\n",
            "[47200] train cost = 3.0143, test cost = 3.5133.\n",
            "[47300] train cost = 3.0143, test cost = 3.5133.\n",
            "[47400] train cost = 3.0143, test cost = 3.5133.\n",
            "[47500] train cost = 3.0143, test cost = 3.5133.\n",
            "[47600] train cost = 3.0143, test cost = 3.5133.\n",
            "[47700] train cost = 3.0143, test cost = 3.5133.\n",
            "[47800] train cost = 3.0141, test cost = 3.5139.\n",
            "[47900] train cost = 3.0141, test cost = 3.5138.\n",
            "[48000] train cost = 3.0141, test cost = 3.5139.\n",
            "[48100] train cost = 3.0141, test cost = 3.5139.\n",
            "[48200] train cost = 3.0139, test cost = 3.5139.\n",
            "[48300] train cost = 3.0139, test cost = 3.5138.\n",
            "[48400] train cost = 3.0139, test cost = 3.5138.\n",
            "[48500] train cost = 3.0139, test cost = 3.5138.\n",
            "[48600] train cost = 3.0138, test cost = 3.5135.\n",
            "[48700] train cost = 3.0138, test cost = 3.5136.\n",
            "[48800] train cost = 3.0138, test cost = 3.5136.\n",
            "[48900] train cost = 3.0138, test cost = 3.5137.\n",
            "[49000] train cost = 3.0138, test cost = 3.5136.\n",
            "[49100] train cost = 3.0136, test cost = 3.5139.\n",
            "[49200] train cost = 3.0134, test cost = 3.5139.\n",
            "[49300] train cost = 3.0134, test cost = 3.5139.\n",
            "[49400] train cost = 3.0134, test cost = 3.5141.\n",
            "[49500] train cost = 3.0134, test cost = 3.5140.\n",
            "[49600] train cost = 3.0134, test cost = 3.5140.\n",
            "[49700] train cost = 3.0134, test cost = 3.5139.\n",
            "[49800] train cost = 3.0134, test cost = 3.5140.\n",
            "[49900] train cost = 3.0134, test cost = 3.5140.\n",
            "[50000] train cost = 3.0134, test cost = 3.5140.\n",
            "[50100] train cost = 3.0134, test cost = 3.5140.\n",
            "[50200] train cost = 3.0133, test cost = 3.5140.\n",
            "[50300] train cost = 3.0133, test cost = 3.5139.\n",
            "[50400] train cost = 3.0133, test cost = 3.5139.\n",
            "[50500] train cost = 3.0131, test cost = 3.5140.\n",
            "[50600] train cost = 3.0131, test cost = 3.5140.\n",
            "[50700] train cost = 3.0131, test cost = 3.5140.\n",
            "[50800] train cost = 3.0131, test cost = 3.5141.\n",
            "[50900] train cost = 3.0131, test cost = 3.5141.\n",
            "[51000] train cost = 3.0128, test cost = 3.5142.\n",
            "[51100] train cost = 3.0124, test cost = 3.5134.\n",
            "[51200] train cost = 3.0121, test cost = 3.5135.\n",
            "[51300] train cost = 3.0121, test cost = 3.5135.\n",
            "[51400] train cost = 3.0121, test cost = 3.5135.\n",
            "[51500] train cost = 3.0121, test cost = 3.5134.\n",
            "[51600] train cost = 3.0118, test cost = 3.5142.\n",
            "[51700] train cost = 3.0116, test cost = 3.5139.\n",
            "[51800] train cost = 3.0116, test cost = 3.5140.\n",
            "[51900] train cost = 3.0115, test cost = 3.5142.\n",
            "[52000] train cost = 3.0113, test cost = 3.5142.\n",
            "[52100] train cost = 3.0112, test cost = 3.5142.\n",
            "[52200] train cost = 3.0108, test cost = 3.5147.\n",
            "[52300] train cost = 3.0107, test cost = 3.5142.\n",
            "[52400] train cost = 3.0107, test cost = 3.5141.\n",
            "[52500] train cost = 3.0107, test cost = 3.5141.\n",
            "[52600] train cost = 3.0107, test cost = 3.5141.\n",
            "[52700] train cost = 3.0107, test cost = 3.5141.\n",
            "[52800] train cost = 3.0107, test cost = 3.5141.\n",
            "[52900] train cost = 3.0107, test cost = 3.5141.\n",
            "[53000] train cost = 3.0107, test cost = 3.5140.\n",
            "[53100] train cost = 3.0107, test cost = 3.5140.\n",
            "[53200] train cost = 3.0107, test cost = 3.5140.\n",
            "[53300] train cost = 3.0107, test cost = 3.5140.\n",
            "[53400] train cost = 3.0107, test cost = 3.5139.\n",
            "[53500] train cost = 3.0107, test cost = 3.5139.\n",
            "[53600] train cost = 3.0107, test cost = 3.5139.\n",
            "[53700] train cost = 3.0107, test cost = 3.5139.\n",
            "[53800] train cost = 3.0107, test cost = 3.5139.\n",
            "[53900] train cost = 3.0107, test cost = 3.5139.\n",
            "[54000] train cost = 3.0105, test cost = 3.5135.\n",
            "[54100] train cost = 3.0105, test cost = 3.5136.\n",
            "[54200] train cost = 3.0105, test cost = 3.5136.\n",
            "[54300] train cost = 3.0105, test cost = 3.5137.\n",
            "[54400] train cost = 3.0105, test cost = 3.5137.\n",
            "[54500] train cost = 3.0103, test cost = 3.5138.\n",
            "[54600] train cost = 3.0103, test cost = 3.5138.\n",
            "[54700] train cost = 3.0103, test cost = 3.5138.\n",
            "[54800] train cost = 3.0102, test cost = 3.5143.\n",
            "[54900] train cost = 3.0102, test cost = 3.5141.\n",
            "[55000] train cost = 3.0100, test cost = 3.5144.\n",
            "[55100] train cost = 3.0100, test cost = 3.5144.\n",
            "[55200] train cost = 3.0098, test cost = 3.5143.\n",
            "[55300] train cost = 3.0097, test cost = 3.5144.\n",
            "[55400] train cost = 3.0097, test cost = 3.5142.\n",
            "[55500] train cost = 3.0097, test cost = 3.5142.\n",
            "[55600] train cost = 3.0097, test cost = 3.5142.\n",
            "[55700] train cost = 3.0094, test cost = 3.5143.\n",
            "[55800] train cost = 3.0094, test cost = 3.5142.\n",
            "[55900] train cost = 3.0094, test cost = 3.5142.\n",
            "[56000] train cost = 3.0094, test cost = 3.5140.\n",
            "[56100] train cost = 3.0092, test cost = 3.5138.\n",
            "[56200] train cost = 3.0092, test cost = 3.5139.\n",
            "[56300] train cost = 3.0090, test cost = 3.5142.\n",
            "[56400] train cost = 3.0090, test cost = 3.5141.\n",
            "[56500] train cost = 3.0089, test cost = 3.5143.\n",
            "[56600] train cost = 3.0089, test cost = 3.5140.\n",
            "[56700] train cost = 3.0089, test cost = 3.5140.\n",
            "[56800] train cost = 3.0089, test cost = 3.5140.\n",
            "[56900] train cost = 3.0089, test cost = 3.5140.\n",
            "[57000] train cost = 3.0089, test cost = 3.5140.\n",
            "[57100] train cost = 3.0089, test cost = 3.5140.\n",
            "[57200] train cost = 3.0089, test cost = 3.5140.\n",
            "[57300] train cost = 3.0085, test cost = 3.5144.\n",
            "[57400] train cost = 3.0085, test cost = 3.5140.\n",
            "[57500] train cost = 3.0085, test cost = 3.5140.\n",
            "[57600] train cost = 3.0085, test cost = 3.5140.\n",
            "[57700] train cost = 3.0085, test cost = 3.5140.\n",
            "[57800] train cost = 3.0085, test cost = 3.5140.\n",
            "[57900] train cost = 3.0085, test cost = 3.5140.\n",
            "[58000] train cost = 3.0085, test cost = 3.5139.\n",
            "[58100] train cost = 3.0085, test cost = 3.5140.\n",
            "[58200] train cost = 3.0085, test cost = 3.5140.\n",
            "[58300] train cost = 3.0085, test cost = 3.5139.\n",
            "[58400] train cost = 3.0085, test cost = 3.5140.\n",
            "[58500] train cost = 3.0083, test cost = 3.5141.\n",
            "[58600] train cost = 3.0083, test cost = 3.5141.\n",
            "[58700] train cost = 3.0082, test cost = 3.5140.\n",
            "[58800] train cost = 3.0082, test cost = 3.5138.\n",
            "[58900] train cost = 3.0082, test cost = 3.5139.\n",
            "[59000] train cost = 3.0078, test cost = 3.5145.\n",
            "[59100] train cost = 3.0075, test cost = 3.5145.\n",
            "[59200] train cost = 3.0074, test cost = 3.5145.\n",
            "[59300] train cost = 3.0073, test cost = 3.5145.\n",
            "[59400] train cost = 3.0069, test cost = 3.5144.\n",
            "[59500] train cost = 3.0068, test cost = 3.5143.\n",
            "[59600] train cost = 3.0068, test cost = 3.5142.\n",
            "[59700] train cost = 3.0062, test cost = 3.5146.\n",
            "[59800] train cost = 3.0057, test cost = 3.5146.\n",
            "[59900] train cost = 3.0057, test cost = 3.5146.\n",
            "[60000] train cost = 3.0057, test cost = 3.5146.\n",
            "[60100] train cost = 3.0057, test cost = 3.5146.\n",
            "[60200] train cost = 3.0057, test cost = 3.5146.\n",
            "[60300] train cost = 3.0053, test cost = 3.5141.\n",
            "[60400] train cost = 3.0053, test cost = 3.5142.\n",
            "[60500] train cost = 3.0052, test cost = 3.5145.\n",
            "[60600] train cost = 3.0052, test cost = 3.5144.\n",
            "[60700] train cost = 3.0052, test cost = 3.5143.\n",
            "[60800] train cost = 3.0050, test cost = 3.5139.\n",
            "[60900] train cost = 3.0050, test cost = 3.5140.\n",
            "[61000] train cost = 3.0050, test cost = 3.5140.\n",
            "[61100] train cost = 3.0049, test cost = 3.5138.\n",
            "[61200] train cost = 3.0042, test cost = 3.5141.\n",
            "[61300] train cost = 3.0042, test cost = 3.5140.\n",
            "[61400] train cost = 3.0042, test cost = 3.5139.\n",
            "[61500] train cost = 3.0042, test cost = 3.5138.\n",
            "[61600] train cost = 3.0042, test cost = 3.5138.\n",
            "[61700] train cost = 3.0040, test cost = 3.5136.\n",
            "[61800] train cost = 3.0040, test cost = 3.5136.\n",
            "[61900] train cost = 3.0040, test cost = 3.5136.\n",
            "[62000] train cost = 3.0040, test cost = 3.5137.\n",
            "[62100] train cost = 3.0040, test cost = 3.5137.\n",
            "[62200] train cost = 3.0040, test cost = 3.5137.\n",
            "[62300] train cost = 3.0040, test cost = 3.5137.\n",
            "[62400] train cost = 3.0040, test cost = 3.5137.\n",
            "[62500] train cost = 3.0040, test cost = 3.5137.\n",
            "[62600] train cost = 3.0040, test cost = 3.5138.\n",
            "[62700] train cost = 3.0037, test cost = 3.5146.\n",
            "[62800] train cost = 3.0037, test cost = 3.5145.\n",
            "[62900] train cost = 3.0037, test cost = 3.5145.\n",
            "[63000] train cost = 3.0037, test cost = 3.5144.\n",
            "[63100] train cost = 3.0037, test cost = 3.5143.\n",
            "[63200] train cost = 3.0035, test cost = 3.5143.\n",
            "[63300] train cost = 3.0035, test cost = 3.5141.\n",
            "[63400] train cost = 3.0035, test cost = 3.5141.\n",
            "[63500] train cost = 3.0035, test cost = 3.5141.\n",
            "[63600] train cost = 3.0035, test cost = 3.5141.\n",
            "[63700] train cost = 3.0034, test cost = 3.5137.\n",
            "[63800] train cost = 3.0034, test cost = 3.5138.\n",
            "[63900] train cost = 3.0034, test cost = 3.5139.\n",
            "[64000] train cost = 3.0032, test cost = 3.5141.\n",
            "[64100] train cost = 3.0032, test cost = 3.5141.\n",
            "[64200] train cost = 3.0032, test cost = 3.5141.\n",
            "[64300] train cost = 3.0032, test cost = 3.5140.\n",
            "[64400] train cost = 3.0032, test cost = 3.5141.\n",
            "[64500] train cost = 3.0030, test cost = 3.5145.\n",
            "[64600] train cost = 3.0030, test cost = 3.5143.\n",
            "[64700] train cost = 3.0030, test cost = 3.5143.\n",
            "[64800] train cost = 3.0029, test cost = 3.5144.\n",
            "[64900] train cost = 3.0029, test cost = 3.5145.\n",
            "[65000] train cost = 3.0029, test cost = 3.5145.\n",
            "[65100] train cost = 3.0029, test cost = 3.5144.\n",
            "[65200] train cost = 3.0029, test cost = 3.5144.\n",
            "[65300] train cost = 3.0029, test cost = 3.5143.\n",
            "[65400] train cost = 3.0027, test cost = 3.5140.\n",
            "[65500] train cost = 3.0027, test cost = 3.5138.\n",
            "[65600] train cost = 3.0027, test cost = 3.5138.\n",
            "[65700] train cost = 3.0027, test cost = 3.5138.\n",
            "[65800] train cost = 3.0027, test cost = 3.5137.\n",
            "[65900] train cost = 3.0027, test cost = 3.5138.\n",
            "[66000] train cost = 3.0025, test cost = 3.5131.\n",
            "[66100] train cost = 3.0025, test cost = 3.5131.\n",
            "[66200] train cost = 3.0025, test cost = 3.5132.\n",
            "[66300] train cost = 3.0025, test cost = 3.5132.\n",
            "[66400] train cost = 3.0025, test cost = 3.5133.\n",
            "[66500] train cost = 3.0025, test cost = 3.5133.\n",
            "[66600] train cost = 3.0025, test cost = 3.5135.\n",
            "[66700] train cost = 3.0025, test cost = 3.5134.\n",
            "[66800] train cost = 3.0025, test cost = 3.5135.\n",
            "[66900] train cost = 3.0025, test cost = 3.5135.\n",
            "[67000] train cost = 3.0024, test cost = 3.5134.\n",
            "[67100] train cost = 3.0024, test cost = 3.5137.\n",
            "[67200] train cost = 3.0024, test cost = 3.5137.\n",
            "[67300] train cost = 3.0024, test cost = 3.5136.\n",
            "[67400] train cost = 3.0024, test cost = 3.5136.\n",
            "[67500] train cost = 3.0024, test cost = 3.5137.\n",
            "[67600] train cost = 3.0024, test cost = 3.5136.\n",
            "[67700] train cost = 3.0024, test cost = 3.5137.\n",
            "[67800] train cost = 3.0024, test cost = 3.5136.\n",
            "[67900] train cost = 3.0024, test cost = 3.5137.\n",
            "[68000] train cost = 3.0024, test cost = 3.5137.\n",
            "[68100] train cost = 3.0024, test cost = 3.5137.\n",
            "[68200] train cost = 3.0024, test cost = 3.5136.\n",
            "[68300] train cost = 3.0024, test cost = 3.5137.\n",
            "[68400] train cost = 3.0022, test cost = 3.5135.\n",
            "[68500] train cost = 3.0022, test cost = 3.5134.\n",
            "[68600] train cost = 3.0022, test cost = 3.5133.\n",
            "[68700] train cost = 3.0022, test cost = 3.5133.\n",
            "[68800] train cost = 3.0022, test cost = 3.5133.\n",
            "[68900] train cost = 3.0022, test cost = 3.5134.\n",
            "[69000] train cost = 3.0022, test cost = 3.5134.\n",
            "[69100] train cost = 3.0022, test cost = 3.5135.\n",
            "[69200] train cost = 3.0022, test cost = 3.5134.\n",
            "[69300] train cost = 3.0022, test cost = 3.5134.\n",
            "[69400] train cost = 3.0021, test cost = 3.5132.\n",
            "[69500] train cost = 3.0020, test cost = 3.5131.\n",
            "[69600] train cost = 3.0020, test cost = 3.5133.\n",
            "[69700] train cost = 3.0020, test cost = 3.5133.\n",
            "[69800] train cost = 3.0020, test cost = 3.5134.\n",
            "[69900] train cost = 3.0020, test cost = 3.5134.\n",
            "[70000] train cost = 3.0020, test cost = 3.5134.\n",
            "[70100] train cost = 3.0020, test cost = 3.5134.\n",
            "[70200] train cost = 3.0020, test cost = 3.5134.\n",
            "[70300] train cost = 3.0020, test cost = 3.5135.\n",
            "[70400] train cost = 3.0020, test cost = 3.5135.\n",
            "[70500] train cost = 3.0020, test cost = 3.5135.\n",
            "[70600] train cost = 3.0020, test cost = 3.5136.\n",
            "[70700] train cost = 3.0020, test cost = 3.5135.\n",
            "[70800] train cost = 3.0020, test cost = 3.5136.\n",
            "[70900] train cost = 3.0020, test cost = 3.5135.\n",
            "[71000] train cost = 3.0020, test cost = 3.5136.\n",
            "[71100] train cost = 3.0020, test cost = 3.5135.\n",
            "[71200] train cost = 3.0017, test cost = 3.5141.\n",
            "[71300] train cost = 3.0017, test cost = 3.5139.\n",
            "[71400] train cost = 3.0017, test cost = 3.5139.\n",
            "[71500] train cost = 3.0017, test cost = 3.5139.\n",
            "[71600] train cost = 3.0017, test cost = 3.5139.\n",
            "[71700] train cost = 3.0017, test cost = 3.5138.\n",
            "[71800] train cost = 3.0015, test cost = 3.5139.\n",
            "[71900] train cost = 3.0014, test cost = 3.5135.\n",
            "[72000] train cost = 3.0014, test cost = 3.5134.\n",
            "[72100] train cost = 3.0014, test cost = 3.5134.\n",
            "[72200] train cost = 3.0014, test cost = 3.5134.\n",
            "[72300] train cost = 3.0014, test cost = 3.5134.\n",
            "[72400] train cost = 3.0014, test cost = 3.5135.\n",
            "[72500] train cost = 3.0014, test cost = 3.5135.\n",
            "[72600] train cost = 3.0014, test cost = 3.5134.\n",
            "[72700] train cost = 3.0014, test cost = 3.5135.\n",
            "[72800] train cost = 3.0014, test cost = 3.5135.\n",
            "[72900] train cost = 3.0014, test cost = 3.5135.\n",
            "[73000] train cost = 3.0014, test cost = 3.5135.\n",
            "[73100] train cost = 3.0014, test cost = 3.5135.\n",
            "[73200] train cost = 3.0012, test cost = 3.5138.\n",
            "[73300] train cost = 3.0012, test cost = 3.5136.\n",
            "[73400] train cost = 3.0012, test cost = 3.5136.\n",
            "[73500] train cost = 3.0012, test cost = 3.5136.\n",
            "[73600] train cost = 3.0012, test cost = 3.5136.\n",
            "[73700] train cost = 3.0012, test cost = 3.5135.\n",
            "[73800] train cost = 3.0012, test cost = 3.5134.\n",
            "[73900] train cost = 3.0012, test cost = 3.5135.\n",
            "[74000] train cost = 3.0012, test cost = 3.5134.\n",
            "[74100] train cost = 3.0012, test cost = 3.5134.\n",
            "[74200] train cost = 3.0012, test cost = 3.5134.\n",
            "[74300] train cost = 3.0012, test cost = 3.5134.\n",
            "[74400] train cost = 3.0012, test cost = 3.5134.\n",
            "[74500] train cost = 3.0012, test cost = 3.5134.\n",
            "[74600] train cost = 3.0012, test cost = 3.5134.\n",
            "[74700] train cost = 3.0010, test cost = 3.5131.\n",
            "[74800] train cost = 3.0010, test cost = 3.5131.\n",
            "[74900] train cost = 3.0010, test cost = 3.5130.\n",
            "[75000] train cost = 3.0010, test cost = 3.5130.\n",
            "[75100] train cost = 3.0010, test cost = 3.5130.\n",
            "[75200] train cost = 3.0010, test cost = 3.5131.\n",
            "[75300] train cost = 3.0010, test cost = 3.5131.\n",
            "[75400] train cost = 3.0010, test cost = 3.5131.\n",
            "[75500] train cost = 3.0009, test cost = 3.5135.\n",
            "[75600] train cost = 3.0009, test cost = 3.5135.\n",
            "[75700] train cost = 3.0009, test cost = 3.5135.\n",
            "[75800] train cost = 3.0009, test cost = 3.5135.\n",
            "[75900] train cost = 3.0009, test cost = 3.5135.\n",
            "[76000] train cost = 3.0009, test cost = 3.5134.\n",
            "[76100] train cost = 3.0009, test cost = 3.5135.\n",
            "[76200] train cost = 3.0007, test cost = 3.5136.\n",
            "[76300] train cost = 3.0007, test cost = 3.5136.\n",
            "[76400] train cost = 3.0007, test cost = 3.5136.\n",
            "[76500] train cost = 3.0007, test cost = 3.5136.\n",
            "[76600] train cost = 3.0007, test cost = 3.5136.\n",
            "[76700] train cost = 3.0007, test cost = 3.5136.\n",
            "[76800] train cost = 3.0007, test cost = 3.5136.\n",
            "[76900] train cost = 3.0007, test cost = 3.5136.\n",
            "[77000] train cost = 3.0007, test cost = 3.5136.\n",
            "[77100] train cost = 3.0004, test cost = 3.5133.\n",
            "[77200] train cost = 3.0004, test cost = 3.5133.\n",
            "[77300] train cost = 3.0004, test cost = 3.5134.\n",
            "[77400] train cost = 3.0004, test cost = 3.5134.\n",
            "[77500] train cost = 3.0002, test cost = 3.5141.\n",
            "[77600] train cost = 3.0001, test cost = 3.5139.\n",
            "[77700] train cost = 2.9999, test cost = 3.5136.\n",
            "[77800] train cost = 2.9999, test cost = 3.5136.\n",
            "[77900] train cost = 2.9997, test cost = 3.5135.\n",
            "[78000] train cost = 2.9997, test cost = 3.5136.\n",
            "[78100] train cost = 2.9997, test cost = 3.5136.\n",
            "[78200] train cost = 2.9997, test cost = 3.5137.\n",
            "[78300] train cost = 2.9997, test cost = 3.5137.\n",
            "[78400] train cost = 2.9995, test cost = 3.5136.\n",
            "[78500] train cost = 2.9995, test cost = 3.5136.\n",
            "[78600] train cost = 2.9995, test cost = 3.5136.\n",
            "[78700] train cost = 2.9995, test cost = 3.5135.\n",
            "[78800] train cost = 2.9995, test cost = 3.5135.\n",
            "[78900] train cost = 2.9995, test cost = 3.5135.\n",
            "[79000] train cost = 2.9992, test cost = 3.5136.\n",
            "[79100] train cost = 2.9992, test cost = 3.5136.\n",
            "[79200] train cost = 2.9990, test cost = 3.5138.\n",
            "[79300] train cost = 2.9990, test cost = 3.5137.\n",
            "[79400] train cost = 2.9990, test cost = 3.5136.\n",
            "[79500] train cost = 2.9990, test cost = 3.5136.\n",
            "[79600] train cost = 2.9990, test cost = 3.5136.\n",
            "[79700] train cost = 2.9990, test cost = 3.5136.\n",
            "[79800] train cost = 2.9990, test cost = 3.5136.\n",
            "[79900] train cost = 2.9990, test cost = 3.5135.\n",
            "[80000] train cost = 2.9990, test cost = 3.5135.\n",
            "[80100] train cost = 2.9990, test cost = 3.5136.\n",
            "[80200] train cost = 2.9990, test cost = 3.5135.\n",
            "[80300] train cost = 2.9990, test cost = 3.5135.\n",
            "[80400] train cost = 2.9990, test cost = 3.5135.\n",
            "[80500] train cost = 2.9990, test cost = 3.5135.\n",
            "[80600] train cost = 2.9990, test cost = 3.5136.\n",
            "[80700] train cost = 2.9990, test cost = 3.5135.\n",
            "[80800] train cost = 2.9989, test cost = 3.5136.\n",
            "[80900] train cost = 2.9989, test cost = 3.5136.\n",
            "[81000] train cost = 2.9989, test cost = 3.5136.\n",
            "[81100] train cost = 2.9989, test cost = 3.5136.\n",
            "[81200] train cost = 2.9989, test cost = 3.5137.\n",
            "[81300] train cost = 2.9989, test cost = 3.5136.\n",
            "[81400] train cost = 2.9989, test cost = 3.5136.\n",
            "[81500] train cost = 2.9989, test cost = 3.5136.\n",
            "[81600] train cost = 2.9989, test cost = 3.5136.\n",
            "[81700] train cost = 2.9989, test cost = 3.5137.\n",
            "[81800] train cost = 2.9989, test cost = 3.5136.\n",
            "[81900] train cost = 2.9989, test cost = 3.5136.\n",
            "[82000] train cost = 2.9989, test cost = 3.5136.\n",
            "[82100] train cost = 2.9989, test cost = 3.5136.\n",
            "[82200] train cost = 2.9989, test cost = 3.5136.\n",
            "[82300] train cost = 2.9989, test cost = 3.5136.\n",
            "[82400] train cost = 2.9989, test cost = 3.5136.\n",
            "[82500] train cost = 2.9987, test cost = 3.5138.\n",
            "[82600] train cost = 2.9987, test cost = 3.5138.\n",
            "[82700] train cost = 2.9987, test cost = 3.5138.\n",
            "[82800] train cost = 2.9987, test cost = 3.5137.\n",
            "[82900] train cost = 2.9987, test cost = 3.5137.\n",
            "[83000] train cost = 2.9987, test cost = 3.5137.\n",
            "[83100] train cost = 2.9987, test cost = 3.5137.\n",
            "[83200] train cost = 2.9987, test cost = 3.5137.\n",
            "[83300] train cost = 2.9987, test cost = 3.5137.\n",
            "[83400] train cost = 2.9987, test cost = 3.5136.\n",
            "[83500] train cost = 2.9987, test cost = 3.5137.\n",
            "[83600] train cost = 2.9987, test cost = 3.5137.\n",
            "[83700] train cost = 2.9987, test cost = 3.5137.\n",
            "[83800] train cost = 2.9987, test cost = 3.5137.\n",
            "[83900] train cost = 2.9987, test cost = 3.5136.\n",
            "[84000] train cost = 2.9987, test cost = 3.5137.\n",
            "[84100] train cost = 2.9987, test cost = 3.5136.\n",
            "[84200] train cost = 2.9984, test cost = 3.5141.\n",
            "[84300] train cost = 2.9984, test cost = 3.5140.\n",
            "[84400] train cost = 2.9984, test cost = 3.5139.\n",
            "[84500] train cost = 2.9984, test cost = 3.5139.\n",
            "[84600] train cost = 2.9984, test cost = 3.5139.\n",
            "[84700] train cost = 2.9984, test cost = 3.5139.\n",
            "[84800] train cost = 2.9984, test cost = 3.5138.\n",
            "[84900] train cost = 2.9984, test cost = 3.5138.\n",
            "[85000] train cost = 2.9984, test cost = 3.5138.\n",
            "[85100] train cost = 2.9984, test cost = 3.5138.\n",
            "[85200] train cost = 2.9984, test cost = 3.5137.\n",
            "[85300] train cost = 2.9984, test cost = 3.5137.\n",
            "[85400] train cost = 2.9984, test cost = 3.5137.\n",
            "[85500] train cost = 2.9982, test cost = 3.5137.\n",
            "[85600] train cost = 2.9982, test cost = 3.5137.\n",
            "[85700] train cost = 2.9982, test cost = 3.5136.\n",
            "[85800] train cost = 2.9982, test cost = 3.5136.\n",
            "[85900] train cost = 2.9982, test cost = 3.5136.\n",
            "[86000] train cost = 2.9982, test cost = 3.5136.\n",
            "[86100] train cost = 2.9982, test cost = 3.5135.\n",
            "[86200] train cost = 2.9982, test cost = 3.5135.\n",
            "[86300] train cost = 2.9982, test cost = 3.5135.\n",
            "[86400] train cost = 2.9982, test cost = 3.5135.\n",
            "[86500] train cost = 2.9982, test cost = 3.5135.\n",
            "[86600] train cost = 2.9982, test cost = 3.5135.\n",
            "[86700] train cost = 2.9982, test cost = 3.5135.\n",
            "[86800] train cost = 2.9982, test cost = 3.5135.\n",
            "[86900] train cost = 2.9982, test cost = 3.5135.\n",
            "[87000] train cost = 2.9982, test cost = 3.5135.\n",
            "[87100] train cost = 2.9982, test cost = 3.5135.\n",
            "[87200] train cost = 2.9982, test cost = 3.5134.\n",
            "[87300] train cost = 2.9982, test cost = 3.5134.\n",
            "[87400] train cost = 2.9982, test cost = 3.5134.\n",
            "[87500] train cost = 2.9982, test cost = 3.5134.\n",
            "[87600] train cost = 2.9982, test cost = 3.5134.\n",
            "[87700] train cost = 2.9982, test cost = 3.5134.\n",
            "[87800] train cost = 2.9982, test cost = 3.5134.\n",
            "[87900] train cost = 2.9982, test cost = 3.5135.\n",
            "[88000] train cost = 2.9981, test cost = 3.5135.\n",
            "[88100] train cost = 2.9981, test cost = 3.5134.\n",
            "[88200] train cost = 2.9981, test cost = 3.5135.\n",
            "[88300] train cost = 2.9981, test cost = 3.5135.\n",
            "[88400] train cost = 2.9979, test cost = 3.5134.\n",
            "[88500] train cost = 2.9979, test cost = 3.5135.\n",
            "[88600] train cost = 2.9977, test cost = 3.5133.\n",
            "[88700] train cost = 2.9977, test cost = 3.5133.\n",
            "[88800] train cost = 2.9976, test cost = 3.5136.\n",
            "[88900] train cost = 2.9974, test cost = 3.5139.\n",
            "[89000] train cost = 2.9972, test cost = 3.5137.\n",
            "[89100] train cost = 2.9971, test cost = 3.5143.\n",
            "[89200] train cost = 2.9971, test cost = 3.5140.\n",
            "[89300] train cost = 2.9971, test cost = 3.5139.\n",
            "[89400] train cost = 2.9969, test cost = 3.5146.\n",
            "[89500] train cost = 2.9969, test cost = 3.5146.\n",
            "[89600] train cost = 2.9967, test cost = 3.5144.\n",
            "[89700] train cost = 2.9967, test cost = 3.5144.\n",
            "[89800] train cost = 2.9967, test cost = 3.5144.\n",
            "[89900] train cost = 2.9967, test cost = 3.5144.\n",
            "[90000] train cost = 2.9967, test cost = 3.5144.\n",
            "[90100] train cost = 2.9967, test cost = 3.5143.\n",
            "[90200] train cost = 2.9967, test cost = 3.5143.\n",
            "[90300] train cost = 2.9967, test cost = 3.5143.\n",
            "[90400] train cost = 2.9967, test cost = 3.5143.\n",
            "[90500] train cost = 2.9967, test cost = 3.5142.\n",
            "[90600] train cost = 2.9967, test cost = 3.5142.\n",
            "[90700] train cost = 2.9966, test cost = 3.5141.\n",
            "[90800] train cost = 2.9966, test cost = 3.5143.\n",
            "[90900] train cost = 2.9966, test cost = 3.5144.\n",
            "[91000] train cost = 2.9966, test cost = 3.5144.\n",
            "[91100] train cost = 2.9966, test cost = 3.5144.\n",
            "[91200] train cost = 2.9966, test cost = 3.5144.\n",
            "[91300] train cost = 2.9966, test cost = 3.5144.\n",
            "[91400] train cost = 2.9966, test cost = 3.5144.\n",
            "[91500] train cost = 2.9966, test cost = 3.5144.\n",
            "[91600] train cost = 2.9964, test cost = 3.5148.\n",
            "[91700] train cost = 2.9964, test cost = 3.5148.\n",
            "[91800] train cost = 2.9964, test cost = 3.5147.\n",
            "[91900] train cost = 2.9964, test cost = 3.5148.\n",
            "[92000] train cost = 2.9964, test cost = 3.5147.\n",
            "[92100] train cost = 2.9964, test cost = 3.5147.\n",
            "[92200] train cost = 2.9964, test cost = 3.5148.\n",
            "[92300] train cost = 2.9964, test cost = 3.5148.\n",
            "[92400] train cost = 2.9964, test cost = 3.5148.\n",
            "[92500] train cost = 2.9964, test cost = 3.5148.\n",
            "[92600] train cost = 2.9964, test cost = 3.5148.\n",
            "[92700] train cost = 2.9962, test cost = 3.5147.\n",
            "[92800] train cost = 2.9962, test cost = 3.5147.\n",
            "[92900] train cost = 2.9962, test cost = 3.5147.\n",
            "[93000] train cost = 2.9962, test cost = 3.5147.\n",
            "[93100] train cost = 2.9962, test cost = 3.5147.\n",
            "[93200] train cost = 2.9962, test cost = 3.5146.\n",
            "[93300] train cost = 2.9962, test cost = 3.5146.\n",
            "[93400] train cost = 2.9962, test cost = 3.5146.\n",
            "[93500] train cost = 2.9962, test cost = 3.5146.\n",
            "[93600] train cost = 2.9962, test cost = 3.5146.\n",
            "[93700] train cost = 2.9962, test cost = 3.5146.\n",
            "[93800] train cost = 2.9962, test cost = 3.5146.\n",
            "[93900] train cost = 2.9962, test cost = 3.5146.\n",
            "[94000] train cost = 2.9962, test cost = 3.5146.\n",
            "[94100] train cost = 2.9962, test cost = 3.5147.\n",
            "[94200] train cost = 2.9962, test cost = 3.5146.\n",
            "[94300] train cost = 2.9962, test cost = 3.5146.\n",
            "[94400] train cost = 2.9962, test cost = 3.5146.\n",
            "[94500] train cost = 2.9962, test cost = 3.5146.\n",
            "[94600] train cost = 2.9962, test cost = 3.5146.\n",
            "[94700] train cost = 2.9962, test cost = 3.5146.\n",
            "[94800] train cost = 2.9962, test cost = 3.5146.\n",
            "[94900] train cost = 2.9962, test cost = 3.5146.\n",
            "[95000] train cost = 2.9961, test cost = 3.5147.\n",
            "[95100] train cost = 2.9961, test cost = 3.5153.\n",
            "[95200] train cost = 2.9961, test cost = 3.5152.\n",
            "[95300] train cost = 2.9961, test cost = 3.5152.\n",
            "[95400] train cost = 2.9961, test cost = 3.5152.\n",
            "[95500] train cost = 2.9961, test cost = 3.5152.\n",
            "[95600] train cost = 2.9961, test cost = 3.5152.\n",
            "[95700] train cost = 2.9961, test cost = 3.5152.\n",
            "[95800] train cost = 2.9961, test cost = 3.5151.\n",
            "[95900] train cost = 2.9961, test cost = 3.5151.\n",
            "[96000] train cost = 2.9961, test cost = 3.5152.\n",
            "[96100] train cost = 2.9961, test cost = 3.5151.\n",
            "[96200] train cost = 2.9961, test cost = 3.5151.\n",
            "[96300] train cost = 2.9961, test cost = 3.5151.\n",
            "[96400] train cost = 2.9961, test cost = 3.5151.\n",
            "[96500] train cost = 2.9961, test cost = 3.5150.\n",
            "[96600] train cost = 2.9957, test cost = 3.5147.\n",
            "[96700] train cost = 2.9957, test cost = 3.5147.\n",
            "[96800] train cost = 2.9957, test cost = 3.5146.\n",
            "[96900] train cost = 2.9957, test cost = 3.5146.\n",
            "[97000] train cost = 2.9957, test cost = 3.5146.\n",
            "[97100] train cost = 2.9957, test cost = 3.5146.\n",
            "[97200] train cost = 2.9957, test cost = 3.5146.\n",
            "[97300] train cost = 2.9957, test cost = 3.5146.\n",
            "[97400] train cost = 2.9957, test cost = 3.5146.\n",
            "[97500] train cost = 2.9957, test cost = 3.5145.\n",
            "[97600] train cost = 2.9957, test cost = 3.5146.\n",
            "[97700] train cost = 2.9957, test cost = 3.5146.\n",
            "[97800] train cost = 2.9957, test cost = 3.5147.\n",
            "[97900] train cost = 2.9957, test cost = 3.5147.\n",
            "[98000] train cost = 2.9957, test cost = 3.5147.\n",
            "[98100] train cost = 2.9957, test cost = 3.5147.\n",
            "[98200] train cost = 2.9957, test cost = 3.5147.\n",
            "[98300] train cost = 2.9957, test cost = 3.5147.\n",
            "[98400] train cost = 2.9957, test cost = 3.5147.\n",
            "[98500] train cost = 2.9957, test cost = 3.5147.\n",
            "[98600] train cost = 2.9957, test cost = 3.5148.\n",
            "[98700] train cost = 2.9957, test cost = 3.5147.\n",
            "[98800] train cost = 2.9957, test cost = 3.5147.\n",
            "[98900] train cost = 2.9957, test cost = 3.5148.\n",
            "[99000] train cost = 2.9957, test cost = 3.5148.\n",
            "[99100] train cost = 2.9957, test cost = 3.5148.\n",
            "[99200] train cost = 2.9957, test cost = 3.5148.\n",
            "[99300] train cost = 2.9957, test cost = 3.5148.\n",
            "[99400] train cost = 2.9957, test cost = 3.5148.\n",
            "[99500] train cost = 2.9957, test cost = 3.5148.\n",
            "[99600] train cost = 2.9957, test cost = 3.5148.\n",
            "[99700] train cost = 2.9957, test cost = 3.5148.\n",
            "[99800] train cost = 2.9957, test cost = 3.5148.\n",
            "[99900] train cost = 2.9957, test cost = 3.5148.\n",
            "[100000] train cost = 2.9957, test cost = 3.5148.\n",
            "[100100] train cost = 2.9957, test cost = 3.5148.\n",
            "[100200] train cost = 2.9957, test cost = 3.5149.\n",
            "[100300] train cost = 2.9957, test cost = 3.5148.\n",
            "[100400] train cost = 2.9957, test cost = 3.5149.\n",
            "[100500] train cost = 2.9957, test cost = 3.5149.\n",
            "[100600] train cost = 2.9957, test cost = 3.5149.\n",
            "[100700] train cost = 2.9957, test cost = 3.5150.\n",
            "[100800] train cost = 2.9957, test cost = 3.5149.\n",
            "[100900] train cost = 2.9957, test cost = 3.5149.\n",
            "[101000] train cost = 2.9957, test cost = 3.5149.\n",
            "[101100] train cost = 2.9957, test cost = 3.5149.\n",
            "[101200] train cost = 2.9957, test cost = 3.5149.\n",
            "[101300] train cost = 2.9957, test cost = 3.5149.\n",
            "[101400] train cost = 2.9957, test cost = 3.5149.\n",
            "[101500] train cost = 2.9957, test cost = 3.5149.\n",
            "[101600] train cost = 2.9956, test cost = 3.5149.\n",
            "[101700] train cost = 2.9956, test cost = 3.5150.\n",
            "[101800] train cost = 2.9956, test cost = 3.5150.\n",
            "[101900] train cost = 2.9956, test cost = 3.5150.\n",
            "[102000] train cost = 2.9956, test cost = 3.5149.\n",
            "[102100] train cost = 2.9956, test cost = 3.5149.\n",
            "[102200] train cost = 2.9956, test cost = 3.5150.\n",
            "[102300] train cost = 2.9956, test cost = 3.5150.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-208-60ee62840fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPRINT_DELAY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9f410b42-9651-48ca-d04b-5c3e22cb4bb7",
        "id": "yrtF1yZtFaKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "ts = range(len(train_losses))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(ts,train_losses)\n",
        "ax.plot(ts,test_losses)\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.plot(ts,np.divide(train_losses,test_losses))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAFlCAYAAADh444SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHtJEsPnFaKZ",
        "colab": {}
      },
      "source": [
        "examples, example_loss = sess.run([outputs, cost], train_feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0b8af926-9987-40e3-d4e1-e69281fd30de",
        "id": "fuwg_z2cFaKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "i = np.random.randint(examples.shape[0])\n",
        "print(np.argmax(examples[i],1))\n",
        "print(np.argmax(y_train[i],1))\n",
        "# ones = np.full((examples.shape[0],1),1)\n",
        "# vocab = np.argmax(np.around(examples),2)[:,:-1]\n",
        "# final = np.concatenate((ones,vocab),1)\n",
        "# decoded = decode_circuit(final[i],N)\n",
        "# label = generate_labels([decoded])\n",
        "# print(final[i])\n",
        "# print(np.concatenate(([1],np.argmax(y_train[i][:-1],1))))\n",
        "# print(label)\n",
        "# print(c_train[i])\n",
        "# decoded.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3]\n",
            "[8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rCXMhRfUJuqX",
        "outputId": "26d7555a-0ad5-4c01-fe7d-40397966c9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example = sess.run(outputs, {inputs: [X_train[14]], cond: [c_train[14]]})\n",
        "print(np.argmax(example))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q-jMC8Y_SjD",
        "colab_type": "code",
        "outputId": "19081bc4-f834-4520-a7dd-d5f2b84e7569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "saver.recover_last_checkpoints(\"Checkpoints\")\n",
        "circ = []\n",
        "condition = [c_train[0]]\n",
        "length = 12\n",
        "ix = [1]\n",
        "X = np.zeros((length,1, VOCAB_DIM))   \n",
        "for i in range(length):\n",
        "    X[i, 0 , ix[-1]] = 1\n",
        "    circ.append(np.argmax(X[0, :]))\n",
        "    if np.argmax(X[0, :]) ==2: break\n",
        "    ix = np.argmax(sess.run([outputs], {inputs:  X, cond: condition})[0],1)\n",
        "print(circ)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}