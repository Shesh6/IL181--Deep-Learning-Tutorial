{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Tutorial Assignment 3 - Deep LSTM Lovecraft.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shesh6/IL181--Deep-Learning-Tutorial/blob/master/Deep_Learning_Tutorial_Assignment_3_Deep_LSTM_Lovecraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYq7yv9PZ8Xx",
        "colab_type": "text"
      },
      "source": [
        "#### Deep Learning Tutorial Assignment 3 - Sequential Prediction\n",
        "\n",
        "### Deep One: The H.P. Lovecraft Text Generator\n",
        "_Yoav Rabinovich, December 2019_\n",
        "\n",
        "A deep recurrent long-short-term memory network that renders the unspeakable readable.\n",
        "\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dyh7pkebHSi",
        "colab_type": "text"
      },
      "source": [
        "#### Introduction\n",
        "\n",
        "H.P Lovecraft is a classic author of horror fiction. Growing up racist and lonely in a cold and dreary New England farm probably played a part in shaping his grotesque, mysterious prose all about monstrosities in the dark depths and alien horrors of cosmic scale.\n",
        "\n",
        "Deep One has been raised under similar conditions, on a lonely GPU in a cold and dreary Google server farm. While it makes slightly less grammatical sense than Lovecraft, I believe it manages to capture a bit of his unique style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCoq3gNlEQsV",
        "colab_type": "text"
      },
      "source": [
        "#### Model:\n",
        "\n",
        "Deep One is a character-based Deep RNN using stacked LSTM layers to learn the distribution of characters in Lovecraft's writings given a preceding string. The input sequence is plugged into the first LSTM layer, which then outputs a string predicting each next character in the sequence, which are then fed to the next LSTM layer. When the recurrent layers are done, the final output sequence is put through a dense layer to generate a prediction for the next character in the sequence. The loss is then calculated based on prediction accuracy, and the parameters of the network are optimized through back-probagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg-QSG85EZKV",
        "colab_type": "text"
      },
      "source": [
        "#### Data\n",
        "\n",
        "I used a dataset of the entirety of Lovecraft's published works, and generated input-output pairs for training by copying the input string and shifting it one character forward. This way each input character is paired with its consequtive character as a target output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xHUFH-xEf_2",
        "colab_type": "text"
      },
      "source": [
        "#### Training:\n",
        "\n",
        "Training the network on a GPU, I used a batch size as large as fit the RAM, based on my chosen sequence length, hidden layer size and number of LSTM layers. This way I reduced the variance of the stochastic gradient updates to allow the network to converge faster. I trained the model for 400 epochs (around 4 hours) on sequences of 100 characters to produce the results explored below (with 5 256-unit LSTM layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLY-wEEBEj3t",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis:\n",
        "\n",
        "I implemented two ways to sample the model: Either with a random initial character, or with a seed string. I use the former to print a random sentence between each training epoch to track the progress, and the latter to analyze the resulting model.\n",
        "\n",
        "During training, a clear pattern emerges when we examine the order in which the model picks up on the correlations in the sequences. Each model trained started out predicting nothing but spaces, since they're a common character that shows little preference regarding which character precedes it. The model then spends a long time producing repeated patterns, corresponding to common words and expressions in the text: First comes the \"the the the...\" pattern, then \"of the of the of the...\", and in the case of this Lovecraft dataset, the networks picks up on the word \"strange\" and keeps repeating it. It seems particularly attracted to words that start with \"st\" or \"co\", probably due to their frequency in the text, producing outputs such as \"strange stranger of the could strange stranger...\". The first non-repeating pattern that's picked up is exhibited when the sampled text begins with a number or a line break: the model learns the common pattern representing a page transition in the raw dataset: two line breaks followed by the page number and another pair of line breaks. It took approximately 400 epochs for my model to rid itself of repeating patterns, and start producing real sentences. However, these still mostly exhibit awareness to two or three words at a time, producing run-on sentences without a clear subject-verb-object structure.\n",
        "\n",
        "However, examining some seeded examples we find that some complex contextual awareness was successfully captured by the model. For example, when seeded with the word \"Written\", the model completes the sentence:\n",
        "\n",
        "\"Written in March of 1926 in Weird Tales \n",
        "\n",
        "Where bulletin of Boston's speech was the strangeness of the strange stones..\"\n",
        "\n",
        "Showing awareness of the context of meta-data presented in between stories, even though no story was written in that particular month. It seems to lost track a bit and confuse the date of writing with the date of publication, since the year is follwed by the title of a popular magazine. It then also knows to start the next sentence with a capital letter, without the need of a period which doesn't appear in the meta-data sections in the dataset.\n",
        "\n",
        "The model also completes \"Prof\" to \"Professor Well\", a capitalized name that isn't found in the dataset, and completes seeds that begin with quotation marks in a way that resembles dialogue, and begins new paragraphs with quotation marks as well, which is perhaps the longest-range correlation I've managed to identify in the model:\n",
        "\n",
        "\" \"Yes to see what I had seen - the season when he saw them \n",
        "again. \n",
        "\n",
        "\"Inn of the party were stumbled to the papers and saw the proper sent \n",
        "of the strange paths and the strange stone brown streets and.. \"\n",
        "\n",
        "To encourage the learning of longer-range correlations I've tried to train the model with larger length sequences, as 100 characters only represent around 20 words. However, those took too long to train. Since the loss was still decreasing when I ceased the training, and since the model exhibited a clear pattern of encoding longer-range correlations as training progressed, I believe that longer training would be able to produce believable text eventually. Another way to encourage these patterns is the use of attention, which is a good next step when I come back to this project in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4pLUv0oJtmL",
        "colab_type": "text"
      },
      "source": [
        "### Code\n",
        " _Adapted from https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qpxL_-cK6fs",
        "colab_type": "text"
      },
      "source": [
        "_Note: Deprecation messages in the code can't be supressed since they're produced inside the Keras functions_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_jV_y7LJwXu",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayYGyBq1zqzi",
        "colab_type": "code",
        "outputId": "ee1dace7-8726-407f-d917-1b5b12617987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras as k"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwo-jm_bKuvf",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhafgAXl0JF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read corpus and isolate characters\n",
        "data = open(\"The-Collected-Works-of-HP-Lovecraft_djvu.txt\", 'r').read()\n",
        "chars = list(set(data))\n",
        "VOCAB_SIZE = len(chars)\n",
        "CORPUS_SIZE = len(data)\n",
        "\n",
        "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
        "char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
        "\n",
        "# Size of sequences to train on\n",
        "SEQ_LENGTH = 100\n",
        "SLICE = int(CORPUS_SIZE//SEQ_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6lhSGuC0xIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create input and output sequence pairs\n",
        "X = np.zeros((SLICE, SEQ_LENGTH, VOCAB_SIZE))\n",
        "y = np.zeros((SLICE, SEQ_LENGTH, VOCAB_SIZE))\n",
        "\n",
        "for i in range(0, SLICE):\n",
        "    # One-hot Encoding\n",
        "    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
        "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
        "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
        "    for j in range(SEQ_LENGTH):\n",
        "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
        "    X[i] = input_sequence\n",
        "\n",
        "    # Shift characters one to the left for output\n",
        "    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
        "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
        "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
        "    for j in range(SEQ_LENGTH):\n",
        "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
        "    y[i] = target_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCrJeDXPKyJ-",
        "colab_type": "text"
      },
      "source": [
        "#### Text Generation (Decoder) Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6zUw9HI2WM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate text from random character\n",
        "def generate_text(model, length):\n",
        "    ix = [np.random.randint(VOCAB_SIZE)]\n",
        "    y_char = [ix_to_char[ix[-1]]]\n",
        "    X = np.zeros((1, length, VOCAB_SIZE))   \n",
        "    for i in range(length):\n",
        "        X[0, i, :][ix[-1]] = 1\n",
        "        print(ix_to_char[ix[-1]], end=\"\")\n",
        "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "        y_char.append(ix_to_char[ix[-1]])\n",
        "    return ('').join(y_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-MmULwRPG07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate text from seed string\n",
        "def generate_text_from_seed(model, length, seed=None):\n",
        "\n",
        "    # Encode seed\n",
        "    seed_length = len(seed)\n",
        "    X = np.zeros((1, length+seed_length, VOCAB_SIZE))\n",
        "    y_char = []\n",
        "    for i in range(seed_length):\n",
        "        ix = [char_to_ix[seed[i]]]\n",
        "        X[0,i,:][ix[-1]] = 1\n",
        "        new_char = ix_to_char[ix[-1]]\n",
        "        print(new_char, end=\"\")\n",
        "        y_char.append(new_char)\n",
        "\n",
        "    # Generate text\n",
        "    for i in range(seed_length,length):\n",
        "        ix = np.argmax(model.predict(X[:, :i, :])[0], 1)\n",
        "        new_char = ix_to_char[ix[-1]]\n",
        "        print(new_char, end=\"\")\n",
        "        X[0, i, :][ix[-1]] = 1\n",
        "        y_char.append(new_char)\n",
        "    return ('').join(y_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCuNXBljK192",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ATFH07u1MOi",
        "colab_type": "code",
        "outputId": "edd5aaa7-90ed-4d99-ba65-6ca41317c2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# Model Parameters\n",
        "HIDDEN_DIM = 256\n",
        "LAYER_NUM = 5\n",
        "\n",
        "# Build Model\n",
        "model = k.Sequential()\n",
        "model.add(k.layers.LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), \\\n",
        "                        return_sequences=True))\n",
        "for i in range(LAYER_NUM - 1):\n",
        "    model.add(k.layers.LSTM(HIDDEN_DIM, return_sequences=True))\n",
        "model.add(k.layers.TimeDistributed(k.layers.Dense(VOCAB_SIZE)))\n",
        "model.add(k.layers.Activation('softmax'))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, None, 256)         355328    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 256)         525312    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 256)         525312    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 256)         525312    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, None, 256)         525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, None, 90)          23130     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, None, 90)          0         \n",
            "=================================================================\n",
            "Total params: 2,479,706\n",
            "Trainable params: 2,479,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKV74nPdLOOU",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSewNxxU3DJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters\n",
        "BATCH_SIZE = 256\n",
        "GENERATE_LENGTH = 100\n",
        "\n",
        "# Load model (Optional)\n",
        "# model = k.models.load_model(\"model_checkpoint_256_epoch_400_batchsize_1024_seqlength_100.hdf5\")\n",
        "\n",
        "# Custom training cycle, to allow for printing samples between epochs\n",
        "nb_epoch = 401\n",
        "while True:\n",
        "    print(\"\\n---Epoch {}---\".format(nb_epoch))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
        "    nb_epoch += 1\n",
        "    generate_text(model, GENERATE_LENGTH)\n",
        "    # Save snapshot of weights every 10 epochs\n",
        "    if nb_epoch % 10 == 0:\n",
        "        model.save_weights('checkpoint_{}_epoch_{}_batchsize_{}_seqlength_{}.hdf5'\\\n",
        "                           .format(HIDDEN_DIM, nb_epoch,BATCH_SIZE,SEQ_LENGTH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keJ_HokwK-_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model (Optional)\n",
        "#model.save('model_checkpoint_{}_epoch_{}_batchsize_{}_seqlength_{}.hdf5'.format(HIDDEN_DIM, nb_epoch,BATCH_SIZE,SEQ_LENGTH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnq7aZO4ELK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "528678c7-0623-440f-ffac-07c18153acf7"
      },
      "source": [
        "generate_text(model, 200)\n",
        "print(\"\\n - end of sequence - \")\n",
        "print(\"----------------\")\n",
        "generate_text_from_seed(model, 200, \"Deep in the sunken city of\")\n",
        "print(\"\\n - end of sequence - \")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n the stars and the \n",
            "strange passage which had been too supernece in the course of the \n",
            "party which made me shudder, and the strange stone broad stairces which had \n",
            "seemed to be the stones of the stra\n",
            " - end of sequence - \n",
            "Deep in the sunken city of the \n",
            "contourable and incredible castle and brownings and column to the street \n",
            "of the strange papers and the strange stones of the strange paths and \n",
            "distant seamows and con\n",
            " end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ0NR3sgWnGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c62eec64-85ce-4851-802b-c7e87a06ddbe"
      },
      "source": [
        "generate_text_from_seed(model, 200, \"Written\")\n",
        "print(\"\\n - end of sequence - \")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Written in March of 1926 in Weird Tales \n",
            "\n",
            "Where bulletin of Boston's speech was the strangeness of the strange stones and \n",
            "changes of the present balustradia and deaded watchers and the province to th\n",
            " - end of sequence - \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSxiigUwXDfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "86691d04-8b7e-4087-d2e9-e7a0df6c0020"
      },
      "source": [
        "generate_text_from_seed(model, 100, \"Prof\")\n",
        "print(\"\\n - end of sequence - \")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Professor Well, and had been able to \n",
            "seek his get at the sea in the sea and sense of the present fl\n",
            " - end of sequence - \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIR5GMOmXIpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "6f8b8b8b-06f7-4be5-8c02-fb1a6b99a882"
      },
      "source": [
        "generate_text_from_seed(model, 200, \"\\\"Yes\")\n",
        "print(\"\\n - end of sequence - \")"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Yes to see what I had seen - the season when he saw them \n",
            "again. \n",
            "\n",
            "\"Inn of the party were stumbled to the papers and saw the proper sent \n",
            "of the strange paths and the strange stone brown streets and \n",
            " - end of sequence - \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}