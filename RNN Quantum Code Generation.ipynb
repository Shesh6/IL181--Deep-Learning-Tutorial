{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantum CodeGen.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shesh6/IL181--Deep-Learning-Tutorial/blob/master/RNN%20Quantum%20Code%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N1FZLzbfzVU",
        "colab_type": "text"
      },
      "source": [
        "#### CP194 Final Assignment\n",
        "\n",
        "### Capstone Complete Work and Plans\n",
        "_Yoav Rabinovich, December 2019_\n",
        "\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNdghp0rfvQd",
        "colab_type": "text"
      },
      "source": [
        "#### Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZaX7qNKHD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install qiskit\n",
        "#!pip install tensorflow --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ycOxIXKsKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import qiskit as qk\n",
        "import tensorflow as tf\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG_UWsozhs3i",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing and Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPgC4Wa8Ky4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_state(n):\n",
        "    \"\"\"Sample a random, normalized n-qubit quantum state\"\"\"\n",
        "    return\n",
        "\n",
        "def sample_circuits(n,size,amount):\n",
        "    \"\"\"Sample an amount of random n-qubit circuits with a certain size in\n",
        "    number of operations from the allowed set\"\"\"\n",
        "\n",
        "    circuits =[]\n",
        "    for _ in range(amount):\n",
        "        # Create circuit object of n qubits\n",
        "        circ = qk.QuantumCircuit(n)\n",
        "        # Generate random gates on random qubits from the universal set {H,S,CX}\n",
        "        for _ in range(size):\n",
        "            gate = np.random.randint(0,3)\n",
        "            target = np.random.randint(0,n)\n",
        "            if gate==0: # Hadamard\n",
        "                circ.h(target)\n",
        "            if gate==1: # S-gate\n",
        "                circ.s(target)\n",
        "            if gate==2: # CNOT\n",
        "                control = np.random.randint(0,n)\n",
        "                if control == target:\n",
        "                    circ.h(target)\n",
        "                else:\n",
        "                    circ.cx(control,target)\n",
        "        circuits.append(circ)\n",
        "    return circuits\n",
        "\n",
        "def generate_labels(circuits,encoded=True):\n",
        "    \"\"\"Simulate each in an array of circuits, and return the resultant state.\n",
        "    The state can be encoded as an array of size 2n where the real and imaginary\n",
        "    components of each amplitude are concatenated.\"\"\"\n",
        "\n",
        "    backend = qk.Aer.get_backend('statevector_simulator')\n",
        "    labels=[]\n",
        "    for circ in circuits:\n",
        "        # Simulate each circuit and retrieve final quantum state\n",
        "        job = qk.execute(circ, backend)\n",
        "        outputstate = job.result().get_statevector(circ, decimals=3)\n",
        "        if encoded:\n",
        "            # encode complex amplitudes as flattened arrays\n",
        "            separated = []\n",
        "            separated.append(outputstate.real)\n",
        "            separated.append(outputstate.imag)\n",
        "            outputstate = separated\n",
        "        labels.append(np.array(outputstate).flatten())\n",
        "    return np.array(labels)\n",
        "\n",
        "def encode_circuits(circuits,n,max_size,label=True):\n",
        "    \"\"\"Takes an array of n-qubit QuantumCircuit objects, and encodes them based on a\n",
        "    vocabulary of possible gates to apply, including tokens to signify the start\n",
        "    and end of sequences. Elements after EoS are padded to match maximum circuit\n",
        "    size using a special token.\n",
        "    Labels can be also be generated for the circuits.\"\n",
        "\n",
        "    Vocabulary scheme:\n",
        "    Padding = 0,\n",
        "    SoS = 1,\n",
        "    EoS = 2,\n",
        "    h[0]=3, h[1]=3+1...\n",
        "    s[0]=3+n, s[1]=3+n+1...\n",
        "    cx[0,0]=3+2n, cx[0,1]=3+2n+1...\n",
        "    cx[1,0]=3+(2+1)n, cx[1,1]=3+(2+1)n+1... etc. \"\"\"\n",
        "\n",
        "    encoded = []\n",
        "    for circ in circuits:\n",
        "        # Use the QASM format to convert the circuit to a string\n",
        "        lines = circ.qasm().splitlines()[3:]\n",
        "        size = len(lines)\n",
        "        # Initialize to padding tokens\n",
        "        encoded_circ = np.zeros(max_size+2)\n",
        "        # Add SoS and EoS tokens\n",
        "        encoded_circ[0] = 1\n",
        "        encoded_circ[size+1]=2\n",
        "        for i,line in enumerate(lines):\n",
        "            # Detect gate name and qubits involved\n",
        "            gate_str = line[:2]\n",
        "            integers = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*',line)]\n",
        "            # Encode gates based on scheme above\n",
        "            if gate_str==\"h \":\n",
        "                encoded_circ[i+1]=int(3+integers[0])\n",
        "            if gate_str==\"s \":\n",
        "                encoded_circ[i+1]=int(3+n+integers[0])\n",
        "            if gate_str==\"cx\":\n",
        "                encoded_circ[i+1]=int(3+(2+integers[0])*n+integers[1])\n",
        "        encoded.append(encoded_circ)\n",
        "    encoded = np.array(encoded)\n",
        "    if label:\n",
        "        # Simulate labels for each circuit and attach to dataset\n",
        "        labels = generate_labels(circuits)\n",
        "        return np.concatenate((encoded,labels),axis=1)\n",
        "    else:\n",
        "        return np.array(encoded)\n",
        "\n",
        "def decode_circuit(encoded,n):\n",
        "    \"\"\"Takes an encoded output from the network and generates the corresponding\n",
        "    circuit as described above.\"\"\"\n",
        "\n",
        "    # Start with opening syntax\n",
        "    decoded = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[\"+str(n)+\"];\\n\"\n",
        "    for line in encoded:\n",
        "        # decode each non-token element into its QASM string\n",
        "        line = int(line)\n",
        "        if line > 2:\n",
        "            gate_num = int(np.ceil((line-2)/n))\n",
        "            if gate_num==1:\n",
        "                decoded += \"h q[\"+str(line-3)\n",
        "            elif gate_num==2:\n",
        "                decoded += \"s q[\"+str(line-n-3)\n",
        "            else:\n",
        "                decoded += \"cx q[\"+str(gate_num-3)+\"],q[\"+str(line-(gate_num-1)*n-3)\n",
        "            decoded += \"];\\n\"\n",
        "        if line == 2:\n",
        "            decoded = decoded[:-1]\n",
        "    # Build circuit object from QASM string\n",
        "    return qk.QuantumCircuit.from_qasm_str(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzIg4aOrfqhI",
        "colab_type": "text"
      },
      "source": [
        "#### Demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQEY6ukkH9rm",
        "colab_type": "text"
      },
      "source": [
        "We demonstrate our ability to sample algorithms of arbitrary dimensions, encode them correctly using our vocabulary scheme, and decode them exactly to their original circuit objects (up to allowable swaps in non-interacting gates)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9zaYBzcfqCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "max_size = 30\n",
        "\n",
        "# Sample, Encode and Label circuits\n",
        "sampled_circuits = sample_circuits(n, max_size, 20)\n",
        "encoded_circuits = encode_circuits(sampled_circuits, n, max_size, label=False)\n",
        "labels = generate_labels(sampled_circuits, encoded=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oAPxQPcIJ9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "6b9d7e76-262a-423b-8232-3ff6a04afef2"
      },
      "source": [
        "# Graph sample circuit\n",
        "samp = sampled_circuits[0]\n",
        "samp.draw()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐┌───┐┌───┐                              ┌───┐┌───┐          \n",
              "q_0: |0>┤ X ├┤ H ├┤ H ├──────────────────────■───────┤ X ├┤ S ├──────────\n",
              "        └─┬─┘└───┘├───┤     ┌───┐            │  ┌───┐└─┬─┘└───┘          \n",
              "q_1: |0>──┼────■──┤ H ├─────┤ X ├───────■────┼──┤ S ├──┼─────────────────\n",
              "          │    │  ├───┤     └─┬─┘       │    │  └───┘  │            ┌───┐\n",
              "q_2: |0>──■────┼──┤ H ├───────┼─────────┼────┼────■────┼─────────■──┤ S ├\n",
              "        ┌───┐  │  ├───┤┌───┐  │  ┌───┐  │    │  ┌─┴─┐  │  ┌───┐  │  └───┘\n",
              "q_3: |0>┤ H ├──┼──┤ X ├┤ H ├──┼──┤ H ├──┼────┼──┤ X ├──┼──┤ H ├──┼───────\n",
              "        ├───┤  │  └─┬─┘├───┤  │  ├───┤  │    │  ├───┤  │  └───┘┌─┴─┐┌───┐\n",
              "q_4: |0>┤ S ├──┼────┼──┤ S ├──┼──┤ S ├──┼────┼──┤ S ├──┼───────┤ X ├┤ S ├\n",
              "        └───┘  │    │  ├───┤  │  └───┘  │    │  └───┘  │       └───┘└───┘\n",
              "q_5: |0>───────┼────■──┤ H ├──┼─────────┼────┼─────────┼─────────────────\n",
              "        ┌───┐  │  ┌───┐├───┤  │         │    │         │                 \n",
              "q_6: |0>┤ S ├──┼──┤ S ├┤ S ├──┼─────────┼────┼─────────┼─────────────────\n",
              "        └───┘  │  └───┘└───┘  │       ┌─┴─┐  │  ┌───┐  │                 \n",
              "q_7: |0>───────┼──────────────■───────┤ X ├──┼──┤ H ├──┼─────────────────\n",
              "               │                      └───┘┌─┴─┐└───┘  │                 \n",
              "q_8: |0>───────┼───────────────────────────┤ X ├───────┼─────────────────\n",
              "             ┌─┴─┐                         └───┘       │                 \n",
              "q_9: |0>─────┤ X ├─────────────────────────────────────■─────────────────\n",
              "             └───┘                                                       </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7fc4e0dfe978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGtS9TQuIWDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "6bde5fc6-6c03-477c-8e7b-daf9ff7beb8f"
      },
      "source": [
        "# Present QASM string and encoded representation\n",
        "print(samp.qasm())\n",
        "enc = encoded_circuits[0]\n",
        "print(enc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPENQASM 2.0;\n",
            "include \"qelib1.inc\";\n",
            "qreg q[10];\n",
            "cx q[2],q[0];\n",
            "cx q[1],q[9];\n",
            "h q[2];\n",
            "h q[0];\n",
            "h q[3];\n",
            "s q[4];\n",
            "s q[6];\n",
            "cx q[5],q[3];\n",
            "h q[1];\n",
            "h q[3];\n",
            "cx q[7],q[1];\n",
            "h q[3];\n",
            "cx q[1],q[7];\n",
            "s q[6];\n",
            "cx q[2],q[3];\n",
            "h q[5];\n",
            "h q[0];\n",
            "h q[7];\n",
            "cx q[0],q[8];\n",
            "h q[3];\n",
            "s q[6];\n",
            "s q[4];\n",
            "s q[4];\n",
            "s q[4];\n",
            "cx q[9],q[0];\n",
            "cx q[2],q[4];\n",
            "s q[2];\n",
            "s q[0];\n",
            "s q[1];\n",
            "s q[4];\n",
            "\n",
            "[  1.  43.  42.   5.   3.   6.  17.  19.  76.   4.   6.  94.   6.  40.\n",
            "  19.  46.   8.   3.  10.  31.   6.  19.  17.  17.  17. 113.  47.  15.\n",
            "  13.  14.  17.   2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnUY7qzhIuNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "6d864a01-1900-4053-971b-dff04c11c3b3"
      },
      "source": [
        "# Decode the encoded representation and graph it to demonstrate similarity\n",
        "dec = decode_circuit(enc,n)\n",
        "dec.draw()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐┌───┐┌───┐                                   ┌───┐┌───┐     \n",
              "q_0: |0>┤ X ├┤ H ├┤ H ├──────────────────────■────────────┤ X ├┤ S ├─────\n",
              "        └─┬─┘└───┘├───┤          ┌───┐       │       ┌───┐└─┬─┘└───┘     \n",
              "q_1: |0>──┼────■──┤ H ├──────────┤ X ├───────┼────■──┤ S ├──┼────────────\n",
              "          │    │  ├───┤          └─┬─┘       │    │  └───┘  │       ┌───┐\n",
              "q_2: |0>──■────┼──┤ H ├────────────┼─────────┼────┼────■────┼────■──┤ S ├\n",
              "        ┌───┐  │  └───┘┌───┐┌───┐  │  ┌───┐  │    │  ┌─┴─┐  │    │  ├───┤\n",
              "q_3: |0>┤ H ├──┼───────┤ X ├┤ H ├──┼──┤ H ├──┼────┼──┤ X ├──┼────┼──┤ H ├\n",
              "        ├───┤  │  ┌───┐└─┬─┘├───┤  │  ├───┤  │    │  └───┘  │  ┌─┴─┐├───┤\n",
              "q_4: |0>┤ S ├──┼──┤ S ├──┼──┤ S ├──┼──┤ S ├──┼────┼─────────┼──┤ X ├┤ S ├\n",
              "        └───┘  │  └───┘  │  ├───┤  │  └───┘  │    │         │  └───┘└───┘\n",
              "q_5: |0>───────┼─────────■──┤ H ├──┼─────────┼────┼─────────┼────────────\n",
              "        ┌───┐  │  ┌───┐┌───┐└───┘  │         │    │         │            \n",
              "q_6: |0>┤ S ├──┼──┤ S ├┤ S ├───────┼─────────┼────┼─────────┼────────────\n",
              "        └───┘  │  └───┘└───┘       │         │  ┌─┴─┐┌───┐  │            \n",
              "q_7: |0>───────┼───────────────────■─────────┼──┤ X ├┤ H ├──┼────────────\n",
              "               │                           ┌─┴─┐└───┘└───┘  │            \n",
              "q_8: |0>───────┼───────────────────────────┤ X ├────────────┼────────────\n",
              "             ┌─┴─┐                         └───┘            │            \n",
              "q_9: |0>─────┤ X ├──────────────────────────────────────────■────────────\n",
              "             └───┘                                                       </pre>"
            ],
            "text/plain": [
              "<qiskit.visualization.text.TextDrawing at 0x7fc4e0b25ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsemFtmoS-VW",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY96ywLFTnID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "n = 5\n",
        "vocab_dim = 2+2*n+n**2\n",
        "embedding_dim = 32\n",
        "max_length = 30\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01_2pq0lV-3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data\n",
        "l_m = encode_circuits(sample_circuits(n,max_length,2000),n,max_length,label=True)\n",
        "l_q = encode_circuits(sample_circuits(n,int(np.ceil(max_length/4)),500),n,max_length,label=True)\n",
        "l_h = encode_circuits(sample_circuits(n,int(np.ceil(max_length/2)),500),n,max_length,label=True)\n",
        "l_tq = encode_circuits(sample_circuits(n,int(np.ceil(3*max_length/4)),500),n,max_length,label=True)\n",
        "Data = np.concatenate((l_m,l_q,l_h,l_tq),axis=0)\n",
        "np.random.shuffle(Data)\n",
        "X = Data[:,32:]\n",
        "y = Data[:,:32]\n",
        "X_train = X[:1500]\n",
        "X_test = X[1500:]\n",
        "y_train = y[:1500]\n",
        "y_test = y[1500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQHAq62aTDvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "df99ee42-c980-4468-ac93-22aeb7ed3509"
      },
      "source": [
        "# Model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_length+2,embedding_dim,mask_zero=True))\n",
        "#model.add(tf.keras.layers.Masking(mask_value=0, input_shape=(max_length,)))\n",
        "#model.add(tf.keras.layers.RepeatVector(max_length, input_shape=(max_length,)))\n",
        "model.add(tf.keras.layers.LSTM(max_length+2, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(max_length+2, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"Adadelta\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_34 (Embedding)     (None, None, 32)          1024      \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                1056      \n",
            "=================================================================\n",
            "Total params: 10,400\n",
            "Trainable params: 10,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awdnLm9neBkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc2e10a3-8c03-487b-8938-286525774198"
      },
      "source": [
        "# Train\n",
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size,\n",
        "          epochs=100)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.1108 - accuracy: 0.0193 - val_loss: 794.2617 - val_accuracy: 0.0140\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0986 - accuracy: 0.0233 - val_loss: 794.2491 - val_accuracy: 0.0260\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0851 - accuracy: 0.0347 - val_loss: 794.2361 - val_accuracy: 0.0260\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0718 - accuracy: 0.0440 - val_loss: 794.2226 - val_accuracy: 0.0440\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0572 - accuracy: 0.0433 - val_loss: 794.2086 - val_accuracy: 0.0420\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0427 - accuracy: 0.0560 - val_loss: 794.1940 - val_accuracy: 0.0680\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0276 - accuracy: 0.0680 - val_loss: 794.1790 - val_accuracy: 0.0680\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 794.0118 - accuracy: 0.0680 - val_loss: 794.1634 - val_accuracy: 0.0680\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9950 - accuracy: 0.0680 - val_loss: 794.1470 - val_accuracy: 0.0680\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9782 - accuracy: 0.0680 - val_loss: 794.1300 - val_accuracy: 0.0680\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9606 - accuracy: 0.0680 - val_loss: 794.1122 - val_accuracy: 0.0680\n",
            "Epoch 12/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9417 - accuracy: 0.0680 - val_loss: 794.0936 - val_accuracy: 0.0680\n",
            "Epoch 13/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9222 - accuracy: 0.0680 - val_loss: 794.0741 - val_accuracy: 0.0680\n",
            "Epoch 14/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.9017 - accuracy: 0.0680 - val_loss: 794.0535 - val_accuracy: 0.0680\n",
            "Epoch 15/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8799 - accuracy: 0.0680 - val_loss: 794.0319 - val_accuracy: 0.0680\n",
            "Epoch 16/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8572 - accuracy: 0.0680 - val_loss: 794.0092 - val_accuracy: 0.0680\n",
            "Epoch 17/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8333 - accuracy: 0.0680 - val_loss: 793.9852 - val_accuracy: 0.0680\n",
            "Epoch 18/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.8081 - accuracy: 0.0680 - val_loss: 793.9597 - val_accuracy: 0.0680\n",
            "Epoch 19/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7812 - accuracy: 0.0680 - val_loss: 793.9327 - val_accuracy: 0.0680\n",
            "Epoch 20/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7522 - accuracy: 0.0680 - val_loss: 793.9039 - val_accuracy: 0.0680\n",
            "Epoch 21/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.7214 - accuracy: 0.0680 - val_loss: 793.8732 - val_accuracy: 0.0680\n",
            "Epoch 22/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6885 - accuracy: 0.0680 - val_loss: 793.8400 - val_accuracy: 0.0680\n",
            "Epoch 23/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6528 - accuracy: 0.0680 - val_loss: 793.8042 - val_accuracy: 0.0680\n",
            "Epoch 24/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6150 - accuracy: 0.0680 - val_loss: 793.7653 - val_accuracy: 0.0680\n",
            "Epoch 25/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.5724 - accuracy: 0.0680 - val_loss: 793.7226 - val_accuracy: 0.0680\n",
            "Epoch 26/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.5261 - accuracy: 0.0680 - val_loss: 793.6755 - val_accuracy: 0.0680\n",
            "Epoch 27/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.4745 - accuracy: 0.0687 - val_loss: 793.6229 - val_accuracy: 0.0840\n",
            "Epoch 28/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.4164 - accuracy: 0.0773 - val_loss: 793.5636 - val_accuracy: 0.0840\n",
            "Epoch 29/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.3492 - accuracy: 0.0773 - val_loss: 793.4954 - val_accuracy: 0.0840\n",
            "Epoch 30/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.2706 - accuracy: 0.0773 - val_loss: 793.4154 - val_accuracy: 0.0840\n",
            "Epoch 31/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.1739 - accuracy: 0.0773 - val_loss: 793.3188 - val_accuracy: 0.0840\n",
            "Epoch 32/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.0570 - accuracy: 0.0773 - val_loss: 793.1983 - val_accuracy: 0.0840\n",
            "Epoch 33/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.8992 - accuracy: 0.0773 - val_loss: 793.0473 - val_accuracy: 0.0840\n",
            "Epoch 34/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.7315 - accuracy: 0.0773 - val_loss: 792.9285 - val_accuracy: 0.0840\n",
            "Epoch 35/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 792.6533 - accuracy: 0.0773 - val_loss: 793.3630 - val_accuracy: 0.0840\n",
            "Epoch 36/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 793.6683 - accuracy: 0.0773 - val_loss: 795.2918 - val_accuracy: 0.0840\n",
            "Epoch 37/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 796.2934 - accuracy: 0.0773 - val_loss: 798.7723 - val_accuracy: 0.0840\n",
            "Epoch 38/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 800.9558 - accuracy: 0.0773 - val_loss: 804.4900 - val_accuracy: 0.0840\n",
            "Epoch 39/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 806.7342 - accuracy: 0.0687 - val_loss: 809.5878 - val_accuracy: 0.0680\n",
            "Epoch 40/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 810.9744 - accuracy: 0.0667 - val_loss: 812.6163 - val_accuracy: 0.0680\n",
            "Epoch 41/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 813.3979 - accuracy: 0.0667 - val_loss: 814.4037 - val_accuracy: 0.0680\n",
            "Epoch 42/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 815.1448 - accuracy: 0.0667 - val_loss: 816.5336 - val_accuracy: 0.0680\n",
            "Epoch 43/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 817.5818 - accuracy: 0.0667 - val_loss: 819.2408 - val_accuracy: 0.0680\n",
            "Epoch 44/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 820.3315 - accuracy: 0.0667 - val_loss: 821.9930 - val_accuracy: 0.0680\n",
            "Epoch 45/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 823.3867 - accuracy: 0.0667 - val_loss: 824.9569 - val_accuracy: 0.0680\n",
            "Epoch 46/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 826.7348 - accuracy: 0.0667 - val_loss: 828.4964 - val_accuracy: 0.0680\n",
            "Epoch 47/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 830.3835 - accuracy: 0.0667 - val_loss: 832.8303 - val_accuracy: 0.0680\n",
            "Epoch 48/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 835.1679 - accuracy: 0.0667 - val_loss: 838.0462 - val_accuracy: 0.0680\n",
            "Epoch 49/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 840.8740 - accuracy: 0.0667 - val_loss: 844.2008 - val_accuracy: 0.0680\n",
            "Epoch 50/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 847.6452 - accuracy: 0.0667 - val_loss: 851.3121 - val_accuracy: 0.0680\n",
            "Epoch 51/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 855.2304 - accuracy: 0.0660 - val_loss: 859.3821 - val_accuracy: 0.0680\n",
            "Epoch 52/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 864.1146 - accuracy: 0.0653 - val_loss: 868.4477 - val_accuracy: 0.0680\n",
            "Epoch 53/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 873.4734 - accuracy: 0.0653 - val_loss: 878.5035 - val_accuracy: 0.0680\n",
            "Epoch 54/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 884.2753 - accuracy: 0.0667 - val_loss: 889.4948 - val_accuracy: 0.0740\n",
            "Epoch 55/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 895.7303 - accuracy: 0.0747 - val_loss: 901.3611 - val_accuracy: 0.0740\n",
            "Epoch 56/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 908.2810 - accuracy: 0.0747 - val_loss: 913.9891 - val_accuracy: 0.0740\n",
            "Epoch 57/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 921.2766 - accuracy: 0.0747 - val_loss: 927.3201 - val_accuracy: 0.0740\n",
            "Epoch 58/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 935.2397 - accuracy: 0.0747 - val_loss: 941.3236 - val_accuracy: 0.0740\n",
            "Epoch 59/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 949.5889 - accuracy: 0.0747 - val_loss: 955.9353 - val_accuracy: 0.0740\n",
            "Epoch 60/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 964.7316 - accuracy: 0.0747 - val_loss: 971.0822 - val_accuracy: 0.0740\n",
            "Epoch 61/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 980.3476 - accuracy: 0.0747 - val_loss: 986.7279 - val_accuracy: 0.0740\n",
            "Epoch 62/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 996.4534 - accuracy: 0.0747 - val_loss: 1002.8438 - val_accuracy: 0.0740\n",
            "Epoch 63/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1013.4105 - accuracy: 0.0747 - val_loss: 1019.3913 - val_accuracy: 0.0740\n",
            "Epoch 64/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1030.0822 - accuracy: 0.0747 - val_loss: 1036.3138 - val_accuracy: 0.0740\n",
            "Epoch 65/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1047.1509 - accuracy: 0.0747 - val_loss: 1053.5756 - val_accuracy: 0.0740\n",
            "Epoch 66/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1064.5440 - accuracy: 0.0747 - val_loss: 1071.1331 - val_accuracy: 0.0740\n",
            "Epoch 67/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1082.3884 - accuracy: 0.0747 - val_loss: 1089.0033 - val_accuracy: 0.0740\n",
            "Epoch 68/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1100.9237 - accuracy: 0.0747 - val_loss: 1107.1381 - val_accuracy: 0.0740\n",
            "Epoch 69/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1119.3317 - accuracy: 0.0747 - val_loss: 1125.5131 - val_accuracy: 0.0740\n",
            "Epoch 70/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1137.9003 - accuracy: 0.0747 - val_loss: 1144.1294 - val_accuracy: 0.0740\n",
            "Epoch 71/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1156.4520 - accuracy: 0.0747 - val_loss: 1162.9580 - val_accuracy: 0.0740\n",
            "Epoch 72/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1175.9621 - accuracy: 0.0747 - val_loss: 1182.0212 - val_accuracy: 0.0740\n",
            "Epoch 73/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1195.5856 - accuracy: 0.0747 - val_loss: 1201.2620 - val_accuracy: 0.0740\n",
            "Epoch 74/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1214.2600 - accuracy: 0.0747 - val_loss: 1220.6798 - val_accuracy: 0.0740\n",
            "Epoch 75/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1234.0045 - accuracy: 0.0747 - val_loss: 1240.2935 - val_accuracy: 0.0740\n",
            "Epoch 76/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1254.2018 - accuracy: 0.0747 - val_loss: 1260.0628 - val_accuracy: 0.0740\n",
            "Epoch 77/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1273.8898 - accuracy: 0.0747 - val_loss: 1279.9555 - val_accuracy: 0.0740\n",
            "Epoch 78/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1294.3798 - accuracy: 0.0747 - val_loss: 1300.0232 - val_accuracy: 0.0740\n",
            "Epoch 79/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1314.3088 - accuracy: 0.0747 - val_loss: 1320.2064 - val_accuracy: 0.0740\n",
            "Epoch 80/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1334.9257 - accuracy: 0.0747 - val_loss: 1340.5135 - val_accuracy: 0.0740\n",
            "Epoch 81/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1355.1978 - accuracy: 0.0747 - val_loss: 1360.9623 - val_accuracy: 0.0740\n",
            "Epoch 82/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1376.0900 - accuracy: 0.0747 - val_loss: 1381.5232 - val_accuracy: 0.0740\n",
            "Epoch 83/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1397.0069 - accuracy: 0.0747 - val_loss: 1402.2087 - val_accuracy: 0.0740\n",
            "Epoch 84/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1417.6396 - accuracy: 0.0747 - val_loss: 1422.9876 - val_accuracy: 0.0740\n",
            "Epoch 85/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1438.5361 - accuracy: 0.0747 - val_loss: 1443.8418 - val_accuracy: 0.0740\n",
            "Epoch 86/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1459.9821 - accuracy: 0.0747 - val_loss: 1464.7824 - val_accuracy: 0.0740\n",
            "Epoch 87/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1480.7023 - accuracy: 0.0747 - val_loss: 1485.8067 - val_accuracy: 0.0740\n",
            "Epoch 88/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1502.2275 - accuracy: 0.0747 - val_loss: 1506.9167 - val_accuracy: 0.0740\n",
            "Epoch 89/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1523.7325 - accuracy: 0.0747 - val_loss: 1528.1184 - val_accuracy: 0.0740\n",
            "Epoch 90/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1544.4803 - accuracy: 0.0747 - val_loss: 1549.4111 - val_accuracy: 0.0740\n",
            "Epoch 91/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1566.1620 - accuracy: 0.0747 - val_loss: 1570.7809 - val_accuracy: 0.0740\n",
            "Epoch 92/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1587.5873 - accuracy: 0.0747 - val_loss: 1592.2300 - val_accuracy: 0.0740\n",
            "Epoch 93/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1609.1873 - accuracy: 0.0747 - val_loss: 1613.7593 - val_accuracy: 0.0740\n",
            "Epoch 94/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1630.8284 - accuracy: 0.0747 - val_loss: 1635.3507 - val_accuracy: 0.0740\n",
            "Epoch 95/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1652.5624 - accuracy: 0.0747 - val_loss: 1657.0287 - val_accuracy: 0.0740\n",
            "Epoch 96/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1674.0945 - accuracy: 0.0747 - val_loss: 1678.7179 - val_accuracy: 0.0740\n",
            "Epoch 97/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1696.1633 - accuracy: 0.0747 - val_loss: 1700.4490 - val_accuracy: 0.0740\n",
            "Epoch 98/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1718.2308 - accuracy: 0.0747 - val_loss: 1722.2234 - val_accuracy: 0.0740\n",
            "Epoch 99/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1739.8223 - accuracy: 0.0747 - val_loss: 1744.0254 - val_accuracy: 0.0740\n",
            "Epoch 100/100\n",
            "1500/1500 [==============================] - 2s 1ms/sample - loss: 1761.7105 - accuracy: 0.0747 - val_loss: 1765.8690 - val_accuracy: 0.0740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fa8495240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqougJSpSScn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c2a545f8-25b9-4a6d-81b8-67479a3b4f47"
      },
      "source": [
        "model.predict(X_test)[1]"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.07551420e-02, 3.53694521e-02, 5.65202162e-02, 9.26227644e-02,\n",
              "       1.31801024e-01, 1.92374557e-01, 2.02626482e-01, 2.44499505e-01,\n",
              "       1.21816928e-02, 6.54638512e-03, 9.04475630e-04, 8.65014081e-05,\n",
              "       1.11552211e-03, 8.93953547e-04, 1.12635980e-03, 5.64334448e-04,\n",
              "       1.97271265e-06, 7.51783830e-07, 9.01427597e-07, 5.44743079e-06,\n",
              "       4.84584291e-07, 3.19799511e-07, 7.80020173e-07, 9.12153382e-07,\n",
              "       2.41702547e-09, 4.57604565e-10, 1.11226334e-10, 3.81706311e-09,\n",
              "       8.70810757e-10, 1.63690728e-10, 3.51982332e-10, 1.91976812e-09],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9HgWMyLldpj",
        "colab_type": "text"
      },
      "source": [
        "#### Code Graveyard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1Arf1fAqwE",
        "colab_type": "text"
      },
      "source": [
        "Old encoding, based on one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb-iCh1_lfZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_circuits(circuits,label=True):\n",
        "    \"\"\"Takes a QuantumCircuit object, and generates an encoding\n",
        "    for use by the network as an array of gates, where each gate is encoded in\n",
        "    one-hot encoding for gate type, target qubit and control qubit.\n",
        "    We also embed tokens to signify the start and end of each sequence, encoded\n",
        "    as extra gate types.\n",
        "    Labels can be included or excluded.\"\"\"\n",
        "    encoded = []\n",
        "    for circ in circuits:\n",
        "        lines = circ.qasm().splitlines()[2:]\n",
        "        n = int(lines[0][7:-2])\n",
        "        lines = lines[1:]\n",
        "        size = len(lines)\n",
        "        encoded_circ = []\n",
        "        eye_g = np.eye(5)\n",
        "        eye_n = np.eye(n)\n",
        "        encoded_circ.append(eye_g[3])\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        for line in lines:\n",
        "            gate_str = line[:2]\n",
        "            integers = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*',line)]\n",
        "            if gate_str==\"h \":\n",
        "                encoded_circ.append(eye_g[0])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "                encoded_circ.append(np.zeros(n))\n",
        "            if gate_str==\"s \":\n",
        "                encoded_circ.append(eye_g[1])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "                encoded_circ.append(np.zeros(n))\n",
        "            if gate_str==\"cx\":\n",
        "                encoded_circ.append(eye_g[2])\n",
        "                encoded_circ.append(eye_n[integers[1]])\n",
        "                encoded_circ.append(eye_n[integers[0]])\n",
        "        encoded_circ.append(eye_g[4])\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded_circ.append(np.zeros(n))\n",
        "        encoded.append(encoded_circ)\n",
        "    encoded = np.array(encoded)\n",
        "    if label:\n",
        "        labels = generate_labels(circuits)\n",
        "        return np.array(encoded),labels\n",
        "    else:\n",
        "        return np.array(encoded)\n",
        "\n",
        "def decode_circuit(encoded):\n",
        "    \"\"\"Takes an encoded output from the network and generates the corresponding\n",
        "    circuit as described above.\"\"\"\n",
        "    decoded = \"OPENQASM 2.0;\\ninclude \\\"qelib1.inc\\\";\\nqreg q[\"\n",
        "    decoded += str(len(encoded[1]))+\"];\\n\"\n",
        "    encoded = [encoded[n:n+3] for n in range(0, len(encoded), 3)]\n",
        "    for line in encoded:\n",
        "        gate_num = np.argmax(line[0])\n",
        "        target = str(np.argmax(line[1]))\n",
        "        if gate_num==0:\n",
        "            decoded += \"h q[\"+target\n",
        "        if gate_num==1:\n",
        "            decoded += \"s q[\"+target\n",
        "        if gate_num==2:\n",
        "            control = str(np.argmax(line[2]))\n",
        "            decoded += \"cx q[\"+control+\"],q[\"+target\n",
        "        if gate_num < 3:\n",
        "            decoded += \"];\"\n",
        "        if gate_num==4:\n",
        "            decoded += \"\\n\"\n",
        "    return qk.QuantumCircuit.from_qasm_str(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi4rG-xPFdjZ",
        "colab_type": "text"
      },
      "source": [
        "Old loss function, uses simulation to test generated circuits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnTdN252p-Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function\n",
        "@tf.function\n",
        "def simulation_loss(y_true,y_pred):\n",
        "    # global N\n",
        "    decoded = decode_circuit(y_pred,5)\n",
        "    y_pred = generate_labels([decoded])[0]\n",
        "    return K.mse(y_true,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}